{"url": "https://www.atlassian.com/blog/artificial-intelligence/artificial-intelligence-101-the-basics-of-ai", "title": "Artificial Intelligence 101: The basics of AI everyone should know\u00a0 - Work Life by Atlassian", "content": "Atlassian Artificial Intelligence 101: The basics of AI everyone should know\u00a0 AI Explained in Simple Terms Head of Product, Atlassian Intelligence Artificial Intelligence, or AI for short, is a groundbreaking technology that\u2019s changing how we do everything. In this article, we\u2019ll explore what AI can do and how it\u2019s influencing our lives. From smart gadgets to chatbots, AI is everywhere, making tasks easier and faster.\u00a0 When you\u2019re ready to use AI in the workplace, try Atlassian Intelligence. Atlassian Intelligence brings cutting-edge AI tools right into your workflow, whether you\u2019re managing projects in Jira or collaborating in Confluence.\u00a0 Atlassian Intelligence is a suite of AI capabilities that streamlines tasks, boosts productivity, and provides insights into your work\u2014with features tailored to enhance teamwork. With Atlassian Intelligence, you can simplify complex processes and achieve more effortlessly, using advanced technology for everyday project management and collaboration.\u00a0 But what is AI, and how exactly does it work? If you\u2019re new to the concept of AI, you\u2019re not alone. Whether you\u2019re a project manager, marketer, or an executive, you can learn what AI is, how it works, and how it might influence your daily life. Keep reading to learn more about AI.\u00a0 What is artificial intelligence? Artificial intelligence (AI) involves creating machines that can think like humans and imitate their actions. This field uses various technologies to enable computers to do things that normally need human intelligence, like recognizing images, understanding speech, making decisions, and translating languages. Essentially, artificial intelligence is like having a smart computer that can learn from experience, solve problems, and make decisions on its own \u2014 just like a human.\u00a0 How AI works AI learns and becomes more intelligent. It works similarly to how humans learn how to ride a bike. Just like you get better by practicing, AI systems learn from examples and data to improve their performance over time. Instead of being explicitly programmed for every task, AI uses algorithms to learn from experiences.\u00a0 The more data AI systems have and the more they practice, the better they become at their tasks. This ability to learn and improve without constant human instruction makes AI so powerful and versatile in solving complex problems.\u00a0 AI terminology explained When you\u2019re first learning about AI, many of the technical terms may seem complicated. Let\u2019s break down some of these terms to make them easier to grasp.\u00a0 Large Language Models (LLMs) Imagine having a conversation with a knowledgeable computer that can understand what you\u2019re saying and respond in a way that makes sense. That\u2019s what large language models (LLMS) can do. They\u2019re powerful systems that can generate human-like text and help us with tasks like writing articles or answering questions.\u00a0 Datasets Datasets are large collections of information that AI systems use to learn. They can include things like images, paragraphs of text, or even numbers from sensors. AI systems look at these examples and can figure out patterns, allowing them to make decisions just like we do when we learn from seeing examples repeatedly.\u00a0 Machine learning Machine learning allows computers to learn from data. For instance, it allows a computer to recognize cats in pictures by being trained on many examples of images labeled as cats and images labeled as not cats. This training process involves the computer identifying patterns in the data, allowing it to make predictions or decisions based on new information. Algorithm An algorithm is a set of rules that tells the computer how to solve a problem or perform a task, just like following a recipe to bake a cake. Algorithms are used in everything from sorting numbers to recommending movies on streaming platforms.\u00a0 Neural networks Neural networks are like a team of tiny brains inside a computer. They\u2019re computational models inspired by how our brains work, designed to recognize patterns and solve complex problems. Each neuron in the network processes information and passes it on to others, working together to solve puzzles or identify objects in images.\u00a0 Natural language processing Natural language processing (NLP) is a subset of AI that helps computers understand, interpret, and generate human language. It teaches computers to understand and talk like humans do, similar to how we interact with virtual assistants like Siri or Alexa. NLP allows machines to read text, translate languages, and even generate responses in conversations.\u00a0 Big data Big data refers to massive collections of information or data that AI uses to learn and make decisions humans might miss. AI images analyze a large amount of data, such as images, texts, or numbers, to find patterns and insights. This can help businesses and scientists make better decisions based on data rather than guesses.\u00a0 Deep learning Deep learning uses neural network algorithms to process complex data and achieve high accuracy in tasks like recognizing faces in photos or understanding spoken language. Similar to how we learn from examples to get smarter, computers learn from vast amounts of data to improve their performance. Structured query language (SQL) SQL is a programming language used to communicate with databases\u2014huge libraries that store information and data. It allows people to ask specific questions (queries) and get answers quickly. Imagine a huge library where you can ask the librarian to find all the books published after 2010. SQL lets you ask the database similar questions to find specific information quickly. Jira query language Jira query language (JQL) is a unique language used within Jira. JQL works similarly to SQL, helping users search for and filter issues, which can be tasks, bugs, or other types of work items, within Jira based on specific criteria.\u00a0 For example, in Jira, you might use JQL to find all the tasks assigned to you due this week. This language allows you to specify conditions like who the task is assigned to, when it\u2019s due, or its status. JQL helps you efficiently manage your work by finding and organizing tasks directly within Jira. Using AI in our everyday lives Artificial intelligence is already seamlessly integrated into our daily routines. When you search the internet, AI algorithms work behind the scenes to understand your query and provide relevant results quickly. Whether you\u2019re looking for a recipe, learning about a topic, or shopping, AI streamlines information retrieval and makes browsing more personalized.\u00a0 Think about how AI powers personalized recommendations on streaming platforms like Netflix or music services like Spotify. Artificial intelligence analyzes your viewing or listening habits and suggests content that matches your preferences, making entertainment choices more enjoyable and tailored to your tastes.\u00a0 Meanwhile, voice assistants like Siri, Alexa, and Google Assistant also use AI to understand and respond to voice commands, helping you set reminders, play music, or even control smart home devices with natural language.\u00a0 AI represents a monumental leap in technology. It allows computers to respond to human commands and queries by letting you ask questions like a human and get answers tailored to your needs. Just as you would ask a friend for help, AI allows us to ask questions and get answers in intuitive and natural ways.\u00a0 Incorporate AI into your workflows with Atlassian Intelligence, a suite of AI capabilities embedded within tools like Jira, Confluence, Jira Service Management, and more. These tools streamline workflows by automating repetitive tasks, providing intelligent insights from analytics, and enhancing collaboration among teams. Whether you\u2019re managing projects, tracking individual tasks, or sharing knowledge, Atlassian Intelligence helps teams work smarter. \nJamil Valliani\nHead of Product, Atlassian Intelligence\n More in AI \n\n\t\t\tAI Collaboration Report: \u201cUsing\u201d AI is not enough \u2013 here\u2019s what your organization is missing\t\t\n \n\n\t\t\tWhat is conversational AI?\t\t\n \n\n\t\t\tHow to learn AI for beginners\t\t\n \n\n\t\t\tUnderstanding responsible AI practices\t\t\n More Collections \n\t\t\t\tCompany Culture\t\t\t Embrace transparency, foster a sense of belonging, form connections \u2013 and have fun along the way. \n\t\t\t\tYour Personality at Work\t\t\t Navigating and celebrating the complexities of our individuality. \n\t\t\t\tEarnings Reports\t\t\t A look at what we're sharing with our investors and stakeholders each quarter. Artificial Intelligence 101: The basics of AI everyone should know\u00a0 By Atlassian Culture, tech, teams, and tips, delivered twice a month Advice, stories, and expertise about work life today."}
{"url": "https://www.simplilearn.com/tutorials/artificial-intelligence-tutorial/what-is-artificial-intelligence", "title": "What is Artificial Intelligence? How AI Works & Key Concepts", "content": "Tutorial Playlist  Artificial Intelligence Tutorial for Beginners  What is Artificial Intelligence: Types, History, and Future  24 Cutting-Edge Artificial Intelligence Applications in 2025  How Does AI Work  Types of Artificial Intelligence That You Should Know in 2024  Discover the Differences Between AI vs. Machine Learning vs. Deep Learning  What Is NLP? Introductory Guide to Natural Language Processing!  How to Become an AI Engineer  The Top Five Humanoid Robots  What Is Virtual Reality? Everything You Need to Know  Best AR Apps in 2025  Want an Easy Idea About What Is Augmented Reality? Read This!  The Ultimate Guide to \u2018What Is SparkAR\u2019 and Its Effects on Social Media  Introduction to Long Short-Term Memory(LSTM)  Top 30 AI Projects in 2024: Basic, Mid, Advanced  The Ultimate Guide to Forward and Backward Chaining in AI  Top Artificial Intelligence Techniques: Cracking the Code  What are the Major Goals of Artificial Intelligence?  Conversational AI: Enhancing Customer Engagement and Support  Overview of Narrow AI  Explainable AI  A Complete Guide on the Role of AI in Healthcare  Top AI Developer Tools You Need to Know in 2025 What is Artificial Intelligence? How AI Works & Key Concepts Lesson 1 of 22By Nikita Duggal  Artificial Intelligence Tutorial for Beginners  What is Artificial Intelligence: Types, History, and Future  24 Cutting-Edge Artificial Intelligence Applications in 2025  How Does AI Work  Types of Artificial Intelligence That You Should Know in 2024  Discover the Differences Between AI vs. Machine Learning vs. Deep Learning  What Is NLP? Introductory Guide to Natural Language Processing!  How to Become an AI Engineer  The Top Five Humanoid Robots  What Is Virtual Reality? Everything You Need to Know  Best AR Apps in 2025  Want an Easy Idea About What Is Augmented Reality? Read This!  The Ultimate Guide to \u2018What Is SparkAR\u2019 and Its Effects on Social Media  Introduction to Long Short-Term Memory(LSTM)  Top 30 AI Projects in 2024: Basic, Mid, Advanced  The Ultimate Guide to Forward and Backward Chaining in AI  Top Artificial Intelligence Techniques: Cracking the Code  What are the Major Goals of Artificial Intelligence?  Conversational AI: Enhancing Customer Engagement and Support  Overview of Narrow AI  Explainable AI  A Complete Guide on the Role of AI in Healthcare  Top AI Developer Tools You Need to Know in 2025 Table of Contents Artificial intelligence (AI) is currently one of the hottest buzzwords in tech and with good reason. The last few years have seen several innovations and advancements that have previously been solely in the realm of science fiction slowly transform into reality.\u00a0 Experts regard artificial intelligence as a factor of production, which has the potential to introduce new sources of growth and change the way work is done across industries. For instance, this PWC article predicts that AI could potentially contribute $15.7 trillion to the global economy by 2035. China and the United States are primed to benefit the most from the coming AI boom, accounting for nearly 70% of the global impact.  Become an AI and Machine Learning Expert  This tutorial provides an overview of AI, including how it works, its pros and cons, its applications, certifications, and why it\u2019s a good field to master.\u00a0 What Is Artificial Intelligence? Artificial intelligence (AI) is the simulation of human intelligence in machines that are programmed to think and act like humans. Learning, reasoning, problem-solving, perception, and language comprehension are all examples of cognitive abilities. Artificial Intelligence is a method of making a computer, a computer-controlled robot, or a software think intelligently like the human mind. AI is accomplished by studying the patterns of the human brain and by analyzing the cognitive process. The outcome of these studies develops intelligent software and systems. For example, Natural Language Processing (NLP) uses AI to analyze and interpret human language in text or speech. It enables applications like chatbots and virtual assistants to understand user queries, extract meaningful information, and deliver accurate, context-aware responses, transforming unstructured communication into actionable insights. Weak AI vs. Strong AI When discussing artificial intelligence (AI), it is common to distinguish between two broad categories: weak AI and strong AI. Let's explore the characteristics of each type: Weak AI (Narrow AI) Weak AI refers to AI systems that are designed to perform specific tasks and are limited to those tasks only. These AI systems excel at their designated functions but lack general intelligence. Examples of weak AI include voice assistants like Siri or Alexa, recommendation algorithms, and image recognition systems. Weak AI operates within predefined boundaries and cannot generalize beyond their specialized domain. Strong AI (General AI) Strong AI, also known as general AI, refers to AI systems that possess human-level intelligence or even surpass human intelligence across a wide range of tasks. Strong AI would be capable of understanding, reasoning, learning, and applying knowledge to solve complex problems in a manner similar to human cognition. However, the development of strong AI is still largely theoretical and has not been achieved to date. Learn Core AI Engineering Skills and Tools Types of Artificial Intelligence\u00a0 Below are the various types of AI: 1. Purely Reactive These machines do not have any memory or data to work with, specializing in just one field of work. For example, in a chess game, the machine observes the moves and makes the best possible decision to win.\u00a0 2. Limited Memory These machines collect previous data and continue adding it to their memory. They have enough memory or experience to make proper decisions, but memory is minimal. For example, this machine can suggest a restaurant based on the location data that has been gathered. 3. Theory of Mind This kind of AI can understand thoughts and emotions, as well as interact socially. However, a machine based on this type is yet to be built.\u00a0 4. Self-Aware Self-aware machines are the future generation of these new technologies. They will be intelligent, sentient, and conscious.  Become an AI and Machine Learning Expert  Deep Learning vs. Machine Learning Let's explore the contrast between deep learning and machine learning: Machine Learning:\u00a0 Machine Learning focuses on the development of algorithms and models that enable computers to learn from data and make predictions or decisions without explicit programming. Here are key characteristics of machine learning: Deep Learning:\u00a0 Deep Learning is a subset of machine learning that focuses on training artificial neural networks inspired by the human brain's structure and functioning. Here are key characteristics of deep learning:  Artificial Intelligence Engineer  How Does Artificial Intelligence Work? Put simply, AI systems work by merging large with intelligent, iterative processing algorithms. This combination allows AI to learn from patterns and features in the analyzed data. Each time an Artificial Intelligence system performs a round of data processing, it tests and measures its performance and uses the results to develop additional expertise. Ways of Implementing AI\u00a0 Let\u2019s explore the following ways that explain how we can implement AI: Machine Learning It is machine learning that gives AI the ability to learn. This is done by using algorithms to discover patterns and generate insights from the data they are exposed to.\u00a0 Deep Learning Deep learning, which is a subcategory of machine learning, provides AI with the ability to mimic a human brain\u2019s neural network. It can make sense of patterns, noise, and sources of confusion in the data. Consider an image shown below:  Here, we segregated the various kinds of images using deep learning. The machine goes through multiple features of photographs and distinguishes them with feature extraction. The machine segregates the features of each photo into different categories, such as landscape, portrait, or others.\u00a0 Let us understand how deep learning works.\u00a0 Consider the image shown below:  The above image depicts the three main layers of a neural network: Input Layer The images that we want to segregate go into the input layer. Arrows are drawn from the image on to the individual dots of the input layer. Each of the white dots in the yellow layer (input layer) are a pixel in the picture. These images fill the white dots in the input layer. We should clearly know these three layers while going through this artificial intelligence tutorial. Hidden Layer The hidden layers are responsible for all our inputs' mathematical computations or feature extraction. In the above image, the layers shown in orange represent the hidden layers. The lines that are seen between these layers are called \u2018weights\u2019. Each one of them usually represents a float number, or a decimal number, which is multiplied by the value in the input layer. All the weights add up in the hidden layer. The dots in the hidden layer represent a value based on the sum of the weights. These values are then passed to the next hidden layer. You may be wondering why there are multiple layers. The hidden layers function as alternatives to some degree. The more the hidden layers are, the more complex the data that goes in and what can be produced. The accuracy of the predicted output generally depends on the number of hidden layers present and the complexity of the data going in.  Master the Right AI Tools for the Right Job!  Output Layer The output layer gives us segregated photos. Once the layer adds up all these weights being fed in, it'll determine if the picture is a portrait or a landscape. Example - Predicting Airfare Costs This prediction is based on various factors, including: We begin with some historical data on ticket prices to train the machine. Once our machine is trained, we share new data that will predict the costs. Earlier, when we learned about four kinds of machines, we discussed machines with memory. Here, we talk about the memory only, and how it understands a pattern in the data and uses it to make predictions for the new prices as shown below:  AI Programming Cognitive Skills: Learning, Reasoning and Self-Correction Artificial Intelligence emphasizes three cognitive skills of learning, reasoning, and self-correction, skills that the human brain possess to one degree or another. We define these in the context of AI as: However, researchers and programmers have extended and elaborated the goals of AI to the following: Logical Reasoning\u00a0 Knowledge Representation Planning and Navigation\u00a0 Natural Language Processing\u00a0 Perception\u00a0 Emergent Intelligence\u00a0 Some of the tasks performed by AI-enabled devices include: Advance Your Career With Top AI Engineering Skills Advantages and Disadvantages of AI Artificial intelligence has its pluses and minuses, much like any other concept or innovation. Here\u2019s a quick rundown of some pros and cons. Pros Cons Let us continue this article on What is Artificial Intelligence by discussing the applications of AI. Applications of Artificial Intelligence Artificial intelligence (AI) has a wide range of applications across various industries and domains. Here are some notable applications of AI: Natural Language Processing (NLP) AI is used in NLP to analyze and understand human language. It powers applications such as speech recognition, machine translation, sentiment analysis, and virtual assistants like Siri and Alexa. Image and Video Analysis AI techniques, including computer vision, enable the analysis and interpretation of images and videos. This finds application in facial recognition, object detection and tracking, content moderation, medical imaging, and autonomous vehicles. Robotics and Automation AI plays a crucial role in robotics and automation systems. Robots equipped with AI algorithms can perform complex tasks in manufacturing, healthcare, logistics, and exploration. They can adapt to changing environments, learn from experience, and collaborate with humans. Recommendation Systems AI-powered recommendation systems are used in e-commerce, streaming platforms, and social media to personalize user experiences. They analyze user preferences, behavior, and historical data to suggest relevant products, movies, music, or content. Financial Services AI is extensively used in the finance industry for fraud detection, algorithmic trading, credit scoring, and risk assessment. Machine learning models can analyze vast amounts of financial data to identify patterns and make predictions. Healthcare AI applications in healthcare include disease diagnosis, medical imaging analysis, drug discovery, personalized medicine, and patient monitoring. AI can assist in identifying patterns in medical data and provide insights for better diagnosis and treatment. Virtual Assistants and Chatbots AI-powered virtual assistants and chatbots interact with users, understand their queries, and provide relevant information or perform tasks. They are used in customer support, information retrieval, and personalized assistance. Gaming AI algorithms are employed in gaming for creating realistic virtual characters, opponent behavior, and intelligent decision-making. AI is also used to optimize game graphics, physics simulations, and game testing. Smart Homes and IoT AI enables the development of smart home systems that can automate tasks, control devices, and learn from user preferences. AI can enhance the functionality and efficiency of Internet of Things (IoT) devices and networks. Cybersecurity AI helps detect and prevent cyber threats by analyzing network traffic, identifying anomalies, and predicting potential attacks. It can also enhance the security of systems and data through advanced threat detection and response mechanisms. These are just a few examples of how AI is applied in various fields. AI's potential is vast, and its applications continue to expand as technology advances. Become an AI Engineer in 11 Months Artificial Intelligence Examples Artificial Intelligence (AI) has become an integral part of our daily lives, revolutionizing various industries and enhancing user experiences. Here are some notable examples of AI applications: ChatGPT ChatGPT is an advanced language model developed by OpenAI. It can generate human-like responses and engage in natural language conversations. It uses deep learning techniques to understand and generate coherent text, making it useful for customer support, chatbots, and virtual assistants. Google Maps Google Maps utilizes AI algorithms to provide real-time navigation, traffic updates, and personalized recommendations. It analyzes vast amounts of data, including historical traffic patterns and user input, to suggest the fastest routes, estimate arrival times, and even predict traffic congestion. Smart Assistants Smart assistants like Amazon's Alexa, Apple's Siri, and Google Assistant employ AI technologies to interpret voice commands, answer questions, and perform tasks. These assistants use natural language processing and machine learning algorithms to understand user intent, retrieve relevant information, and carry out requested actions. Snapchat Filters Snapchat's augmented reality filters, or \"Lenses,\" incorporate AI to recognize facial features, track movements, and overlay interactive effects on users' faces in real-time. AI algorithms enable Snapchat to apply various filters, masks, and animations that align with the user's facial expressions and movements. Self-Driving Cars Self-driving cars rely heavily on AI for perception, decision-making, and control. Using a combination of sensors, cameras, and machine learning algorithms, these vehicles can detect objects, interpret traffic signs, and navigate complex road conditions autonomously, enhancing safety and efficiency on the roads. Wearables Wearable devices, such as fitness trackers and smartwatches, utilize AI to monitor and analyze users' health data. They track activities, heart rate, sleep patterns, and more, providing personalized insights and recommendations to improve overall well-being. MuZero MuZero is an AI algorithm developed by DeepMind that combines reinforcement learning and deep neural networks. It has achieved remarkable success in playing complex board games like chess, Go, and shogi at a superhuman level. MuZero learns and improves its strategies through self-play and planning. These examples demonstrate the wide-ranging applications of AI, showcasing its potential to enhance our lives, improve efficiency, and drive innovation across various industries. Find Our Artificial Intelligence Course in Top Cities India United States Other Countries Different Artificial Intelligence Certifications 1. Introduction to Artificial Intelligence Course Simplilearn's Artificial Intelligence basics program is designed to help learners decode the mystery of artificial intelligence and its business applications. The course provides an overview of AI concepts and workflows, machine learning and deep learning, and performance metrics. You\u2019ll learn the difference between supervised, unsupervised and reinforcement learning, be exposed to use cases, and see how clustering and classification algorithms help identify AI business applications. 2. Machine Learning Course Simplilearn\u2019s Machine Learning Course will make you an expert in machine learning, a form of artificial intelligence that automates data analysis to enable computers to learn and adapt through experience to do specific tasks without explicit programming. You'll master machine learning concepts and techniques including supervised and unsupervised learning, mathematical and heuristic aspects, and hands-on modeling to develop algorithms and prepare you for the role of a Machine Learning Engineer. 3. Artificial Intelligence Engineer Master\u2019s Program Simplilearn's Masters in AI, in collaboration with IBM, gives training on the skills required for a successful career in AI. Throughout this exclusive training program, you'll master Deep Learning, Machine Learning, and the programming languages required to excel in this domain and kick-start your career in Artificial Intelligence. Reasons to Get an Artificial Intelligence Certification: The Key Takeaways Here are the top reasons why you should get a certification in AI if you\u2019re looking to join this exciting and growing field: 1. Demand for Certified AI Professionals will Continue to Grow The McKinsey Global Institute predicts that approximately 70 percent of businesses will be using at least one type of Artificial Intelligence technology by 2030, and about half of all big companies will embed a full range of Artificial Intelligence technology in their processes.\u00a0 AI will help companies offer customized solutions and instructions to employees in real-time. Therefore, the demand for professionals with skills in emerging technologies like AI will only continue to grow.\u00a0 2. New and Unconventional Career Paths A Future of Jobs Report released by the World Economic Forum in 2020 predicts that 85 million jobs will be lost to automation by 2025. However, it goes on to say that 97 new positions and roles will be created as industries figure out the balance between machines and humans. Because of AI, new skill sets are required in the workforce, leading to new job opportunities. Some of the top AI roles include: 3. Improve Your Earning Potential Many of the top tech enterprises are investing in hiring talent with AI knowledge. The average Artificial Intelligence Engineer can earn $164,000 per year, and AI certification is a step in the right direction for enhancing your earning potential and becoming more marketable. Start your AI journey with our AI & Machine Learning Bootcamp. 4. Higher Chances of a Discussion If you are looking to join the AI industry, then becoming knowledgeable in Artificial Intelligence is just the first step; next, you need verifiable credentials. Certification earned after pursuing Simplilearn\u2019s AI and Ml course will help you reach the interview stage as you\u2019ll possess skills that many people in the market do not. Certification will help convince employers that you have the right skills and expertise for a job, making you a valuable candidate. Artificial Intelligence is emerging as the next big thing in technology. Organizations are adopting AI and budgeting for certified professionals in the field, thus the growing demand for trained and certified professionals. As this emerging field continues to grow, it will have an impact on everyday life and lead to considerable implications for many industries. FAQs 1. Where is AI used? Artificial intelligence is frequently utilized to present individuals with personalized suggestions based on their prior searches and purchases and other online behavior. AI is extremely crucial in commerce, such as product optimization, inventory planning, and logistics. Machine learning, cybersecurity, customer relationship management, internet searches, and personal assistants are some of the most common applications of AI. Voice assistants, picture recognition for face unlocking in cellphones, and ML-based financial fraud detection are all examples of AI software that is now in use. 2. What is artificial intelligence in simple words? Artificial Intelligence (AI) in simple words refers to the ability of machines or computer systems to perform tasks that typically require human intelligence. It is a field of study and technology that aims to create machines that can learn from experience, adapt to new information, and carry out tasks without explicit programming. Artificial Intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions. 3. What Are the 4 Types of AI? The current categorization system categorizes AI into four basic categories: reactive, theory of mind, limited memory, and self-aware. 4. How Is AI Used Today? Machines today can learn from experience, adapt to new inputs, and even perform human-like tasks with help from artificial intelligence (AI). Artificial intelligence examples today, from chess-playing computers to self-driving cars, are heavily based on deep learning and natural language processing. There are several examples of AI software in use in daily life, including voice assistants, face recognition for unlocking mobile phones and machine learning-based financial fraud detection. AI software is typically obtained by downloading AI-capable software from an internet marketplace, with no additional hardware required. 6. How is AI helping in our life? AI and ML-powered software and gadgets mimic human brain processes to assist society in advancing with the digital revolution. AI systems perceive their environment, deal with what they observe, resolve difficulties, and take action to help with duties to make daily living easier. People check their social media accounts on a frequent basis, including Facebook, Twitter, Instagram, and other sites. AI is not only customizing your feeds behind the scenes, but it is also recognizing and deleting bogus news. So, AI is assisting you in your daily life. 7. What are the three types of AI? The three types of AI are: 8. Is AI dangerous? Aside from planning for a future with super-intelligent computers, artificial intelligence in its current state might already offer problems. 9. What are the advantages of AI? The advantages of AI include reducing the time it takes to complete a task, reducing the cost of previously done activities, continuously and without interruption, with no downtime, and improving the capacities of people with disabilities. 10. What are the 7 main areas of AI? The main seven areas of AI are: Find our Post Graduate Program in AI and Machine Learning Online Bootcamp in top cities: About the Author Nikita Duggal is a passionate digital marketer with a major in English language and literature, a word connoisseur who loves writing about raging technologies, digital marketing, and career conundrums. Recommended Programs Post Graduate Program in AI and Machine Learning  Artificial Intelligence Engineer  Caltech Post Graduate Program in AI and Machine Learning  *Lifetime access to high-quality, self-paced e-learning content. Recommended Resources How Does AI Work Artificial Intelligence Career Guide: A Comprehensive Playbook to Becoming an AI Expert What is Artificial Intelligence and Why Gain AI Certification How to Become an AI Engineer What is ChatGPT AI and Why Does it Matter? Introduction to Artificial Intelligence: A Beginner's Guide \u00a9 2009 -2024- Simplilearn Solutions. Follow us! Company Work with us Discover For Businesses Learn On the Go! Trending Post Graduate Programs Trending Master Programs Trending Courses Trending Categories Trending Resources"}
{"url": "https://learn.microsoft.com/en-us/training/modules/fundamentals-machine-learning/", "title": "Fundamentals of machine learning - Training | Microsoft Learn", "content": "This browser is no longer supported. Upgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support. Fundamentals of machine learning Machine learning is the basis for most modern artificial intelligence solutions. A familiarity with the core concepts on which machine learning is based is an important foundation for understanding AI. \n\t\t\t\t\t\t\t\tLearning objectives\n\t\t\t\t\t\t\t After completing this module, you will be able to: Prerequisites Before starting this module, you should have: \n\t\t\t\t\t\t\t\t\t\tModule Assessment Results Assess your understanding of this module. Sign in and answer all questions correctly to earn a pass designation on your profile."}
{"url": "https://www.blog.trainindata.com/machine-learning-fundamentals/", "title": "Machine Learning Fundamentals - Train in Data's Blog", "content": "This site uses cookies We use cookies to recognize your repeated visits and preferences, to measure the effectiveness of our blogs and find out if users find what they're searching \t\tfor. By continuing using this site, you consent to the use of these cookies. Learn More. Machine Learning Fundamentals  by Priyansh Soni | Mar 11, 2024 | Machine Learning At the heart of the digital revolution lies machine learning\u2014a powerful tool shaping the future of innovation. At its core, machine learning allows computers to learn from data and make decisions without explicit programming. Understanding the fundamentals of machine learning is the first step towards starting your journey into the fields of machine learning and data science. It is crucial for effectively solving real-world problems using appropriate techniques and models, enabling accurate evaluation, troubleshooting, and fostering innovation in the field. So let\u2019s dive into the fundamentals of machine learning. \u00a0 What is Machine Learning? In today\u2019s digital age, machines can perform tasks that were once thought to be solely in the realm of human expertise. How can machines carry out these tasks? Thanks to machine learning. Machine learning is a field of computer science that consists of developing procedures that enable computers to learn from data without being explicitly programmed. These procedures are called algorithms. In simple terms, machine learning allows computers to learn from data and make decisions based on what they\u2019ve learned. It\u2019s like teaching a computer to recognize faces in photos, understand spoken language, translate texts, or even play games like chess or Go\u2014all without being explicitly programmed to do so. \u00a0 Machine Learning, Deep Learning and Artificial intelligence You probably hear these terms a lot. How are they related? Machine learning is a subset of artificial intelligence that focuses on developing algorithms that enable computers to learn from data. Deep learning is a specific type of machine learning that uses neural networks to learn complex patterns in data. Artificial neural networks are modeled after the workings of the human brain.. It is said that a neural network can approximate, i.e., learn, any mathematical function, and therefore, their learning potential is enormous.  Artificial intelligence (AI) is a broader concept involving any technique or system that tries to mimic human intelligence. That includes machine learning and deep learning as specific approaches within the field.  Data science is an interdisciplinary field that employs scientific methods and machine learning algorithms to extract insights and knowledge from structured and unstructured data. \u00a0 What Can Machine Learning Do? Machine learning allows computers to recognize patterns in data, understand language, identify objects in images or videos, make recommendations, and predict future outcomes based on past data. In fact, machine learning is revolutionizing numerous industries with its ability to analyze vast amounts of data and extract valuable insights. These are some of the key applications: \u00a0 Natural Language Processing (NLP) NLP enables computers to understand, interpret, and generate human language. Thanks to NLP, computers can detect sentiment, translate and make text summaries. Generative AI consists of more modern algorithms that allow computers to return human-like text. These algorithms are called \u201cGenerative Pretrained Transformers\u201d or GPT. They are the state-of-the-art models for NLP. \u00a0 Computer Vision Computer vision teaches computers to interpret and analyze information from images and videos. It enables machines to \u201csee\u201d and \u201cunderstand\u201d the world. Computer vision is used in facial recognition for security systems and authentication, and in self-driving cars for detecting pedestrians, traffic signs, and other objects on the road. Additionally, it\u2019s used in healthcare for diagnosing diseases from X-ray images and MRI scans. \u00a0 Predictive Analytics Predictive analytics empowers computers to learn patterns from past data, and use them to forecast future trends, behaviors, or outcomes. Data scientists use predictive analytics across industries, for example, to detect fraud, assess credit risk, understand and anticipate customer churn, forecast energy demand, and optimize the supply chain, among many other applications. \u00a0 Recommendation Systems Recommendation systems are algorithms that analyze user preferences. They analyze past behavior, for example, past purchases, viewed films, or listened to and liked songs, to suggest personalized content, products, or services that the customer might be interested in. Recommender systems are used in streaming services like Spotify or Netflix and in e-commerce like Amazon. \u00a0 Speech Recognition Speech recognition involves converting spoken language into text. Once it is in the form of text, we can use NLP to allow computers to understand it. Speech recognition is used in virtual assistants and customer services to understand and respond to users and customers. \u00a0 Other Applications of Machine Learning Other areas, like robotics\u2014powered by reinforcement learning\u2014are common examples where machine learning plays a pivotal role. As you can see, machine learning offers boundless applications in the real world. These are possible thanks to different types of machine learning methodologies. What are these methodologies?  Types of Machine Learning Algorithms Machine learning methodologies can be broadly categorized into two main types: supervised learning and unsupervised learning. \u00a0 Supervised Learning Supervised learning involves training a model on labeled data, where each input is associated with an output. The goal of supervised learning is to learn a mapping function from input variables to output variables. This allows the algorithm to make predictions or decisions when given new, unseen data. As we see in the diagram, initially we have a training set containing many observations, and each observation is labeled. Some are triangles, some are circles, and some are squares. We use that data to train a machine-learning algorithm. The model learns to match observations to shapes based on their characteristics. And later on, we can give new observations to the model, and it will be able to tell us which shape they have.  We can use supervised learning for regression and for classification. \u00a0 Regression Regression models predict continuous values. For example, predicting house prices based on features like square footage, number of bedrooms, and location is an example of a regression. Popular algorithms for regression are linear regression, polynomial regression, decision tree regression, random forest regression, and support vector regression. \u00a0 Classification Classification models predict discrete outcomes, or categories. For instance, classifying emails as spam or non-spam based on their content is an example of classification. Popular algorithms for classification are Logistic Regression, Naive Bayes, Support Vector Machines, Decision Trees, Random Forest Classifiers, and K-Nearest Neighbors (KNN). \u00a0 Unsupervised Learning Unsupervised learning is a type of machine learning where the algorithm learns patterns and structures from unlabeled data. Unlike supervised learning, there are no predefined labels for unsupervised learning tasks. Instead, the algorithm seeks to discover hidden patterns or groupings within the data.  In the following diagram, we pass a dataset without labels to a machine learning model, which, by analyzing the intrinsic data patterns, learns to group observations based on their similarities:  Unsupervised learning has many applications. It can be used in clustering to find groups of similar observations. It can be used to simplify the data representation through dimensionality reduction. It can also be used to find anomalies. \u00a0 Clustering Clustering algorithms group similar data points together into clusters. The goal is to identify natural groupings or clusters in the data without any prior knowledge of their labels. The grouping is done by identifying similar patterns among variables. Clustering can be used, for example, in customer segmentation to group together customers with similar purchasing behaviors. Some machine learning techniques used for clustering are K-Means Clustering, Hierarchical Clustering, and DBSCAN. \u00a0 Dimensionality Reduction Dimensionality reduction techniques aim to reduce the number of features in a dataset while preserving its essential information. Principal Component Analysis is a popular dimensionality reduction technique that projects high-dimensional data into a lower dimension while preserving as much information as possible. This can help visualize and analyze complex datasets more effectively. \u00a0 Anomaly Detection Anomaly detection with unsupervised learning involves identifying unusual patterns or outliers in data without labeled examples. By analyzing the inherent structure and distribution of the data, unsupervised learning algorithms detect deviations or irregularities that stand out from the typical patterns, thus flagging potential anomalies. Anomaly detection can be done by clustering and finding observations that do not fit in any cluster, by determining distributions and flagging outliers, or by using specific machine learning techniques, like one-class support vector machines or isolation forests. \u00a0 Fundamentals of Machine Learning As you can see, machine learning has many applications in the real world, thanks to different types of machine learning methodologies. However, the fundamentals of machine learning are the same across applications and algorithms. These machine learning basics include key components such as data, algorithms, training, testing, and evaluation techniques, which are essential for building effective models that generalize well to new, unseen data. Let\u2019s flesh these components out one by one. \u00a0 Data in Machine Learning Machines learn patterns, make predictions, and generate insights from data. Data is essential for model performance, decision-making, and optimization. In fact, the field of data science is devoted to analyzing, processing, and preparing data, either for machine learning or to extract insight to drive decisions. Data comes in many forms. We can have tables with numbers, images, or text. Images and texts are self-explanatory. Tabular data, however, has different flavors. Let\u2019s discover some of them. \u00a0 Features and Labels Tabular data comes in the form of tables, where each row is an observation and each column is a feature or attribute. Features, also called variables, are individual measurable properties or characteristics of the data being analyzed. For example, height is a feature, weight is another feature, as is color, vehicle make, city of residence, and so on. Features serve as input variables for machine learning algorithms and can be numeric, categorical, or binary in nature. They provide the information necessary for the algorithm to learn patterns and make predictions or decisions.  Labels, also known as targets or responses, are the outcomes or values we want to predict. In a dataset of house prices, features and variables may include square footage, number of bedrooms, and location, while the label would be the actual sale price of the house.  \u00a0 Numerical and Categorical Data Numerical data consists of numerical values that represent quantities or measurements. Examples of numerical variables are the number of rooms in a house, the median income, and the blood pressure, among others.  Categorical data consists of categories or labels that represent qualitative attributes or characteristics. Examples of categorical features are gender, marital status, vehicle make, city of residence, and so on. \u00a0 Data Preprocessing The data that is collected either by automated sensors, machines, or systems is not suitable in its raw format to train machine learning models. Instead, data scientists devote a lot of time to preparing data to train machine learning models. Data preprocessing is done to convert the raw data into a processable form that can be fed to a machine-learning model for training and making predictions. In fact, data preprocessing is the initial step in data analysis and machine learning projects. Data preprocessing includes among other things, the following:  Exploratory Data Analysis Data preprocessing goes hand in hand with exploratory data analysis (EDA). Through EDA, data scientists seek to understand data patterns, correlations, and trends to gain insights into the structure, characteristics, and relationships between features. Visualizations, graphs, and plots are actively used during EDA. This step is crucial for data-driven decision-making and hypothesis-testing. EDA also aids in creating predictive features and optimizing model performance.  Training and Validation Data Sets After data preprocessing and EDA, we are ready to start training machine learning models. To train machine learning models, we typically split the original data set into two or more sets: a training set, a testing set, and a validation set. \u00a0 Training data The training dataset is used to train the machine learning model by adjusting its parameters based on the input features and corresponding target labels. \u00a0 Validation data This set is used to evaluate and adjust a model during training. It acts like pseudo-test data, which provides an independent measure of how well the model generalizes to new data and makes adjustments to improve its effectiveness.  Test data This set is used to evaluate the final performance of a trained machine learning model, providing independent examples with input features and target labels that the model has not seen during training or validation. It serves as an unbiased measure to assess the model\u2019s effectiveness in real-world scenarios. \u00a0 Model Training With the data ready, it is time to train and evaluate the machine learning models. Model training involves feeding the training data into a machine learning algorithm to adjust its parameters and optimize its performance. During model training and evaluation, it\u2019s important to watch out for two common pitfalls: overfitting and underfitting. \u00a0 Overfitting & Underfitting Overfitting occurs when a model learns the training data too well, capturing noise or irrelevant patterns that do not generalize to new data. This leads to poor performance on unseen data. Underfitting occurs when a model is too simplistic to capture the underlying patterns in the data. This leads to poor performance both on the training and test datasets. \u00a0 Bias & Variance Overfitting and underfitting are related to the trade-off between bias and variance in model performance. Bias refers to the error due to overly simplistic assumptions in the model, and variance relates to the model\u2019s sensitivity to fluctuations in the training data. Bias represents the error introduced by the model\u2019s assumptions or simplifications. High-bias models underfit the data, leading to poor performance on both training and test datasets. Variance represents the sensitivity of our model to a given data point. High-variance models may overfit the data, capturing noise or irrelevant patterns and failing to generalize to new data. A good model should strike a balance between bias and variance, known as the \u201cbias-variance tradeoff.\u201d  Hyperparameters Hyperparameters are like settings or configurations that govern how a machine learning model operates. These parameters are not learned from the data but are rather adjusted to control the learning of a model. Hyperparameters can be considered like the knobs of the machine learning model, which we can adjust to make changes to how the model fits the data. Examples of hyperparameters are the maximum depth of a decision tree, the number of trees in a random forest, or the kernel type in SVM. Methods like grid search and random search are used to find the optimal values for these hyperparameters in a process called hyperparameter optimization to achieve the best performance from a model.  Cross-validation Cross-validation is a technique used to assess the performance and generalization ability of machine learning models. It involves dividing the dataset into multiple subsets, training the model on different combinations of these subsets, and evaluating its performance on the remaining data, aiding in obtaining a more reliable estimate of the model\u2019s performance. K-fold cross-validation is a popular cross-validation technique. The dataset is divided into K equal-sized subsets (folds). The model is trained K times, each time using K-1 folds for training and the remaining fold for validation. This ensures that each data point is used for validation exactly once. The final performance is calculated by averaging the results from the K validation runs. \u00a0 Model Evaluation To assess the performance of a model, we use evaluation metrics. These metrics measure the error in the model\u2019s predictions. \u201cError\u201d in machine learning refers to the difference between the predicted values generated by a model and the actual values observed in the dataset. The smaller the error, the better the performance of the model. There are evaluation metrics for regression and for classification models. \u00a0 Regression Metrics There are several metrics that help us determine the performance of a regression model. Here, I describe the most common ones. Mean Squared Error (MSE): Measures the average squared difference between the predicted and actual values. A smaller MSE indicates better model performance. Root Mean Squared Error (RMSE): Similar to MSE but takes the square root of the average squared difference. It\u2019s easier to interpret since it\u2019s in the same units as the target variable. Mean Absolute Error (MAE): Measures the average absolute difference between the predicted and actual values. It provides a more interpretable measure of error compared to MSE. R-squared: Indicates how well the independent variables in a regression model explain the variation in the dependent variable. R-qsuared values vary between 0 and 1, with higher values indicating better model fit.  Classification Metrics These are the most common evaluation metrics for classification: Accuracy: Measures the proportion of correctly classified instances. Precision: measures the proportion of true positive predictions out of all positive predictions made by the model. It focuses on the accuracy of positive predictions. Recall: Measures the proportion of true positive predictions out of all actual positive instances in the dataset. It focuses on the model\u2019s ability to capture all positive instances. F1 Score: The harmonic mean of precision and recall, the F1 score provides a balance between precision and recall. ROC Curve (Receiver Operating Characteristic Curve): A graphical plot that illustrates the trade-off between true positive rate (TPR) and false positive rate (FPR) across different threshold values. The higher the area under the ROC curve, the better the performance. Confusion matrix: a table that summarizes the performance of a classification model by comparing actual and predicted class labels. It provides insights into the model\u2019s true positive, true negative, false positive, and false negative predictions.  Conclusion Machine learning is revolutionizing how we approach digital challenges. It empowers computers to learn autonomously, uncover patterns in data, and transform industries with predictive insights. By grasping the machine learning basics, we open doors to endless possibilities, enabling collaboration between humans and machines for a brighter, more innovative future. We\u2019ve gathered a list of data science and machine learning courses and machine learning books that can get you started on your journey.  If you\u2019ve already taken your first steps, you can boost your skills with our advanced machine learning courses. Happy learning and good luck! Follow us\u00a0 \ud83d\udc47 Pages Courses Books Legal Privacy Policy Terms of use Impressum Our reviews on Trustpilot \u00a9 Train in Data 2024"}
{"url": "https://developer.ibm.com/articles/cc-machine-learning-deep-learning-architectures/", "title": "IBM Developer", "content": ""}
{"url": "https://www.projectpro.io/article/deep-learning-architectures/996", "title": "8 Deep Learning Architectures Data Scientists Must Master", "content": "Project Library   Courses  Custom Project Path  Resources  8 Deep Learning Architectures Data Scientists Must Master From artificial neural networks to transformers, explore 8 deep learning architectures every data scientist must know. Deep learning architectures are at the forefront of transforming artificial intelligence (AI) by introducing innovative capabilities. These advanced structures, inspired by the human brain's neural networks, empower machines to comprehend, learn, and make independent decisions. Through intricate processing layers, deep learning models excel in deciphering complex patterns and adapting to sophisticated applications. Deep learning architectures have led to remarkable advancements in applications like image recognition and natural language processing, revolutionizing how machines interact with and interpret information. Their capacity to extract meaningful features from data and continually learn enables machines to evolve, promising a future where intelligent systems play a pivotal role in various aspects of our lives.\u00a0  Build CNN for Image Colorization using Deep Transfer Learning   Downloadable solution code | Explanatory videos | Tech Support  In this blog, we will explore the inner workings of popular deep-learning architectures in a beginner-friendly manner. You will learn about the evolution of these models over time and their diverse applications. The blog will also highlight recent developments in the industry, providing invaluable insights into the dynamic landscape of deep learning architecture. Table of Contents Introduction to Deep Learning Architectures Deep learning architectures have evolved significantly over time, driven by advancements in research, computational power, and data availability. Initially, simple architectures like perceptrons paved the way for more complex models such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs). Introducing dropout regularization, batch normalization, and attention mechanisms improved model performance. Recent trends include the rise of Transformer models for natural language processing (NLP) systems and the exploration of hybrid architectures combining neural networks with symbolic reasoning. Overall, the evolution has been characterized by continuous innovation, leading to increasingly powerful and versatile deep-learning architectures, as summarized in this post by Diji Jayakaran.  Before we unravel the nitty grits of complex deep learning architectures, let's take a step back and explore the fundamental components that form the backbone of these intelligent systems.  Key Components of Deep Learning Architectures The journey of exploring the world of deep learning architectures is like navigating a complex maze. However, fear not, as we will begin our exploration with a quick revision to the simplest form of a neural network: perceptron. A perceptron makes binary decisions \u2013 a simple \"yes\" or \"no\" based on the weighted sum of input features. We recommend you dive into the details of this model in this Neural Network blog. Here is a quick recap that will prepare you for the details of the deep learning architectures we will soon discuss.  Neurons Neons are at the heart of any neural network, including the perceptron. These digital entities receive inputs, apply weights, and produce an output. Neurons are the building blocks through which information flows in a neural network. Just as neurons in our brains communicate, these fundamental blocks communicate in the language of numbers. Activation Functions Activation functions, such as the step function in the perceptron, determine whether the neuron fires. In other words, they decide whether the information flowing through should be allowed to contribute to the output. Picture it as a threshold\u2014if the incoming data is above a certain level, the perceptron 'fires' or produces an output; otherwise, it remains silent. This binary decision-making process showcases the essence of activation functions in shaping the output of our digital neurons. Weights and Biases Weights and biases are the adjustable parameters in the network that influence the importance of input features and establish a threshold for activation. Each input data feature is given a certain weight, indicating its importance in the decision. Biases act as the minimum requirement for a feature to contribute. Adjusting weights and biases during model training refines its ability to make accurate predictions. Loss Functions Loss functions quantify the difference between the predicted output and the actual target. The objective is to minimize this difference during the training process. In the context of the perceptron, the concept of error or loss becomes evident. The perceptron learns by reducing the difference between its prediction and the actual target, laying the groundwork for understanding more sophisticated loss functions in advanced neural networks. Mean Squared Error (MSE) and Cross-Entropy Loss are examples of loss functions commonly used in different use cases.. Optimizers Although more straightforward in the perceptron context, optimizers are crucial for adjusting weights and biases based on the computed loss. They fine-tune the model parameters to minimize the loss and improve overall performance. This mechanism hints at the broader optimization techniques in more complex deep learning architectures. Popular optimizing techniques include Stochastic Gradient Descent (SGD), Adam, and RMSprop. Understanding these foundational deep learning concepts will help us explore more complex deep learning architectures, where multiple layers of neurons interact to solve intricate problems. Now that you are thoroughly familiar with the basics, let's explore the different types of deep learning architectures. Check Out ProjectPro's Deep Learning Course to Gain Practical Skills in Building and Training Neural Networks! \n    Here's what valued users are saying about ProjectPro\n \n                    Gautam Vermani \n                 \n                    Data Consultant at Confidential                 \n                    Ed Godalle \n                 \n                    Director Data Analytics at EY / EY Tech                 \n        Not sure what you are looking for?\n     Deep Learning Architectures Types This section will discuss different deep learning architectures used for AI applications. We will start with the simplest architectures and gradually move to the complex ones. Artificial Neural Networks Artificial Neural Networks (ANNs) serve as the foundational architecture in deep learning. At its core, an ANN is characterized by layers of interconnected nodes comprising an input layer, one or more hidden layers, and an output layer. Information flows in a unidirectional manner, progressing from the input layer through the hidden layers to generate the final output.  Each node in the network represents a neuron and is connected to nodes in the adjacent layers. These connections are associated with weights, which determine the strength of the connection. The input data is fed into the network, and as it passes through each layer, the weighted sum is computed, and an activation function determines the output of each node. This process continues until the final layer produces the network's output. Next, the backpropagation technique improves the model's performance and accuracy. Backpropagation, a vital concept, facilitates learning by adjusting these weights based on prediction errors. Input data traverses the network, accumulating weighted sums at each layer, where activation functions govern node outputs. This iterative process reaches its peak in the final output layer, generating the network's output. Video: https://www.youtube.com/watch?v=5bxZ4hd7wcA\u00a0 The advantages of ANNs lie in their simplicity, ease of implementation, and the ability to model complex relationships within data. Their capacity to learn from large datasets enables them to generalize well to new, unseen data, making them a preferred choice in various real-world scenarios.\u00a0 Applications ANNs find applications in diverse fields, showcasing their adaptability and effectiveness. They excel in pattern recognition systems like image and speech recognition. The layered structure allows them to learn hierarchical representations of features, making them adept at discerning intricate patterns within data. These networks are widely employed in classification problems, where the goal is to assign inputs to specific categories, and regression problems, where the network predicts numerical values. Applications range from predicting stock prices to classifying emails as spam or non-spam. One of the key strengths is their capability to approximate any continuous function. This makes ANNs versatile, enabling them to model complex relationships in diverse domains. Video: https://www.youtube.com/watch?v=IyxfkXOxRrA\u00a0 Convolutional Neural Networks (CNNs) A Convolutional Neural Network (CNN) is a robust architecture for image processing, feature learning, and classification projects. It comprises three essential layers and is tailored to efficiently process input image data, making it indispensable in various applications.  Convolutional Layer (CONV) At the core of CNNs, convolutional layers execute crucial operations. These layers utilize kernels or filters to perform convolution operations, adjusting horizontally and vertically based on the stride rate. The convolutional layer incorporates non-linear activation functions, with Rectified Linear Unit (ReLU) being the most widely used. ReLU enhances the network's ability to capture complex patterns by introducing non-linearity. Pooling Layer (POOL) The pooling layer, responsible for dimensionality reduction, minimizes computational requirements while retaining essential features. Two common types are maximum pooling, which selects the maximum value within a kernel area, and average pooling, which calculates the average values within the kernel. Pooling enhances the network's robustness and helps manage computational resources efficiently. Fully Connected Layers (FC) Operating on a flattened input, the fully connected layer connects each input to every neuron, initiating the classification process. Typically located near the network's end, FC layers perform mathematical operations, finalizing the classification task. Video: https://www.youtube.com/shorts/5bxZ4hd7wcA\u00a0 Beyond these layers, additional components contribute to CNN functionality. The activation function influences the classification outcome, especially in the last fully connected layer. The softmax function, often employed, normalizes output values to represent target class probabilities. Dropout layers prevent overfitting during training, nullify specific neurons' contributions, and promote more robust learning across different training data batches. Understanding these building blocks forms the basis for exploring popular CNN architectures such as ImageNet, VGG-16, VGG-19, etc., each tailored to specific projects within the expansive domain of deep learning algorithms. Applications Convolutional Neural Networks (CNNs) are pivotal in diverse applications, revolutionizing image and pattern recognition. In computer vision, CNNs excel in object detection, facial recognition, and autonomous vehicle navigation systems. Their prowess extends to medical imaging, aiding in diagnosis through accurate analysis of medical scans. In natural language processing, CNNs contribute to sentiment analysis and language translation. Additionally, CNNs find applications in fields such as finance for fraud detection and robotics to enhance perception capabilities. The adaptability and efficiency of CNNs make them indispensable across industries, shaping the forefront of innovative technological advancements. Video: https://www.youtube.com/watch?v=W9PkTnNywE0\u00a0 Recurrent Neural Networks (RNNs) Recurrent Neural Networks (RNNs) stand out in their ability to handle sequential data by introducing a memory element, allowing them to capture temporal dependencies. Unlike feedforward networks, RNNs possess connections that form loops, enabling them to retain information about previous inputs. This recursive architecture allows RNNs to process sequences that are well-suited for speech recognition, language modeling, and time-series prediction problems. However, traditional RNNs have limitations in capturing long-term dependencies. Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) architectures address this issue, enhancing RNNs' efficacy in handling and learning from sequential data.  Applications In Natural Language Processing (NLP), RNNs shine in language modeling, machine translation, and sentiment analysis. Their ability to understand and generate sequences makes them invaluable in context comprehension problems. Additionally, RNNs are significant in time-series analysis, forecasting trends in financial markets, predicting stock prices, and analyzing physiological data. The recurrent nature of these networks enables them to model dependencies over time, providing accurate predictions and insights in dynamic and sequential data scenarios. The versatility of RNNs positions them as essential tools in extracting meaningful patterns from temporal information across various domains. Long Short-Term Memory (LSTM)\u00a0 Long Short-Term Memory (LSTM) networks represent a crucial advancement in recurrent neural network (RNN) architecture, specifically tailored to overcome the challenges posed by traditional RNNs in capturing long-range dependencies within sequential data. LSTMs excel in tasks such as speech recognition, language modeling, and more, offering several key features: Memory Cells LSTMs are equipped with specialized memory cells capable of retaining information over extended periods. This attribute allows LSTMs to capture and maintain context and dependencies crucial for understanding sequential data over long sequences. Gates LSTMs integrate three essential gates: the input gate, forget gate, and output gate. These gates play a pivotal role in regulating the flow of information into and out of the memory cell. The input gate manages the flow of new information into the memory cell, the forget gate decides which information to discard, and the output gate controls the information flow to the next layer. Vanishing Gradient Problem LSTMs effectively address the vanishing gradient problem encountered in traditional RNNs. By maintaining a constant error flow through time, LSTMs ensure the unhindered propagation of gradients during backpropagation. This capability facilitates better learning of long-term dependencies, enhancing the network's ability to capture intricate patterns in sequential data.  Applications LSTMs have emerged as a backbone in various applications that demand sequential data processing. Their unique ability to capture and retain intricate patterns and dependencies over extended sequences makes them indispensable in natural language processing, speech recognition, and time-series analysis. Incorporating memory cells and gating mechanisms sets LSTMs apart, enabling them to excel in scenarios where traditional RNNs struggle to capture the complexity of long-term dependencies. Gated Recurrent Unit (GRU) Gated Recurrent Units (GRUs) are a distinctive recurrent neural network (RNN) architecture designed to overcome challenges like the vanishing gradient problem and enhance the modeling of long-term dependencies in sequential training datasets. GRUs share similarities with Long-Short-Term Memory (LSTM) networks but feature a simpler structure for the learning process.  GRUs use a gating mechanism crucial for controlling information flow within the network. This mechanism comprises two gates: the update gate and the reset gate. The update gate determines the proportion of information to retain or discard from the previous state, while the reset gate regulates the resetting of the internal state. GRUs have a more straightforward architecture than LSTMs, featuring fewer parameters and computations. This simplicity increases efficiency in training time and computational resources, making GRUs advantageous for various applications. Applications GRUs find widespread use across diverse applications demanding the processing of sequential data: GRUs are extensively used in NLP applications, such as language modeling, sentiment analysis, and machine translation. Their ability to capture context and dependencies in sequential data makes them valuable for understanding and generating human-like language. In speech recognition systems, GRUs play a pivotal role in modeling sequential patterns, aiding in accurately recognizing spoken language. GRUs are well-suited to solve complex problems in time series prediction projects, forecasting future values based on historical data. Their efficiency in capturing temporal dependencies makes them valuable in financial forecasting, weather prediction, and other time-dependent domains. Despite their more straightforward structure, GRUs have effectively captured long-term dependencies. Their successful applications in natural language processing, speech recognition, and time series prediction underscore their versatility in handling sequential data across various domains. The balance between efficiency and performance makes GRUs compelling for problems requiring recurrent neural networks with streamlined architecture. Autoencoders Autoencoders are a specialized class of deep learning algorithms proficient in learning efficient representations of input data without explicit labels. Tailored for unsupervised learning, they focus on compressing and effectively representing input data through a two-fold structure comprising an encoder and a decoder.  Encoder The encoder transforms raw input data into a reduced-dimensional representation termed the \"latent space\" or \"encoding.\" This encoding captures essential features and patterns from the input data, facilitating the extraction of meaningful information. Bottleneck Layer Situated at the end of the encoder, the bottleneck layer drastically reduces the dimensionality of the input data. It represents a compressed encoding of the original information, essential for efficient representation learning. Decoder The decoder reconstructs the initial input data from the encoded representation. Its objective is to rebuild the input as accurately as possible from the compressed encoding generated by the encoder, effectively performing the inverse operation. Autoencoders have emerged as a potent tool in deep learning, offering a unique approach to unsupervised learning. Their versatility extends to domains like image processing, anomaly detection, and beyond, showcasing their value in diverse applications across the field of artificial intelligence. Autoencoders are extensively used for dimensionality reduction tasks, compressing high-dimensional data into lower-dimensional representations while retaining crucial information. They excel in anomaly detection systems, identifying deviations from standard patterns by comparing reconstructed data with original inputs. Autoencoders find application in data denoising tasks, cleaning up noisy images or audio data by reconstructing clean versions from corrupted inputs.\u00a0 Generative Adversarial Networks (GANs) Generative Adversarial Networks (GANs) are a class of deep neural networks designed for generating new, realistic data samples. Introduced by Ian Goodfellow and his colleagues in 2014, GANs have become a powerful and widely used technique in machine learning and deep learning. The key idea behind GANs is to pit two neural networks against each other in a kind of adversarial game, leading to the generation of high-quality, realistic data. A generative adversarial network (GAN) system consists of a generator and a discriminator network engaged in adversarial training. The generator analyzes and modifies data attributes, while the discriminator evaluates the generated output's authenticity. Through iterative training, the generator aims to maximize the discriminator's error, while the discriminator seeks to minimize errors. This dynamic interplay continues until an equilibrium is reached, rendering the discriminator unable to distinguish between real and synthesized data. This signifies the completion of the training process, illustrating GANs' iterative, competitive nature in producing realistic data.  Generative Adversarial Networks (GANs) encompass various types tailored for specific applications. Vanilla GANs involve basic adversarial training, while Conditional GANs incorporate additional conditional information. Deep Convolutional GANs (DCGAN) use convolutional networks for image generation. Wasserstein GANs (WGAN) employ Wasserstein distance for stability. Progressive GANs (PGAN) gradually increase image resolution. CycleGAN enables unpaired image-to-image translation, and StyleGAN focuses on style control. Numerous other variants, such as InfoGAN and BigGAN, address diverse challenges, showcasing GANs' versatility in generating realistic data for various purposes.\u00a0 Applications A few typical applications of GANs include: GANs are widely used to generate high-quality, realistic images. They have been applied to create faces, objects, artwork, and even deepfake videos. GANs can be used for training data augmentation. This helps improve model generalization by exposing it to a more diverse set of examples. GANs can be employed for style transfer, allowing an image to transform its style while preserving its content. Transformer Architecture The Transformer model is a revolutionary encoder-decoder architecture that redefined machine learning, particularly in natural language processing (NLP). It was introduced by Vaswani et al. in 2017 and has revolutionized sequence processing problems in NLP by replacing recurrent layers with self-attention mechanisms. In a traditional RNN, each element in a sequence is processed one at a time. In contrast, self-attention allows the model to weigh the importance of different elements in the sequence concerning each other. It also enables parallelization of computation, as each component can attend to all others simultaneously. This results in faster training compared to sequential processing.  Applications Transformers have become the go-to architecture for various natural language processing (NLP) projects, such as machine translation, sentiment analysis, and text summarization. The attention mechanism in Transformers allows the model to consider each word's context in a sentence when making predictions. This helps in capturing long-range dependencies in language. BERT, a pre-trained Transformer-based model, performs well on various NLP benchmarks. It considers both left and right context during pre-training, improving contextual understanding. Having explored so many architectures of deep learning algorithms, it is time to compare them and summarize their key differences. Comparison of Deep Learning Architectures in Python Let's compare the strengths and weaknesses of different architectures based on factors such as performance, training time, and resource requirements. Architecture Strengths  Weaknesses  Performance Training Time Resource Requirements Convolutional Neural Networks (CNNs) Excell in image-related projects Require substantial data for optimal performance High accuracy in image-related projects Moderate training time Moderate to high GPU requirements Recurrent Neural Networks (RNNs) Effective for sequential data Sensitive to vanishing/exploding gradient problems Strong in projects involving time dependencies Longer training time for deep networks Moderate GPU requirements Long Short-Term Memory (LSTM) Mitigate vanishing/exploding gradient issues Higher computational demands compared to basic RNNs Efficient in capturing long-term dependencies Longer training time for complex networks Moderate to high GPU requirements Gated Recurrent Unit (GRU) Simplifies architecture compared to LSTM May not perform as well on certain projects Balances performance and computational cost Faster training than LSTM Moderate GPU requirements Autoencoders  Useful for unsupervised learning, data compression Sensitive to noise and outliers Efficient in learning data representations Moderate training time Moderate GPU requirements Generative Adversarial Networks (GANs) Exceptional at generating realistic data Prone to mode collapse, training instability High-quality data generation Sensitive to hyperparameter tuning High GPU requirements Transformer Architecture Enables efficient parallelization May struggle with very long sequences State-of-the-art in NLP projects Faster training due to parallelization Moderate to high GPU requirements Now that we've thoroughly reviewed deep learning algorithms and architectures, let's delve into their diverse applications across various domains of artificial intelligence. Deep Learning Architectures Applications Deep learning architectures have made significant strides in computer vision and natural language processing (NLP), revolutionizing how we perceive and understand visual and textual data.\u00a0  Deep Learning Architectures for Computer Vision Deep learning architectures have brought a new era of image understanding and analysis. Convolutional Neural Networks (CNNs) are the critical element in this domain, offering unparalleled performance in systems like image classification, object detection, and semantic segmentation. CNNs leverage hierarchical layers of learnable filters to extract meaningful features from images, enabling accurate recognition and classification. Recent advancements in deep learning architectures for image classification have further enhanced the capabilities of CNNs. Techniques like transfer learning, where pre-trained CNN models are fine-tuned on specific datasets, have democratized access to state-of-the-art image classification models. Additionally, architectures like ResNet, DenseNet, and EfficientNet have pushed the boundaries of accuracy and efficiency, showcasing the continuous evolution of deep learning in computer vision. Here are a few project ideas that you can practice and realize the significance of these multilayer neural network architectures. OpenCV Project for Beginners to Learn Computer Vision Basics\u00a0 Medical Image Segmentation Deep Learning Project\u00a0 Image Segmentation using Mask R-CNN with Tensorflow\u00a0 Build a Multi Class Image Classification Model Python using CNN OpenCV Project to Master Advanced Computer Vision Concepts\u00a0 Build a CNN Model with PyTorch for Image Classification\u00a0 Build CNN for Image Colorization using Deep Transfer Learning\u00a0 Deep Learning Architectures for Natural Language Processing Deep learning architectures have also transformed how we analyze and understand textual data. Recurrent Neural Networks (RNNs) and their variants, such as Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs), have been instrumental in projects like language modeling, sentiment analysis, and machine translation. These architectures excel at processing sequential data, capturing temporal dependencies and contextual information within text. Text Classification with Transformers-RoBERTa and XLNet Model\u00a0 Build a Text Classification Model with Attention Mechanism NLP\u00a0 NLP Project for Multi Class Text Classification using BERT Model\u00a0 Build Multi Class Text Classification Models with RNN and LSTM Deep learning architectures continue to drive innovation and breakthroughs in computer vision and natural language processing. From CNNs serving as deep learning architectures for image classification to RNNs for language understanding, these architectures represent the foundation upon which cutting-edge AI applications are built, shaping the future of technology and human-machine interaction. As we explored the evolution of deep learning architectures and their evolution, it's evident that these applications grow alongside algorithm updates. Let's explore the latest deep learning architectures and a few trends. Recent Trends in Deep Learning Architectures Several trends and breakthroughs, including the emergence of novel architectures and significant improvements in existing ones, have marked recent advancements in deep learning architectures. Some of the latest trends and developments include: Self-supervised learning has gained traction, particularly in computer vision and NLP. Models are trained to predict certain portions of the input data, leading to representations that capture rich semantic information. Contrastive learning and momentum contrast methods have shown promising results in this area. GANs continue to evolve with advancements in training stability and quality of generated samples. Techniques like progressive growing, spectral normalization, and self-attention mechanisms have generated high-fidelity images and diverse data samples. Capsule networks, inspired by the human visual system, offer a new perspective on representing hierarchical relationships within data. Recent research focuses on enhancing the robustness and interpretability of capsule networks for projects like image recognition and object detection. GNNs have gained prominence in analyzing structured data such as graphs and networks. Recent advancements include graph attention networks (GATs) and graph convolutional networks (GCNs), enabling effective relational information and node embedding modeling. Diffusion Convolutional Recurrent Neural Network (DCRNN) is tailored to enhance the efficiency of deep learning models, particularly in resource-constrained environments. It addresses issues like the vanishing gradient problem, enabling more effective utilization of computational resources. Federated Learning is an approach that facilitates collaborative model training across multiple devices without sharing raw data, mitigating privacy concerns and reducing computing and storage demands. Federated learning represents a paradigm shift in decentralized machine learning, offering scalable and privacy-preserving solutions. Explainable Artificial Intelligence (XAI) is dedicated to increasing the transparency and interpretability of machine learning models. By providing insights into model decision-making processes, XAI ensures fairness and impartiality, fostering trust and understanding between humans and AI systems. Hybrid architectures combining neural networks with symbolic reasoning or probabilistic models have emerged to address the limitations of purely data-driven approaches. These architectures aim to integrate the strengths of different paradigms for improved performance and interpretability. Keeping up with these trends might be challenging, and implementing them in your system could be even more complex. But don't worry\u2014in the next section, we've got a secret to share. Master Deep Learning Architectures with ProejctPro Yann LeCun, VP and Chief AI Scientist at Meta, rightfully underscores the importance of practical implementation beyond theory in this linkedin post.  If you are looking for one platform that guides you through the path of deep learning through real-world projects, then ProjectPro stands as the answer. ProjectPro offers a repository of solved projects in data science and big data crafted by industry experts. Through hands-on experience, subscribers grasp practical limitations and fundamentals. Continuously updated with the latest tech tools, ProjectPro ensures staying ahead of trends. The dynamic platform provides a comprehensive learning experience, bridging the gap between theory and practice. Don't hesitate\u2014subscribe today to ride on a journey of mastering deep learning architectures and keeping pace in the ever-evolving field of AI. FAQs 1. What is the architecture of the deep learning model? The architecture of a deep learning model refers to its structure, including the arrangement and connectivity of its layers, such as convolutional, recurrent, or dense layers, along with activation functions and connections between neurons. 2. Is CNN a deep learning architecture? Yes, CNN (Convolutional Neural Network) is a deep learning architecture commonly used in computer vision tasks, characterized by hierarchical layers of convolutional and pooling operations, followed by fully connected layers for classification or regression. 3. What are the 3 types of architecture of the neural network? The three popular types of architecture of neural networks are: Feed forward Neural Networks (FNNs) Recurrent Neural Networks (RNNs) Convolutional Neural Networks (CNNs) \n \u00a0 PREVIOUS NEXT  About the Author \n\t\t\t\t\t\t\t\t\tManika\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\tManika Nagpal is a versatile professional with a strong background in both Physics and Data Science. As a Senior Analyst at ProjectPro, she leverages her expertise in data science and writing to create engaging and insightful blogs that help businesses and individuals stay up-to-date with the\t\t\t\t\t\t\t\t Meet The Author  Start Your First Project Learn By Doing Related Blogs on Deep Learning Projects Trending Blog Categories Project Categories Projects Blogs Certification Courses Tutorials \nProjectPro\n\n\n\n \u00a9 2024  \u00a9 2024 Iconiq Inc. About us Contact us Privacy policy User policy Write for ProjectPro"}
{"url": "https://addepto.com/blog/deep-learning-architecture/", "title": "Deep Learning Architecture Examples - Addepto", "content": "Deep Learning Architecture Examples Author: CSO & Co-Founder Reading time: As you know from our previous article about machine learning and deep learning, DL is an advanced technology based on neural networks that try to imitate the way the human cortex works. Today, we want to get deeper into this subject. You have to know that neural networks are by no means homogenous. In fact, we can indicate at least six types of neural networks and deep learning architectures that are built on them. In this article, we are going to show you the most popular and versatile types of deep learning architecture. Soon, abbreviations like RNN, CNN, or DSN will no longer be mysterious.  First of all, we have to state that deep learning architecture consists of deep/neural networks of varying topologies. The general principle is that neural networks are based on several layers that proceed data\u2013an input layer (raw data), hidden layers (they process and combine input data), and an output layer (it produces the outcome: result, estimation, forecast, etc.). Thanks to the development of numerous layers of neural networks (each providing some function), deep learning is now more practical. It\u2019s a bit like a machine learning framework\u2013it allows you to make more practical use of this technology, accelerates your work, and enables various endeavors without the need to build an ML algorithm entirely from scratch. When it comes to deep learning, you have various types of neural networks. And deep learning architectures are based on these networks. Today, we can indicate six of the most common deep learning architectures: Don\u2019t worry if you don\u2019t know these abbreviations; we are going to explain each one of them. Let\u2019s start with the first one. What Are the Different Deep Learning Architectures RNN: Recurrent Neural Networks (RNNs) RNN is one of the fundamental network architectures from which other deep learning architectures are built. RNNs consist of a rich set of deep learning architectures. They can use their internal state (memory) to process variable-length sequences of inputs. Let\u2019s say that RNNs have a memory. Every processed information is captured, stored, and utilized to calculate the final outcome. This makes them useful when it comes to, for instance, speech recognition[1]. Moreover, the recurrent network might have connections that feedback into prior layers (or even into the same layer). This feedback allows them to maintain the memory of past inputs and solve problems in time. RNNs are very useful when it comes to fields where the sequence of presented information is key. They are commonly used in NLP (i.a. chatbots), speech synthesis, and machine translations. Currently, we can indicate two types of RNN:  LSTM: Long Short-Term Memory It\u2019s also a type of RNN. However, LSTM has feedback connections. This means that it can process not only single data points (such as images) but also entire sequences of data (such as audio or video files)[3]. LSTM derives from neural network architectures and is based on the concept of a memory cell. The memory cell can retain its value for a short or long time as a function of its inputs, which allows the cell to remember what\u2019s essential and not just its last computed value. A typical LSTM architecture is composed of a cell, an input gate, an output gate, and a forget gate. The cell remembers values over arbitrary time intervals, and these three gates regulate the flow of information into and out of the cell. Today, LSTMs are commonly used in such fields as text compression, handwriting recognition, speech recognition, gesture recognition, and image captioning[4]. GRU This abbreviation stands for Gated Recurrent Unit. It\u2019s a type of LSTM. The major difference is that GRU has fewer parameters than LSTM, as it lacks an output gate[5]. GRUs are used for smaller and less frequent datasets, where they show better performance.  CNN: Convolutional Neural Networks (CNNs) This architecture is commonly used for image processing, image recognition, video analysis, and NLP. CNN can take in an input image, assign importance to various aspects/objects in the image, and be able to differentiate one from the others[6]. The name \u2018convolutional\u2019 derives from a mathematical operation involving the convolution of different functions. CNNs consist of an input and an output layer, as well as multiple hidden layers. The CNN\u2019s hidden layers typically consist of a series of convolutional layers. Here\u2019s how CNNs work: First, the input is received by the network. Each input (for instance, image) will pass through a series of convolution layers with various filters. The control layer controls how the signal flows from one layer to the other. Next, you have to flatten the output and feed it into the fully connected layer where all the layers of the network are connected with every neuron from a preceding layer to the neurons from the subsequent layer. As a result, you can classify the output.  DBN: Deep Belief Network DBN is a multilayer network (typically deep, including many hidden layers) in which each pair of connected layers is a Restricted Boltzmann Machine (RBM). Therefore, we can state that DBN is a stack of RBMs. DBN is composed of multiple layers of latent variables (\u201chidden units\u201d), with connections between the layers but not between units within each layer[7]. DBNs use probabilities and unsupervised learning to produce outputs. Unlike other models, each layer in DBN learns the entire input. In CNNs, the first layers only filter inputs for basic features, and the latter layers recombine all the simple patterns found by the previous layers. DBNs work holistically and regulate each layer in order. DBNs can be used i.a. in image recognition and NLP. DSN: Deep Stacking Network We saved DSN for last because this deep learning architecture is different from the others. DSNs are also frequently called DCN\u2013Deep Convex Network. DSN/DCN comprises a deep network, but it\u2019s actually a set of individual deep networks. Each network within DSN has its own hidden layers that process data. This architecture has been designed in order to improve the training issue, which is quite complicated when it comes to traditional deep learning models. Thanks to many layers, DSNs consider training, not a single problem that has to be solved but a set of individual problems. According to a paper \u201cAn Evaluation of Deep Learning Miniature Concerning in Soft Computing\u201d[8] published in 2015, \u201cthe central idea of the DSN design relates to the concept of stacking, as proposed originally, where simple modules of functions or classifiers are composed first and then they are stacked on top of each other in order to learn complex functions or classifiers.\u201d Typically, DSNs consist of three or more modules. Each module consists of an input layer, a hidden layer, and an output layer. These modules are stacked one on top of another, which means that the input of a given module is based on the output of prior modules/layers. This construction enables DSNs to learn more complex classification than it would be possible with just one module. These six architectures are the most common ones in the modern deep learning architecture world. At this point, we should also mention the last, and considered the most straightforward, architecture. Let\u2019s talk for a second about autoencoders.  \u00a0 Transformer The Transformer is a powerful deep learning architecture that has significantly impacted the field of natural language processing (NLP). It was first introduced in a 2017 paper by Google researchers and has since become a cornerstone in various advanced language models. Unlike traditional models that rely on Recurrent Neural Networks (RNNs) for sequential information extraction, Transformers leverage self-attention mechanisms to understand context and relationships between different elements in a sequence. Key points about the Transformer architecture include: The Transformer\u2019s ability to capture complex relationships in data, its parallel processing capabilities, and its impact on various NLP tasks make it a fundamental architecture in modern deep learning research, driving advancements in language understanding and generation. Generative Adversarial Networks (GANs) Generative Adversarial Networks (GANs) are a powerful class of deep learning models used for generative tasks, where they automatically learn and generate new data instances that resemble the original dataset. GANs consist of two primary components: Key points about GANs include: GANs have revolutionized generative modeling by enabling the creation of high-quality, realistic data that can be used in various domains such as image synthesis, content creation, and pattern recognition. Their ability to learn complex patterns and generate new data has made them a fundamental tool in the field of deep learning.  Read more: Deep Learning Applications  What Is the Architecture of the Deep Learning Model? Deep learning models, including various architectures like recurrent neural networks (RNN), convolutional neural networks (CNN), and deep belief networks (DBN), are structured in a specific manner to enable learning from complex data and making predictions or classifications. The architecture of a deep learning model typically consists of several interconnected layers, each serving a specific purpose in processing and transforming input data to generate useful outputs. Here\u2019s an overview of the architecture components: Input Layer This is the initial layer of the deep learning model where raw data is fed into the network. The number of neurons in this layer corresponds to the dimensionality of the input data. Each neuron represents a feature or attribute of the input data. Hidden Layers These are intermediate layers between the input and output layers where the actual processing of data occurs. Each hidden layer comprises multiple neurons, and each neuron performs a weighted sum of inputs followed by the application of an activation function. The number of hidden layers and neurons in each layer can vary depending on the complexity of the problem and the architecture of the model. Output Layer This is the final layer of the model where the predictions or classifications are generated. The number of neurons in the output layer depends on the nature of the task. For instance, in a binary classification task, there might be one neuron representing each class with a sigmoid activation function to produce probabilities. In a multi-class classification task, there would be one neuron per class with a softmax activation function. Connections and Weights Each neuron in a layer is connected to every neuron in the subsequent layer, forming a fully connected or dense network. These connections have associated weights that are learned during the training process. The weights determine the strength of the connections between neurons and are adjusted iteratively to minimize the difference between the model\u2019s predictions and the actual outputs. Activation Functions Activation functions introduce non-linearity into the model, enabling it to learn complex patterns and relationships in the data. Common activation functions include sigmoid, tanh, ReLU (Rectified Linear Unit), and softmax. Loss Function The loss function measures the difference between the model\u2019s predictions and the actual targets. It serves as the objective function during training, guiding the optimization process to minimize prediction errors. The choice of loss function depends on the nature of the task, such as mean squared error for regression tasks and categorical cross-entropy for classification tasks. Optimization Algorithm Optimization algorithms, such as stochastic gradient descent (SGD), Adam, or RMSprop, are used to update the weights of the model iteratively based on the gradients of the loss function with respect to the weights. These algorithms aim to find the optimal set of weights that minimize the loss function. \u00a0 Overall, the architecture of a deep learning model is designed to efficiently process and learn from complex data, enabling it to make accurate predictions or classifications on unseen examples. The effectiveness of the architecture depends on various factors, including the choice of layers, activation functions, optimization algorithms, and hyperparameters, which are often determined through experimentation and tuning. \u00a0 Deep Learning Architecture \u2013 Autoencoders Autoencoders are a specific type of feedforward neural network. The general idea is that the input and the output are pretty much the same. What does it mean? Simply put, Autoencoders condense the input into a lower-dimensional code. Based on this, the outcome is produced. In this model, the code is a compact version of the input. One of Autoencoders\u2019 main tasks is to identify and determine what constitutes regular data and then identify the anomalies or aberrations. Autoencoders comprise three components: Autoencoders are mainly used for dimensionality reduction and, naturally, anomaly detection (for instance, frauds). Simplicity is one of their greatest advantages. They are easy to build and train. However, there\u2019s also the other side of the coin. You need high-quality, representative training data. If you don\u2019t, the information that comes out of the Autoencoder can be unclear or biased.  Deep Learning Architecture \u2013 conclusion As you can see, although deep learning architectures are, generally speaking, based on the same idea, there are various ways to achieve a goal. That\u2019s why it\u2019s so important to choose deep learning architecture correctly. If you want to find out more about this tremendous technology, get in touch with us. With our help, your organization can benefit from deep learning architecture. Let us show you how! This article is an updated version of the publication from Jul, 21 2020.\u00a0 References [1] Wikipedia. Recurrent neural network. URL: https://en.wikipedia.org/wiki/Recurrent_neural_network. Accessed\u00a0 Jul 21, 2020. [2] Wikipedia. Bidirectional recurrent neural networks. URL: https://en.wikipedia.org/wiki/Bidirectional_recurrent_neural_networks. Accessed\u00a0 Jul 21, 2020. [3] Wikipedia. Long short-term memory. URL: https://en.wikipedia.org/wiki/Long_short-term_memory. Accessed\u00a0 Jul 21, 2020. [4] Samaya Madhavan. Deep learning architectures. Jan 25, 2021. URL: https://developer.ibm.com/technologies/artificial-intelligence/articles/cc-machine-learning-deep-learning-architectures/. Accessed\u00a0 Jul 21, 2020. [5] Wikipedia. Gated recurrent unit. URL: https://en.wikipedia.org/wiki/Gated_recurrent_unit. Accessed\u00a0 Jul 21, 2020. [6] Sumit Saha. A Comprehensive Guide to Convolutional Neural Networks \u2014 the ELI5 way. Dec 15, 2018. URL: https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53. Accessed\u00a0 Jul 21, 2020. [7] Wikipedia. Deep belief network. URL: https://en.wikipedia.org/wiki/Deep_belief_network. Accessed\u00a0 Jul 21, 2020. [8] Dr. Yusuf Perwej. An Evaluation of Deep Learning Miniature Concerning in Soft Computing. Feb 2015. URL: https://www.researchgate.net/figure/A-Deep-Stacking-Network-Architecture_fig1_272885058. Accessed\u00a0 Jul 21, 2020. \u00a0 Category: Machine Learning Consulting  Sign Up for Newsletter See our Privacy Policy Related articles Case Study: Addepto\u2019s AI Solutions for The Aviation Industry The collaboration between Addepto and a leading global aviation technology company exemplifies a successful long-term pa... Use Case: Leveraging Gen AI and Machine Learning in Investment Management with BlackRock\u2019s Thematic Robot The private investment sector presents unique challenges that general-purpose generative AI solutions have struggled to ... Machine learning and Deep Learning in Economics Although machine learning (ML) continues to gain interest among economists, there is still a lack of practical informati... AI & ML Churn Prediction. Calculate Customer Churn Prediction ROI Many companies use statistical models to optimize their activities. An example of this type of models are scoring system... \n                                    Addepto sp. z o.o.\n                                    \u015awieradowska 47, 02-662\n                                    Warsaw, Poland\n                                 AI-powered Knowledge Base Assistant for your business Document research, report generation, and code migration, is here to streamline and accelerate your entire knowledge base operations."}
{"url": "https://news.mit.edu/2017/explained-neural-networks-deep-learning-0414", "title": "Explained: Neural networks | MIT News | Massachusetts Institute of Technology", "content": "Suggestions or feedback? \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMIT News | Massachusetts Institute of Technology\n Browse By Topics Departments Centers, Labs, & Programs Schools Breadcrumb Explained: Neural networks \n  Press Contact:\n Media Download *Terms of Use: \n    Images for download on the MIT News office website are made available to non-commercial entities, press and the general public under a \n    Creative Commons Attribution Non-Commercial No Derivatives license.\n    You may not alter the images provided, other than to crop them to size. A credit line must be used when reproducing images; if one is not provided \n    below, credit the images to \"MIT.\" \n   \n\n\n\n\n\n\n\n\n\n\n\n\nPrevious image\nNext image\n\n\n\n\n\n\n\n\n\n\n\n\n\n In the past 10 years, the best-performing artificial-intelligence systems \u2014 such as the speech recognizers on smartphones or Google\u2019s latest automatic translator \u2014 have resulted from a technique called \u201cdeep learning.\u201d Deep learning is in fact a new name for an approach to artificial intelligence called neural networks, which have been going in and out of fashion for more than 70 years. Neural networks were first proposed in 1944 by Warren McCullough and Walter Pitts, two University of Chicago researchers who moved to MIT in 1952 as founding members of what\u2019s sometimes called the first cognitive science department. Neural nets were a major area of research in both neuroscience and computer science until 1969, when, according to computer science lore, they were killed off by the MIT mathematicians Marvin Minsky and Seymour Papert, who a year later would become co-directors of the new MIT Artificial Intelligence Laboratory. The technique then enjoyed a resurgence in the 1980s, fell into eclipse again in the first decade of the new century, and has returned like gangbusters in the second, fueled largely by the increased processing power of graphics chips. \u201cThere\u2019s this idea that ideas in science are a bit like epidemics of viruses,\u201d says Tomaso Poggio, the Eugene McDermott Professor of Brain and Cognitive Sciences at MIT, an investigator at MIT\u2019s McGovern Institute for Brain Research, and director of MIT\u2019s Center for Brains, Minds, and Machines. \u201cThere are apparently five or six basic strains of flu viruses, and apparently each one comes back with a period of around 25 years. People get infected, and they develop an immune response, and so they don\u2019t get infected for the next 25 years. And then there is a new generation that is ready to be infected by the same strain of virus. In science, people fall in love with an idea, get excited about it, hammer it to death, and then get immunized \u2014 they get tired of it. So ideas should have the same kind of periodicity!\u201d Weighty matters Neural nets are a means of doing machine learning, in which a computer learns to perform some task by analyzing training examples. Usually, the examples have been hand-labeled in advance. An object recognition system, for instance, might be fed thousands of labeled images of cars, houses, coffee cups, and so on, and it would find visual patterns in the images that consistently correlate with particular labels. Modeled loosely on the human brain, a neural net consists of thousands or even millions of simple processing nodes that are densely interconnected. Most of today\u2019s neural nets are organized into layers of nodes, and they\u2019re \u201cfeed-forward,\u201d meaning that data moves through them in only one direction. An individual node might be connected to several nodes in the layer beneath it, from which it receives data, and several nodes in the layer above it, to which it sends data. To each of its incoming connections, a node will assign a number known as a \u201cweight.\u201d When the network is active, the node receives a different data item \u2014 a different number \u2014 over each of its connections and multiplies it by the associated weight. It then adds the resulting products together, yielding a single number. If that number is below a threshold value, the node passes no data to the next layer. If the number exceeds the threshold value, the node \u201cfires,\u201d which in today\u2019s neural nets generally means sending the number \u2014 the sum of the weighted inputs \u2014 along all its outgoing connections. When a neural net is being trained, all of its weights and thresholds are initially set to random values. Training data is fed to the bottom layer \u2014 the input layer \u2014 and it passes through the succeeding layers, getting multiplied and added together in complex ways, until it finally arrives, radically transformed, at the output layer. During training, the weights and thresholds are continually adjusted until training data with the same labels consistently yield similar outputs. Minds and machines The neural nets described by McCullough and Pitts in 1944 had thresholds and weights, but they weren\u2019t arranged into layers, and the researchers didn\u2019t specify any training mechanism. What McCullough and Pitts showed was that a neural net could, in principle, compute any function that a digital computer could. The result was more neuroscience than computer science: The point was to suggest that the human brain could be thought of as a computing device. Neural nets continue to be a valuable tool for neuroscientific research. For instance, particular network layouts or rules for adjusting weights and thresholds have reproduced observed features of human neuroanatomy and cognition, an indication that they capture something about how the brain processes information. The first trainable neural network, the Perceptron, was demonstrated by the Cornell University psychologist Frank Rosenblatt in 1957. The Perceptron\u2019s design was much like that of the modern neural net, except that it had only one layer with adjustable weights and thresholds, sandwiched between input and output layers. Perceptrons were an active area of research in both psychology and the fledgling discipline of computer science until 1959, when Minsky and Papert published a book titled \u201cPerceptrons,\u201d which demonstrated that executing certain fairly common computations on Perceptrons would be impractically time consuming. \u201cOf course, all of these limitations kind of disappear if you take machinery that is a little more complicated \u2014 like, two layers,\u201d Poggio says. But at the time, the book had a chilling effect on neural-net research. \u201cYou have to put these things in historical context,\u201d Poggio says. \u201cThey were arguing for programming \u2014 for languages like Lisp. Not many years before, people were still using analog computers. It was not clear at all at the time that programming was the way to go. I think they went a little bit overboard, but as usual, it\u2019s not black and white. If you think of this as this competition between analog computing and digital computing, they fought for what at the time was the right thing.\u201d Periodicity By the 1980s, however, researchers had developed algorithms for modifying neural nets\u2019 weights and thresholds that were efficient enough for networks with more than one layer, removing many of the limitations identified by Minsky and Papert. The field enjoyed a renaissance. But intellectually, there\u2019s something unsatisfying about neural nets. Enough training may revise a network\u2019s settings to the point that it can usefully classify data, but what do those settings mean? What image features is an object recognizer looking at, and how does it piece them together into the distinctive visual signatures of cars, houses, and coffee cups? Looking at the weights of individual connections won\u2019t answer that question. In recent years, computer scientists have begun to come up with ingenious methods for deducing the analytic strategies adopted by neural nets. But in the 1980s, the networks\u2019 strategies were indecipherable. So around the turn of the century, neural networks were supplanted by support vector machines, an alternative approach to machine learning that\u2019s based on some very clean and elegant mathematics. The recent resurgence in neural networks \u2014 the deep-learning revolution \u2014 comes courtesy of the computer-game industry. The complex imagery and rapid pace of today\u2019s video games require hardware that can keep up, and the result has been the graphics processing unit (GPU), which packs thousands of relatively simple processing cores on a single chip. It didn\u2019t take long for researchers to realize that the architecture of a GPU is remarkably like that of a neural net. Modern GPUs enabled the one-layer networks of the 1960s and the two- to three-layer networks of the 1980s to blossom into the 10-, 15-, even 50-layer networks of today. That\u2019s what the \u201cdeep\u201d in \u201cdeep learning\u201d refers to \u2014 the depth of the network\u2019s layers. And currently, deep learning is responsible for the best-performing systems in almost every area of artificial-intelligence research. Under the hood The networks\u2019 opacity is still unsettling to theorists, but there\u2019s headway on that front, too. In addition to directing the Center for Brains, Minds, and Machines (CBMM), Poggio leads the center\u2019s research program in Theoretical Frameworks for Intelligence. Recently, Poggio and his CBMM colleagues have released a three-part theoretical study of neural networks. The first part, which was published last month in the International Journal of Automation and Computing, addresses the range of computations that deep-learning networks can execute and when deep networks offer advantages over shallower ones. Parts two and three, which have been released as CBMM technical reports, address the problems of global optimization, or guaranteeing that a network has found the settings that best accord with its training data, and overfitting, or cases in which the network becomes so attuned to the specifics of its training data that it fails to generalize to other instances of the same categories. There are still plenty of theoretical questions to be answered, but CBMM researchers\u2019 work could help ensure that neural networks finally break the generational cycle that has brought them in and out of favor for seven decades. Share this news article on: Related Links Related Topics Related Articles \nVoice control everywhere  \n\n \nModel sheds light on purpose of inhibitory neurons \n\n \nLearning words from pictures \n\n \nComputer learns to recognize sounds by watching video \n\n \n\n\n\n\n\n\n\n\n\n\n\n\nPrevious item\nNext item\n\n\n\n\n\n\n\n\n\n\n\n\n\n More MIT News \nCreating innovative health solutions for individuals and populations\n\n\n\n\n\n\n\n\n \nRead full story \u2192\n             \nMIT\u2019s Science Policy Initiative holds 14th annual Executive Visit Days\n \nRead full story \u2192\n             \nTroy Van Voorhis to step down as department head of chemistry \n \nRead full story \u2192\n             \nIs there enough land on Earth to fight climate change and feed the world?\n \nRead full story \u2192\n             \nDecarbonizing heavy industry with thermal batteries\n \nRead full story \u2192\n             \nThe MIT Press releases report on the future of open access publishing and policy\n \nRead full story \u2192\n             \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMore about MIT News at Massachusetts Institute of Technology\n This website is managed by the MIT News Office, part of the Institute Office of Communications. News by Schools/College: Resources: Tools: Massachusetts Institute of Technology77 Massachusetts Avenue, Cambridge, MA, USA"}
{"url": "https://aws.amazon.com/what-is/neural-network/", "title": "What is a Neural Network? - Artificial Neural Network Explained - AWS", "content": "What is a Neural Network? What is a neural network? A neural network is a method in artificial intelligence (AI) that teaches computers to process data in a way that is inspired by the human brain. It is a type of machine learning (ML) process, called deep learning, that uses interconnected nodes or neurons in a layered structure that resembles the human brain. It creates an adaptive system that computers use to learn from their mistakes and improve continuously. Thus, artificial neural networks attempt to solve complicated problems, like summarizing documents or recognizing faces, with greater accuracy. Why are neural networks important? Neural networks can help computers make intelligent decisions with limited human assistance. This is because they can learn and model the relationships between input and output data that are nonlinear and complex. For instance, they can do the following tasks. Make generalizations and inferences Neural networks can comprehend unstructured data and make general observations without explicit training. For instance, they can recognize that two different input sentences have a similar meaning: A neural network would know that both sentences mean the same thing. Or it would be able to broadly recognize that Baxter Road is a place, but Baxter Smith is a person\u2019s name. What are neural networks used for? Neural networks have several use cases across many industries, such as the following: We give four of the important applications of neural networks below. Computer vision Computer vision is the ability of computers to extract information and insights from images and videos. With neural networks, computers can distinguish and recognize images similar to humans. Computer vision has several applications, such as the following: Speech recognition Neural networks can analyze human speech despite varying speech patterns, pitch, tone, language, and accent. Virtual assistants like Amazon Alexa and automatic transcription software use speech recognition to do tasks like these: Natural language processing Natural language processing (NLP) is the ability to process natural, human-created text. Neural networks help computers gather insights and meaning from text data and documents. NLP has several use cases, including in these functions: Recommendation engines Neural networks can track user activity to develop personalized recommendations. They can also analyze all user behavior and discover new products or services that interest a specific user. For example, Curalate, a Philadelphia-based startup, helps brands convert social media posts into sales. Brands use Curalate\u2019s intelligent product tagging (IPT) service to automate the collection and curation of user-generated social content. IPT uses neural networks to automatically find and recommend products relevant to the user\u2019s social media activity. Consumers don't have to hunt through online catalogs to find a specific product from a social media image. Instead, they can use Curalate\u2019s auto product tagging to purchase the product with ease. How do neural networks work? The human brain is the inspiration behind neural network architecture. Human brain cells, called neurons, form a complex, highly interconnected network and send electrical signals to each other to help humans process information. Similarly, an artificial neural network is made of artificial neurons that work together to solve a problem. Artificial neurons are software modules, called nodes, and artificial neural networks are software programs or algorithms that, at their core, use computing systems to solve mathematical calculations. Simple neural network architecture A basic neural network has interconnected artificial neurons in three layers: Input Layer Information from the outside world enters the artificial neural network from the input layer. Input nodes process the data, analyze or categorize it, and pass it on to the next layer. Hidden Layer Hidden layers take their input from the input layer or other hidden layers. Artificial neural networks can have a large number of hidden layers. Each hidden layer analyzes the output from the previous layer, processes it further, and passes it on to the next layer. Output Layer The output layer gives the final result of all the data processing by the artificial neural network. It can have single or multiple nodes. For instance, if we have a binary (yes/no) classification problem, the output layer will have one output node, which will give the result as 1 or 0. However, if we have a multi-class classification problem, the output layer might consist of more than one output node. Deep neural network architecture Deep neural networks, or deep learning networks, have several hidden layers with millions of artificial neurons linked together. A number, called weight, represents the connections between one node and another. The weight is a positive number if one node excites another, or negative if one node suppresses the other. Nodes with higher weight values have more influence on the other nodes. Theoretically, deep neural networks can map any input type to any output type. However, they also need much more training as compared to other machine learning methods. They need millions of examples of training data rather than perhaps the hundreds or thousands that a simpler network might need. What are the types of neural networks? Artificial neural networks can be categorized by how the data flows from the input node to the output node. Below are some examples: Feedforward neural networks Feedforward neural networks process data in one direction, from the input node to the output node. Every node in one layer is connected to every node in the next layer. A feedforward network uses a feedback process to improve predictions over time. Backpropagation algorithm Artificial neural networks learn continuously by using corrective feedback loops to improve their predictive analytics. In simple terms, you can think of the data flowing from the input node to the output node through many different paths in the neural network. Only one path is the correct one that maps the input node to the correct output node. To find this path, the neural network uses a feedback loop, which works as follows: Convolutional neural networks The hidden layers in convolutional neural networks perform specific mathematical functions, like summarizing or filtering, called convolutions. They are very useful for image classification because they can extract relevant features from images that are useful for image recognition and classification. The new form is easier to process without losing features that are critical for making a good prediction. Each hidden layer extracts and processes different image features, like edges, color, and depth. How to train neural networks? Neural network training is the process of teaching a neural network to perform a task. Neural networks learn by initially processing several large sets of labeled or unlabeled data. By using these examples, they can then process unknown inputs more accurately. Supervised learning In supervised learning, data scientists give artificial neural networks labeled datasets that provide the right answer in advance. For example, a deep learning network training in facial recognition initially processes hundreds of thousands of images of human faces, with various terms related to ethnic origin, country, or emotion describing each image. The neural network slowly builds knowledge from these datasets, which provide the right answer in advance. After the network has been trained, it starts making guesses about the ethnic origin or emotion of a new image of a human face that it has never processed before. What is deep learning in the context of neural networks? Artificial intelligence is the field of computer science that researches methods of giving machines the ability to perform tasks that require human intelligence. Machine learning is an artificial intelligence technique that gives computers access to very large datasets and teaches them to learn from this data. Machine learning software finds patterns in existing data and applies those patterns to new data to make intelligent decisions. Deep learning is a subset of machine learning that uses deep learning networks to process data. Machine learning vs. deep learning Traditional machine learning methods require human input for the machine learning software to work sufficiently well. A data scientist manually determines the set of relevant features that the software must analyze. This limits the software\u2019s ability, which makes it tedious to create and manage. On the other hand, in deep learning, the data scientist gives only raw data to the software. The deep learning network derives the features by itself and learns more independently. It can analyze unstructured datasets like text documents, identify which data attributes to prioritize, and solve more complex problems. For example, if you were training a machine learning software to identify an image of a pet correctly, you would need to take these steps: What are deep learning services on AWS? AWS deep learning services\u00a0harness the power of cloud computing so that you can scale your deep learning neural networks at a lower cost and optimize them for speed. You can also use AWS services like these to fully manage specific deep learning applications: Get started with deep learning neural networks on AWS with\u00a0Amazon SageMaker\u00a0and quickly and easily build, train, and\u00a0deploy models at scale. You can also use the\u00a0AWS Deep Learning AMIs\u00a0to build custom environments and workflows for deep learning. Create a\u00a0free AWS account\u00a0to get started today!  Next steps on AWS  Learn About AWS  Resources for AWS  Developers on AWS  Help  Ending Support for Internet Explorer"}
{"url": "https://www.ibm.com/topics/neural-networks", "title": "What is a Neural Network? | IBM", "content": "A neural network is a\u00a0machine learning program, or model, that makes decisions in a manner similar to the human brain, by using processes that mimic the way biological neurons work together to identify phenomena, weigh options and arrive at conclusions. Every neural network consists of layers of nodes, or artificial neurons\u2014an input layer, one or more hidden layers, and an output layer. Each node connects to others, and has its own associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network.\n Neural networks rely on training data to learn and improve their accuracy over time. Once they are fine-tuned for accuracy, they are powerful tools in computer science and\u00a0artificial intelligence, allowing us to classify and cluster data at a high velocity. Tasks in speech recognition or image recognition can take minutes versus hours when compared to the manual identification by human experts. One of the best-known examples of a neural network is Google\u2019s search algorithm. Neural networks are sometimes called\u00a0artificial neural networks\u00a0(ANNs) or\u00a0simulated neural networks\u00a0(SNNs).\u00a0They are a subset of machine learning, and at the heart of deep learning models. Learn how to choose the right approach in preparing data sets and employing foundation models. Register for the ebook on generative AI Think of each individual node as its own linear regression model, composed of input data, weights, a bias (or threshold), and an output.\u00a0The formula would look something like this: \u2211wixi + bias = w1x1 + w2x2 + w3x3 + bias output = f(x) = 1 if \u2211w1x1 + b>= 0; 0 if \u2211w1x1 + b < 0 Once an input layer is determined, weights are assigned. These weights help determine the importance of any given variable, with larger ones contributing more significantly to the output compared to other inputs. All inputs are then multiplied by their respective weights and then summed. Afterward, the output is passed through an activation function, which determines the output. If that output exceeds a given threshold, it \u201cfires\u201d (or activates) the node, passing data to the next layer in the network. This results in the output of one node becoming in the input of the next node. This process of passing data from one layer to the next layer defines this neural network as a feedforward network. Let\u2019s break down what one single node might look like using binary values. We can apply this concept to a more tangible example, like whether you should go surfing (Yes: 1, No: 0). The decision to go or not to go is our predicted outcome, or y-hat. Let\u2019s assume that there are three factors influencing your decision-making: Then, let\u2019s assume the following, giving us the following inputs: Now, we need to assign some weights to determine importance. Larger weights signify that particular variables are of greater importance to the decision or outcome. Finally, we\u2019ll also assume a threshold value of 3, which would translate to a bias value of \u20133. With all the various inputs, we can start to plug in values into the formula to get the desired output. Y-hat = (1*5) + (0*2) + (1*4) \u2013 3 = 6 If we use the activation function from the beginning of this section, we can determine that the output of this node would be 1, since 6 is greater than 0. In this instance, you would go surfing; but if we adjust the weights or the threshold, we can achieve different outcomes from the model. When we observe one decision, like in the above example, we can see how a neural network could make increasingly complex decisions depending on the output of previous decisions or layers. In the example above, we used perceptrons to illustrate some of the mathematics at play here, but neural networks leverage sigmoid neurons, which are distinguished by having values between 0 and 1. Since neural networks behave similarly to decision trees, cascading data from one node to another, having x values between 0 and 1 will reduce the impact of any given change of a single variable on the output of any given node, and subsequently, the output of the neural network. As we start to think about more practical use cases for neural networks, like image recognition or classification, we\u2019ll leverage supervised learning, or labeled datasets, to train the algorithm. As we train the model, we\u2019ll want to evaluate its accuracy using a cost (or loss) function. This is also commonly referred to as the mean squared error (MSE). In the equation below, \ud835\udc36\ud835\udc5c\ud835\udc60\ud835\udc61 \ud835\udc39\ud835\udc62\ud835\udc5b\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b= \ud835\udc40\ud835\udc46\ud835\udc38=1/2\ud835\udc5a \u2211129_(\ud835\udc56=1)^\ud835\udc5a\u2592(\ud835\udc66\u00a0\u0302^((\ud835\udc56) )\u2212\ud835\udc66^((\ud835\udc56) ) )^2 Ultimately, the goal is to minimize our cost function to ensure correctness of fit for any given observation. As the model adjusts its weights and bias, it uses the cost function and reinforcement learning to reach the point of convergence, or the local minimum. The process in which the algorithm adjusts its weights is through gradient descent, allowing the model to determine the direction to take to reduce errors (or minimize the cost function). With each training example, the parameters of the model adjust to gradually converge at the minimum.\u00a0\u00a0 See this IBM Developer article for a deeper explanation of the quantitative concepts involved in neural networks. Most deep neural networks are feedforward, meaning they flow in one direction only, from input to output. However, you can also train your model through backpropagation; that is, move in the opposite direction from output to input. Backpropagation allows us to calculate and attribute the error associated with each neuron, allowing us to adjust and fit the parameters of the model(s) appropriately. The all new enterprise studio that brings together traditional machine learning along with new generative AI capabilities powered by foundation models. Neural networks can be classified into different types, which are used for different purposes. While this isn\u2019t a comprehensive list of types, the below would be representative of the most common types of neural networks that you\u2019ll come across for its common use cases: The perceptron is the oldest neural network, created by Frank Rosenblatt in 1958. Feedforward neural networks, or multi-layer perceptrons (MLPs), are what we\u2019ve primarily been focusing on within this article. They are comprised of an input layer, a hidden layer or layers, and an output layer. While these neural networks are also commonly referred to as MLPs, it\u2019s important to note that they are actually comprised of sigmoid neurons, not perceptrons, as most real-world problems are nonlinear. Data usually is fed into these models to train them, and they are the foundation for computer vision, natural language processing, and other neural networks. Convolutional neural networks (CNNs) are similar to feedforward networks, but they\u2019re usually utilized for image recognition, pattern recognition, and/or computer vision. These networks harness principles from linear algebra, particularly matrix multiplication, to identify patterns within an image. Recurrent neural networks (RNNs) are identified by their feedback loops. These learning algorithms are primarily leveraged when using time-series data to make predictions about future outcomes, such as stock market predictions or sales forecasting. \u00a0 Deep Learning and neural networks tend to be used interchangeably in conversation, which can be confusing. As a result, it\u2019s worth noting that the \u201cdeep\u201d in deep learning is just referring to the depth of layers in a neural network. A neural network that consists of more than three layers\u2014which would be inclusive of the inputs and the output\u2014can be considered a deep learning algorithm. A neural network that only has two or three layers is just a basic neural network. To learn more about the differences between neural networks and other forms of artificial intelligence,\u00a0 like machine learning, please read the blog post \u201cAI vs. Machine Learning vs. Deep Learning vs. Neural Networks: What\u2019s the Difference?\u201d The history of neural networks is longer than most people think. While the idea of \u201ca machine that thinks\u201d can be traced to the Ancient Greeks, we\u2019ll focus on the key events that led to the evolution of thinking around neural networks, which has ebbed and flowed in popularity over the years: 1943: Warren S. McCulloch and Walter Pitts published \u201cA logical calculus of the ideas immanent in nervous activity\u00a0(link resides outside ibm.com)\u201d This research sought to understand how the human brain could produce complex patterns through connected brain cells, or neurons. One of the main ideas that came out of this work was the comparison of neurons with a binary threshold to Boolean logic (i.e., 0/1 or true/false statements).\u00a0 \u00a0 1958: Frank Rosenblatt is credited with the development of the perceptron, documented in his research, \u201cThe Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain\u201d (link resides outside ibm.com). He takes McCulloch and Pitt\u2019s work a step further by introducing weights to the equation. Leveraging an IBM 704, Rosenblatt was able to get a computer to learn how to distinguish cards marked on the left vs. cards marked on the right. 1974: While numerous researchers contributed to the idea of backpropagation, Paul Werbos was the first person in the US to note its application within neural networks within his PhD thesis\u00a0(link resides outside ibm.com). 1989: Yann LeCun published a paper (link resides outside ibm.com) illustrating how the use of constraints in backpropagation and its integration into the neural network architecture can be used to train algorithms. This research successfully leveraged a neural network to recognize hand-written zip code digits provided by the U.S. Postal Service. Design complex neural networks. Experiment at scale to deploy optimized learning models within IBM Watson Studio. Build and scale trusted AI on any cloud. Automate the AI lifecycle for ModelOps.  Take the next step to start operationalizing and scaling generative AI and machine learning for business. Granite is IBM's flagship series of LLM foundation models based on decoder-only transformer architecture. Granite language models are trained on trusted enterprise data spanning internet, academic, code, legal and finance. We surveyed 2,000 organizations about their AI initiatives to discover what\u2019s working, what\u2019s not and how you can get ahead. Learn the building blocks and best practices to help your teams accelerate responsible AI.  These terms are often used interchangeably, but what differences make each a unique technology? Register for our ebook for insights into the opportunities, challenges and lessons learned from infusing AI into businesses. Get an in-depth understanding of neural networks, their basic functions and the fundamentals of building one. Train, validate, tune and deploy generative AI, foundation models and machine learning capabilities with\u00a0IBM watsonx.ai, a next-generation enterprise studio for AI builders. Build AI applications in a fraction of the time with a fraction of the data."}
{"url": "https://www.ibm.com/topics/natural-language-processing", "title": "What Is NLP (Natural Language Processing)? | IBM", "content": "Updated: 11 August 2024\nContributor: Cole Stryker, Jim Holdsworth\n Natural language processing (NLP) is a subfield of computer science and artificial intelligence (AI) that uses machine learning to enable computers to understand and communicate with human language.\u00a0 NLP enables computers and digital devices to recognize, understand and generate text and speech by combining computational linguistics\u2014the rule-based modeling of human language\u2014together with statistical modeling, machine learning and deep learning.\u00a0 NLP research has helped enable the era of generative AI, from the communication skills of large language models (LLMs) to the ability of image generation models to understand requests. NLP is already part of everyday life for many, powering search engines, prompting chatbots for customer service with spoken commands, voice-operated GPS systems and question-answering digital assistants on smartphones such as Amazon\u2019s Alexa, Apple\u2019s Siri and Microsoft\u2019s Cortana.\u00a0 NLP also plays a growing role in enterprise solutions that help streamline and automate business operations, increase employee productivity and simplify business processes. Use this model selection framework to choose the most appropriate model while balancing your performance requirements with cost, risks and deployment needs. Register for the white paper on AI governance NLP makes it easier for humans to communicate and collaborate with machines, by allowing them to do so in the natural human language they use every day. This offers benefits across many industries and applications. Automation of repetitive tasks\u00a0 \r\nNLP is especially useful in fully or partially automating tasks like customer support, data entry and document handling. For example, NLP-powered chatbots can handle routine customer queries, freeing up human agents for more complex issues. In document processing, NLP tools can automatically classify, extract key information and summarize content, reducing the time and errors associated with manual data handling. NLP facilitates language translation, converting text from one language to another while preserving meaning, context and nuances.\n Improved data analysis\u00a0 \r\nNLP enhances data analysis by enabling the extraction of insights from unstructured text data, such as customer reviews, social media posts and news articles. By using text mining techniques, NLP can identify patterns, trends and sentiments that are not immediately obvious in large datasets. Sentiment analysis enables the\u00a0extraction of\u00a0 subjective qualities\u2014attitudes, emotions, sarcasm, confusion or suspicion\u2014from text. This is often used for routing communications to the system or the person most likely to make the next response.\u00a0 This allows businesses to better understand customer preferences, market conditions and public opinion. NLP tools can also perform categorization and summarization of vast amounts of text, making it easier for analysts to identify key information and make data-driven decisions more efficiently. Enhanced search\u00a0 \r\nNLP benefits search by enabling systems to understand the intent behind user queries, providing more accurate and contextually relevant results. Instead of relying solely on keyword matching, NLP-powered search engines analyze the meaning of words and phrases, making it easier to find information even when queries are vague or complex. This improves user experience, whether in web searches, document retrieval or enterprise data systems. Powerful content generation\n \r\nNLP powers advanced language models to create human-like text for various purposes. Pre-trained models, such as GPT-4, can generate articles, reports, marketing copy, product descriptions and even creative writing based on prompts provided by users. NLP-powered tools can also assist in automating tasks like drafting emails, writing social media posts or legal documentation. By understanding context, tone and style, NLP sees to it that the generated content is coherent, relevant and aligned with the intended message, saving time and effort in content creation while maintaining quality. NLP combines the power of computational linguistics together with machine learning algorithms and deep learning. Computational linguistics uses data science to analyze language and speech. It includes two main types of analysis: syntactical analysis and semantical analysis. Syntactical analysis determines the meaning of a word, phrase or sentence by parsing the syntax of the words and applying preprogrammed rules of grammar. Semantical analysis uses the syntactic output to draw meaning from the words and interpret their meaning within the sentence structure. \u00a0 The parsing of words can take one of two forms. Dependency parsing looks at the relationships between words, such as identifying nouns and verbs, while constituency parsing then builds a parse tree (or syntax tree): a rooted and ordered representation of the syntactic structure of the sentence or string of words. The resulting parse trees underly the functions of language translators and speech recognition. Ideally, this analysis makes the output\u2014either text or speech\u2014understandable to both NLP models and people. Self-supervised learning (SSL) in particular is useful for supporting NLP because NLP requires large amounts of labeled data to train AI models. Because these labeled datasets require time-consuming annotation\u2014a process involving manual labeling by humans\u2014gathering sufficient data can be prohibitively difficult. Self-supervised approaches can be more time-effective and cost-effective, as they replace some or all manually labeled training data.\u00a0\r\n\u00a0\r\nThree different approaches to NLP include: Rules-based NLP \r\nThe earliest NLP applications were simple if-then decision trees, requiring preprogrammed rules. They are only able to provide answers in response to specific prompts, such as the original version of Moviefone, which had rudimentary natural language generation (NLG) capabilities. Because there is no machine learning or AI capability in rules-based NLP, this function is highly limited and not scalable. Statistical NLP \r\nDeveloped later, statistical NLP automatically extracts, classifies and labels elements of text and voice data and then assigns a statistical likelihood to each possible meaning of those elements. This relies on machine learning, enabling a sophisticated breakdown of linguistics such as part-of-speech tagging.\u00a0\r\n\u00a0\r\nStatistical NLP introduced the essential technique of mapping language elements\u2014such as words and grammatical rules\u2014to a vector representation so that language can be modeled by using mathematical (statistical) methods, including regression or Markov models. This informed early NLP developments such as spellcheckers and T9 texting (Text on 9 keys, to be used on Touch-Tone telephones).\n Deep learning NLP \r\nRecently, deep learning models have become the dominant mode of NLP, by using huge volumes of raw, unstructured data\u2014both text and voice\u2014to become ever more accurate. Deep learning can be viewed as a further evolution of statistical NLP, with the difference that it uses neural network models. There are several subcategories of models: Sequence-to-Sequence (seq2seq) models: Based on recurrent neural networks (RNN), they have mostly been used for machine translation by converting a phrase from one domain (such as the German language) into the phrase of another domain (such as English). Transformer models: They use tokenization of language (the position of each token\u2014words or subwords) and self-attention (capturing dependencies and relationships) to calculate the relation of different language parts to one another. Transformer models can be efficiently trained by using self-supervised learning on massive text databases. A landmark in transformer models was Google\u2019s bidirectional encoder representations from transformers (BERT), which became and remains the basis of how Google\u2019s search engine works. Autoregressive models: This type of transformer model is trained specifically to predict the next word in a sequence, which represents a huge leap forward in the ability to generate text. Examples of autoregressive LLMs include GPT, Llama, Claude and the open-source Mistral. Foundation models: Prebuilt and curated foundation models can speed the launching of an NLP effort and boost trust in its operation. For example, the IBM\u00ae Granite\u2122 foundation models are widely applicable across industries. They support NLP tasks including content generation and insight extraction. Additionally, they facilitate retrieval-augmented generation, a framework for improving the quality of response by linking the model to external sources of knowledge. The models also perform named entity recognition which involves identifying and extracting key information in a text.\u00a0\n Several NLP tasks typically help process human text and voice data in ways that help the computer make sense of what it\u2019s ingesting. Some of these tasks include: Coreference resolution Named entity recognition Part-of-speech tagging Word sense disambiguation Coreference resolution\n \r\nThis is the task of identifying if and when two words refer to the same entity. The most common example is determining the person or object to which a certain pronoun refers (such as \u201cshe\u201d = \u201cMary\u201d). But it can also identify a metaphor or an idiom in the text (such as an instance in which \u201cbear\u201d isn\u2019t an animal, but a large and hairy person).\u00a0 Named entity recognition (NER) \nNER identifies words or phrases as useful entities. NER identifies \u201cLondon\u201d as a location or \u201cMaria\u201d as a person's name.\u00a0\n Part-of-speech tagging\n \r\nAlso called grammatical tagging, this is the process of determining which part of speech a word or piece of text is, based on its use and context. For example, part-of-speech identifies \u201cmake\u201d as a verb in \u201cI can make a paper plane,\u201d and as a noun in \u201cWhat make of car do you own?\u201d\u00a0\n Word sense disambiguation\n \r\nThis is the selection of a word meaning for a word with multiple possible meanings. This uses a process of semantic analysis to examine the word in context. For example, word sense disambiguation helps distinguish the meaning of the verb \u201cmake\u201d in \u201cmake the grade\u201d (to achieve) versus \u201cmake a bet\u201d (to place). Sorting out \u201cI will be merry when I marry Mary\u201d requires a sophisticated NLP system. NLP works by combining various computational techniques to analyze, understand and generate human language in a way that machines can process. Here is an overview of a typical NLP pipeline and its steps: Text preprocessing\u00a0 \r\nNLP text preprocessing prepares raw text for analysis by transforming it into a format that machines can more easily understand. It begins with tokenization, which involves splitting the text into smaller units like words, sentences or phrases. This helps break down complex text into manageable parts. Next, lowercasing is applied to standardize the text by converting all characters to lowercase, ensuring that words like \"Apple\" and \"apple\" are treated the same. Stop word removal is another common step, where frequently used words like \"is\" or \"the\" are filtered out because they don't add significant meaning to the text. Stemming or lemmatization reduces words to their root form (e.g., \"running\" becomes \"run\"), making it easier to analyze language by grouping different forms of the same word. Additionally, text cleaning removes unwanted elements such as punctuation, special characters and numbers that may clutter the analysis.\u00a0 After preprocessing, the text is clean, standardized and ready for machine learning models to interpret effectively. Feature extraction\u00a0 \r\nFeature extraction is the process of converting raw text into numerical representations that machines can analyze and interpret. This involves transforming text into structured data by using NLP techniques like Bag of Words and TF-IDF, which quantify the presence and importance of words in a document. More advanced methods include word embeddings like Word2Vec or GloVe, which represent words as dense vectors in a continuous space, capturing semantic relationships between words. Contextual embeddings further enhance this by considering the context in which words appear, allowing for richer, more nuanced representations.\u00a0 Text analysis\u00a0 \r\nText analysis involves interpreting and extracting meaningful information from text data through various computational techniques. This process includes tasks such as part-of-speech (POS) tagging, which identifies grammatical roles of words and named entity recognition (NER), which detects specific entities like names, locations and dates. Dependency parsing analyzes grammatical relationships between words to understand sentence structure, while sentiment analysis determines the emotional tone of the text, assessing whether it is positive, negative or neutral. Topic modeling identifies underlying themes or topics within a text or across a corpus of documents. Natural language understanding (NLU) is a subset of NLP that focuses on analyzing the meaning behind sentences. NLU enables software to find similar meanings in different sentences or to process words that have different meanings. Through these techniques, NLP text analysis transforms unstructured text into insights.\u00a0\u00a0\u00a0 Model training \r\nProcessed data is then used to train machine learning models, which learn patterns and relationships within the data. During training, the model adjusts its parameters to minimize errors and improve its performance. Once trained, the model can be used to make predictions or generate outputs on new, unseen data. The effectiveness of NLP modeling is continually refined through evaluation, validation and fine-tuning to enhance accuracy and relevance in real-world applications. Different software environments are useful throughout the said processes. For example, the Natural Language Toolkit (NLTK) is a suite of libraries and programs for English that is written in the Python programming language. It supports text classification, tokenization, stemming, tagging, parsing and semantic reasoning functionalities. TensorFlow is a free and open-source software library for machine learning and AI that can be used to train models for NLP applications. Tutorials and certifications abound for those interested in familiarizing themselves with such tools.\u00a0\n Even state-of-the-art NLP models are not perfect, just as human speech is prone to error. As with any AI technology, NLP comes with potential pitfalls. Human language is filled with ambiguities that make it difficult for programmers to write software that accurately determines the intended meaning of text or voice data. Human language might take years for humans to learn\u2014and many never stop learning. But then programmers must teach natural language-powered applications to recognize and understand irregularities so their applications can be accurate and useful.\u00a0Associated risks might include: Biased training\n \r\nAs with any AI function, biased data used in training will skew the answers. The more diverse the users of an NLP function, the more significant this risk becomes, such as in government services, healthcare and HR interactions. Training datasets scraped from the web, for example, are prone to bias. Misinterpretation\n \r\nAs in programming, there is a risk of garbage in, garbage out (GIGO). Speech recognition, also known as speech-to-text, is the task of reliably converting voice data into text data. But NLP solutions can become confused if spoken input is in an obscure dialect, mumbled, too full of slang, homonyms, incorrect grammar, idioms, fragments, mispronunciations, contractions or recorded with too much background noise.\n New vocabulary\n \r\nNew words are continually being invented or imported. The conventions of grammar can evolve or be intentionally broken. In these cases, NLP can either make a best guess or admit it\u2019s unsure\u2014and either way, this creates a complication. Tone of voice\n \r\nWhen people speak, their verbal delivery or even body language can give an entirely different meaning than the words alone. Exaggeration for effect, stressing words for importance or sarcasm can be confused by NLP, making the semantic analysis more difficult and less reliable. NLP applications can now be found across virtually every industry.\u00a0 Finance\n \r\nIn financial dealings, nanoseconds might make the difference between success and failure when accessing data, or making trades or deals. NLP can speed the mining of information from financial statements, annual and regulatory reports, news releases or even social media.\n Healthcare\n \r\nNew medical insights and breakthroughs can arrive faster than many healthcare professionals can keep up. NLP and AI-based tools can help speed the analysis of health records and medical research papers, making better-informed medical decisions possible, or assisting in the detection or even prevention of medical conditions. Insurance\n \r\nNLP can analyze claims to look for patterns that can identify areas of concern and find inefficiencies in claims processing\u2014leading to greater optimization of processing and employee efforts.\n Legal\n \r\nAlmost any legal case might require reviewing mounds of paperwork, background information and legal precedent. NLP can help automate legal discovery, assisting in the organization of information, speeding review and making sure that all relevant details are captured for consideration. Accelerate the business value of artificial intelligence with a powerful and flexible portfolio of libraries, services and applications. Infuse powerful natural language AI into commercial applications with a containerized library designed to empower IBM partners with greater flexibility. Granite is IBM's flagship series of LLM foundation models based on decoder-only transformer architecture. Granite language models are trained on trusted enterprise data spanning internet, academic, code, legal and finance. We surveyed 2,000 organizations about their AI initiatives to discover what\u2019s working, what\u2019s not and how you can get ahead. Learn the fundamental concepts for AI and generative AI, including prompt engineering, large language models and the best open source projects. Discover how natural language processing can help you to converse more naturally with computers. Meet Scout Advisor\u2014an innovative NLP tool built on the IBM\u00ae watsonx\u2122 platform especially for Spain\u2019s Sevilla F\u00fatbol Club.  This article explains how IBM Watson can help you use NLP services to develop increasingly smart applications, with a focus on natural language understanding. Train, validate, tune and deploy generative AI, foundation models and machine learning capabilities with\u00a0IBM watsonx.ai, a next-generation enterprise studio for AI builders. Build AI applications in a fraction of the time with a fraction of the data."}
{"url": "https://en.wikipedia.org/wiki/Natural_language_processing", "title": "Natural language processing - Wikipedia", "content": "Contents Natural language processing Natural language processing (NLP) is a subfield of computer science and especially artificial intelligence. It is primarily concerned with providing computers with the ability to process data encoded in natural language and is thus closely related to information retrieval, knowledge representation and computational linguistics, a subfield of linguistics. Typically data is collected in text corpora, using either rule-based, statistical or neural-based approaches in machine learning and deep learning.\n Major tasks in natural language processing are speech recognition, text classification, natural-language understanding, and natural-language generation.\n History Natural language processing has its roots in the 1950s.[1] Already in 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.\n Symbolic NLP (1950s \u2013 early 1990s) The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.\n Statistical NLP (1990s\u20132010s) Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing.  This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[8]\n Neural NLP (present) In 2003, word n-gram model, at the time the best statistical algorithm, was outperformed by a multi-layer perceptron (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in language modelling) by Yoshua Bengio with co-authors.[9]\n In 2010, Tom\u00e1\u0161 Mikolov (then a PhD student at Brno University of Technology) with co-authors applied a simple recurrent neural network with a single hidden layer to language modelling,[10] and in the following years he went on to develop Word2vec. In the 2010s, representation learning and deep neural network-style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques[11][12] can achieve state-of-the-art results in many natural language tasks, e.g., in language modeling[13] and parsing.[14][15] This is increasingly important in medicine and healthcare, where NLP helps analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care[16] or protect patient privacy.[17]\n Approaches: Symbolic, statistical, neural networks Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular:[18][19] such as by writing grammars or devising heuristic rules for stemming.\n Machine learning approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: \n Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of LLMs in 2023. \n Before that they were commonly used:\n Statistical approach In the late 1980s and mid-1990s, the statistical approach ended a period of AI winter, which was caused by the inefficiencies of the rule-based approaches.[20][21]\n The earliest decision trees, producing systems of hard if\u2013then rules, were still very similar to the old rule-based approaches.\nOnly the introduction of hidden Markov models, applied to part-of-speech tagging, announced the end of the old rule-based approach.\n Neural networks A major drawback of statistical methods is that they require elaborate feature engineering. Since 2015,[22] the statistical approach has been replaced by the neural networks approach, using semantic networks[23] and word embeddings to capture semantic properties of words.  \n Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) are not needed anymore. \n Neural machine translation, based on then-newly-invented sequence-to-sequence transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for statistical machine translation.\n Common NLP tasks The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.\n Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.\n Text and speech processing Morphological analysis Syntactic analysis Lexical semantics (of individual words in context) Relational semantics (semantics of individual sentences) Discourse (semantics beyond individual sentences) Higher-level NLP applications General tendencies and (possible) future directions Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed:[46]\n Cognition Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).\n Cognition refers to \"the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses.\"[47] Cognitive science is the interdisciplinary, scientific study of the mind and its processes.[48] Cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics.[49] Especially during the age of symbolic NLP, the area of computational linguistics maintained strong ties with cognitive studies.\n As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics,[50] with two defining aspects:\n Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar,[53] functional grammar,[54] construction grammar,[55] computational psycholinguistics and cognitive neuroscience (e.g., ACT-R), however, with limited uptake in mainstream NLP (as measured by presence on major conferences[56] of the ACL). More recently, ideas of cognitive NLP have been revived as an approach to achieve explainability, e.g., under the notion of \"cognitive AI\".[57] Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit)[58] and developments in artificial intelligence, specifically tools and technologies using large language model approaches[59] and new directions in artificial general intelligence based on the free energy principle[60] by British neuroscientist and theoretician at University College London Karl J. Friston.\n See also References Further reading External links"}
{"url": "https://www.geeksforgeeks.org/natural-language-processing-overview/", "title": "Natural Language Processing (NLP) - Overview - GeeksforGeeks", "content": "Natural Language Processing (NLP) \u2013 Overview The meaning of NLP is Natural Language Processing (NLP) which is a fascinating and rapidly evolving field that intersects computer science, artificial intelligence, and linguistics. NLP focuses on the interaction between computers and human language, enabling machines to understand, interpret, and generate human language in a way that is both meaningful and useful. With the increasing volume of text data generated every day, from social media posts to research articles, NLP has become an essential tool for extracting valuable insights and automating various tasks.  Natural Language Processing  In this article, we will explore the fundamental concepts and techniques of Natural Language Processing, shedding light on how it transforms raw text into actionable information. From tokenization and parsing to sentiment analysis and machine translation, NLP encompasses a wide range of applications that are reshaping industries and enhancing human-computer interactions. Whether you are a seasoned professional or new to the field, this overview will provide you with a comprehensive understanding of NLP and its significance in today\u2019s digital age. Table of Content What is Natural Language Processing? Natural language processing (NLP) is a field of computer science and a subfield of artificial intelligence that aims to make computers understand human language. NLP uses computational linguistics, which is the study of how language works, and various models based on statistics, machine learning, and deep learning. These technologies allow computers to analyze and process text or voice data, and to grasp their full meaning, including the speaker\u2019s or writer\u2019s intentions and emotions.  NLP powers many applications that use language, such as text translation, voice recognition, text summarization, and chatbots. You may have used some of these applications yourself, such as voice-operated GPS systems, digital assistants, speech-to-text software, and customer service bots. NLP also helps businesses improve their efficiency, productivity, and performance by simplifying complex tasks that involve language. NLP Techniques NLP encompasses a wide array of techniques that aimed at enabling computers to process and understand human language. These tasks can be categorized into several broad areas, each addressing different aspects of language processing. Here are some of the key NLP techniques: 1. Text Processing and Preprocessing In NLP 2. Syntax and Parsing In NLP 3. Semantic Analysis 4. Information Extraction 5. Text Classification in NLP 6. Language Generation 7. Speech Processing 8. Question Answering 9. Dialogue Systems 10. Sentiment and Emotion Analysis in NLP Working of Natural Language Processing (NLP)\u00a0  Working of Natural Language Processing  Working in natural language processing (NLP) typically involves using computational techniques to analyze and understand human language. This can include tasks such as language understanding, language generation, and language interaction. 1. Text Input and Data Collection 2. Text Preprocessing Preprocessing is crucial to clean and prepare the raw text data for analysis. Common preprocessing steps include: 3. Text Representation 4. Feature Extraction Extracting meaningful features from the text data that can be used for various NLP tasks. 5. Model Selection and Training Selecting and training a machine learning or deep learning model to perform specific NLP tasks. 6. Model Deployment and Inference Deploying the trained model and using it to make predictions or extract insights from new text data. 7. Evaluation and Optimization Evaluating the performance of the NLP algorithm using metrics such as accuracy, precision, recall, F1-score, and others. 8. Iteration and Improvement Continuously improving the algorithm by incorporating new data, refining preprocessing techniques, experimenting with different models, and optimizing features. Technologies related to Natural Language Processing There are a variety of technologies related to natural language processing (NLP) that are used to analyze and understand human language. Some of the most common include: Applications of Natural Language Processing (NLP): Future Scope: Future Enhancements:\u00a0 Conclusion In conclusion, the field of Natural Language Processing (NLP) has significantly transformed the way humans interact with machines, enabling more intuitive and efficient communication. NLP encompasses a wide range of techniques and methodologies to understand, interpret, and generate human language. From basic tasks like tokenization and part-of-speech tagging to advanced applications like sentiment analysis and machine translation, the impact of NLP is evident across various domains. As the technology continues to evolve, driven by advancements in machine learning and artificial intelligence, the potential for NLP to enhance human-computer interaction and solve complex language-related challenges remains immense. Understanding the core concepts and applications of Natural Language Processing is crucial for anyone looking to leverage its capabilities in the modern digital landscape. Natural Language Processing \u2013 FAQs What are NLP models? NLP models are computational systems that can process natural language data, such as text or speech, and perform various tasks, such as translation, summarization, sentiment analysis, etc. NLP models are usually based on machine learning or deep learning techniques that learn from large amounts of language data. What are the types of NLP models?\u00a0 NLP models can be classified into two main types: rule-based and statistical. Rule-based models use predefined rules and dictionaries to analyze and generate natural language data. Statistical models use probabilistic methods and data-driven approaches to learn from language data and make predictions. What are the challenges of NLP models?\u00a0 NLP models face many challenges due to the complexity and diversity of natural language. Some of these challenges include ambiguity, variability, context-dependence, figurative language, domain-specificity, noise, and lack of labeled data. What are the applications of NLP models?\u00a0 NLP models have many applications in various domains and industries, such as search engines, chatbots, voice assistants, social media analysis, text mining, information extraction, natural language generation, machine translation, speech recognition, text summarization, question answering, sentiment analysis, and more.  M Similar Reads  What kind of Experience do you want to share?"}
{"url": "https://viso.ai/applications/computer-vision-applications/", "title": "The 100 Most Popular Computer Vision Applications in 2024 - viso.ai", "content": "Train Build Deploy Operate Industries\u200b Use Cases Build, deliver, and scale all AI vision systems one infrastructure. Resources Research Company Why Viso Suite Viso Suite is the leading end to end computer vision infrastructure to build, deploy, and scale AI vision dramatically faster and better. The 100 Most Popular Computer Vision Applications in 2024 Build, deploy, operate computer vision at scale This article covers an extensive list of novel, valuable computer vision applications across all industries. Find the best computer vision projects, computer vision ideas, and high-value use cases in the market right now. In this article, we will cover the following: About us: viso.ai provides Viso Suite, the world\u2019s only end-to-end Computer Vision Platform. The technology enables global organizations to develop, deploy, and scale all computer vision applications in one place. Get a demo. Applications of Computer AI Vision Projects What is Computer Vision? The field of computer vision is a sector of Artificial Intelligence (AI) that uses Machine Learning and Deep Learning to enable computers to see, perform AI pattern recognition, and analyze objects in photos and videos like people do. Computational vision is rapidly gaining popularity for automated AI vision inspection, remote monitoring, and automation. Computer Vision Systems Computer vision systems use (1) cameras to obtain visual data, (2) machine learning models for processing the images, and (3) conditional logic to automate application-specific use cases. Edge intelligence then facilitates scalable, efficient, robust, secure, and private implementations of computer vision. At viso.ai, we provide the world\u2019s only end-to-end computer vision platform Viso Suite. The solution helps leading organizations develop, deploy, scale, and secure their computer vision applications in one place. Get the Whitepaper here. Computer Vision Applications in Manufacturing Read our complete manufacturing industry report here. Image recognition is used in manufacturing for AI vision inspection, quality control, remote monitoring, and system automation. Productivity Analytics Productivity analytics track the impact of workplace change, how employees spend their time and resources and implement various tools. Such data can provide valuable insight into time management, workplace collaboration, and employee productivity. Computer Vision lean management strategies aim to objectively quantify and assess processes with cameras-based vision systems. Visual Inspection of Equipment Computer vision for visual inspection is a key strategy in smart manufacturing. Vision-based inspection systems are also gaining in popularity for automated inspection of Personal Protective Equipment (PPE), such as Mask Detection or Helmet Detection. Computational vision helps to monitor adherence to safety protocols on construction sites or in a smart factory. Quality Management Smart camera applications provide a scalable method to implement automated visual inspection and quality control of production processes and assembly lines in smart factories. Hereby, deep learning uses real-time object detection to provide superior results (detection accuracy, speed, objectiveness, reliability) compared to laborious manual inspection. Compared to traditional machine vision systems, AI vision inspection uses machine learning methods that are highly robust and don\u2019t require expensive special cameras and inflexible settings. Therefore, AI vision methods are very scalable across multiple locations and factories. Skill Training Another application field of vision systems is optimizing assembly line operations in industrial production and human-robot interaction. The evaluation of human action can help construct standardized action models related to different operation steps and evaluate the performance of trained workers. Automatically assessing the action quality of workers can be beneficial by improving working performance, promoting productive efficiency (LEAN optimization), and, more importantly, discovering dangerous actions to lower accident rates. Computer Vision in Healthcare Read our healthcare industry report here. Cancer Detection Machine learning is incorporated into medical industries for purposes such as breast and skin cancer detection. For instance, image recognition allows scientists to detect slight differences between cancerous and non-cancerous images and diagnose data from magnetic resonance imaging (MRI) scans and inputted photos as malignant or benign. COVID-19 diagnosis Computer Vision can be used for coronavirus control. Multiple deep-learning computer vision models exist for x-ray-based COVID-19 diagnosis. The most popular one for detecting COVID-19 cases with digital chest x-ray radiography (CXR) images is named COVID-Net and was developed by Darwin AI, Canada. Cell Classification Machine Learning in medical use cases was used to classify T-lymphocytes against colon cancer epithelial cells with high accuracy. Thus, ML is expected to significantly accelerate the process of disease identification regarding colon cancer efficiently and at little to no cost post-creation. Movement Analysis Neurological and musculoskeletal diseases such as oncoming strokes, balance, and gait problems can be detected using deep learning models and computer vision even without doctor analysis. Pose Estimation computer vision applications that analyze patient movement assist doctors in diagnosing a patient with ease and increased accuracy. Mask Detection Masked Face Recognition is used to detect the use of masks and protective equipment to limit the spread of coronavirus. Likewise, computer Vision systems help countries implement masks as a control strategy to contain the spread of coronavirus disease. For this reason, private companies such as Uber have created computer vision features such as face detection to be implemented in their mobile apps to detect whether passengers are wearing masks or not. Programs like this make public transportation safer during the coronavirus pandemic. Tumor Detection Brain tumors can be seen in MRI scans and are often detected using deep neural networks. Tumor detection software utilizing deep learning is crucial to the medical industry because it can detect tumors at high accuracy to help doctors make their diagnoses. New methods are constantly being developed to heighten the accuracy of these diagnoses. Disease Progression Score Computer vision can be used to identify critically ill patients to direct medical attention (critical patient screening). People infected with COVID-19 are found to have more rapid respiration. Deep Learning with depth cameras can be used to identify abnormal respiratory patterns to perform an accurate and unobtrusive yet large-scale screening of people infected with the COVID-19 virus. Healthcare and Rehabilitation Physical therapy is important for the recovery training of stroke survivors and sports injury patients. The main challenges are related to the costs of supervision by a medical professional, hospital, or agency. Home training with a vision-based rehabilitation application is preferred because it allows people to practice movement training privately and economically. In computer-aided therapy or rehabilitation, human action evaluation can be applied to assist patients in training at home, guide them to perform actions properly, and prevent further injuries. Explore more sports and fitness applications. Medical Skill Training Computer Vision applications are used for assessing the skill level of expert learners on self-learning platforms. For example, augmented reality simulation-based surgical training platforms have been developed for surgical education. In addition, the technique of action quality assessment makes it possible to develop computational approaches that automatically evaluate the surgical students\u2019 performance. Accordingly, meaningful feedback information can be provided to individuals and guide them to improve their skill levels. Computer Vision in Agriculture Read our agriculture industry report here. Animal Monitoring Animal monitoring with computer vision is a key strategy of smart farming. Machine learning uses camera streams to monitor the health of specific livestock such as pigs, cattle, or poultry. Smart vision systems aim to analyze animal behavior to increase productivity, health, and welfare of the animals and thereby influence yields and economic benefits in the industry. Farm Automation Technologies such as harvest, seeding, and weeding robots, autonomous tractors, and vision systems to monitor remote farms, and drones for visual inspection can maximize productivity with labor shortages. The profitability can be significantly increased by automating manual inspection with AI vision, reducing the ecological footprint, and improving decision-making processes. Crop Monitoring The yield and quality of important crops such as rice and wheat determine the stability of food security. Traditionally, crop growth monitoring mainly relies on subjective human judgment and is not timely or accurate. Computer Vision applications allow us to continuously and non-destructively monitor plant growth and the response to nutrient requirements. Compared with manual operations, the real-time monitoring of crop growth by applying computer vision technology can detect the subtle changes in crops due to malnutrition much earlier and can provide a reliable and accurate basis for timely regulation. In addition, computer vision applications can be used to measure plant growth indicators or determine the growth stage. Flowering Detection The heading date of wheat is one of the most important parameters for wheat crops. An automatic computer vision observation system can be used to determine the wheat heading period. Computer vision technology has the advantages of low cost, small error, high efficiency, and good robustness and can be dynamically and continuously analyzed. Plantation Monitoring In intelligent agriculture, image processing with drone images can be used to monitor palm oil plantations remotely. With geospatial orthophotos, it is possible to identify which part of the plantation land is fertile for planted crops. It was also possible to identify areas less fertile in terms of growth. OpenCV is a popular tool for such image-processing tasks. Insect Detection Rapid and accurate recognition and counting of flying insects are of great importance, especially for pest control. However, traditional manual identification and counting of flying insects are inefficient and labor-intensive. Vision-based systems allow the counting and recognizing of flying insects (based on You Only Look Once (YOLO) object detection and classification). Plant Disease Detection Automatic and accurate estimation of disease severity is essential for food security, disease management, and yield loss prediction. The deep learning method avoids labor-intensive feature engineering and threshold-based image segmentation. Automatic image-based plant disease severity estimation using Deep convolutional neural network (CNN) applications was developed, for example, to identify apple black rot. Automatic Weeding Weeds are considered to be harmful plants in agronomy because they compete with crops to obtain water, minerals, and other nutrients in the soil. Spraying pesticides only in the exact locations of weeds greatly reduces the risk of contaminating crops, humans, animals, and water resources. The intelligent detection and removal of weeds are critical to the development of agriculture. A neural network-based computer vision system can be used to identify potato plants and three different weeds for on-site specific spraying. Automatic Harvesting In traditional agriculture, mechanical operations are reliant, on manual harvesting as the mainstay, which results in high costs and low efficiency. However, in recent years, with the continuous application of computer vision technology, high-end intelligent agricultural harvesting machines, such as harvesting machinery and picking robots based on computer vision technology, have emerged in agricultural production, which has been a new step in the automatic harvesting of crops. The main focus of harvesting operations is to ensure product quality during harvesting to maximize the market value. Computer Vision-powered applications include picking cucumbers automatically in a greenhouse environment or the automatic identification of cherries in a natural environment. Agricultural Product Quality Testing The quality of agricultural products is one of the important factors affecting market prices and customer satisfaction. Compared to manual inspections, Computer Vision provides a way to perform external quality checks. AI vision systems can achieve high degrees of flexibility and repeatability at a relatively low cost and with high precision. For example, systems based on machine vision and computer vision are used for rapid testing of sweet lemon damage or non-destructive quality evaluation of potatoes. Irrigation Management Soil management based on using technology to enhance soil productivity through cultivation, fertilization, or irrigation has a notable impact on modern agricultural production. By obtaining useful information about the growth of horticultural crops through images, the soil water balance can be accurately estimated to achieve accurate irrigation planning. Computer vision applications provide valuable information about the irrigation management water balance. A vision-based system can process multi-spectral images taken by unmanned aerial vehicles (UAVs) and obtain the vegetation index (VI) to provide decision support for irrigation management. UAV Farmland Monitoring Real-time farmland information and an accurate understanding of that information play a basic role in precision agriculture. Over recent years, drones (UAV), as a rapidly advancing technology, have allowed the acquisition of agricultural information that has a high resolution, low cost, and fast solutions. In addition, UAV platforms equipped with image sensors provide detailed information on agricultural economics and crop conditions (for example, continuous crop monitoring). As a result, UAV remote sensing has contributed to an increase in agricultural production with a decrease in agricultural costs. Yield Assessment Through the application of computer vision technology, the functions of soil management, maturity detection, and yield estimation for farms have been realized. Moreover, the existing technology can be well applied to methods such as spectral analysis and deep learning. Most of these methods have the advantages of high precision, low cost, good portability, good integration, and scalability and can provide reliable support for management decision-making. An example is the estimation of citrus crop yield via fruit detection and counting using computer vision. Also, the yield from sugarcane fields can be predicted by processing images obtained using UAVs. Computer Vision in Transportation Read our smart city industry report here. Vehicle Classification Computer Vision applications for automated vehicle classification have a long history. The technologies for automated vehicle classification for vehicle counting have been evolving over the decades. Deep learning methods make it possible to implement large-scale traffic analysis systems using common, inexpensive security cameras. With rapidly growing affordable sensors such as closed\u2010circuit television (CCTV) cameras, light detection and ranging (LiDAR), and even thermal imaging devices, vehicles can be detected, tracked, and categorized in multiple lanes simultaneously. The accuracy of vehicle classification can be improved by combining multiple sensors such as thermal imaging, and LiDAR imaging with RGB cameras (common surveillance, IP cameras). In addition, there are multiple specializations; for example, a deep-learning-based computer vision solution for construction vehicle detection has been employed for purposes such as safety monitoring, productivity assessment, and managerial decision-making. Moving Violations Detection Law enforcement agencies and municipalities are increasing the deployment of camera\u2010based roadway monitoring systems to reduce unsafe driving behavior. Probably the most critical application is the detection of stopped vehicles in dangerous areas. Also, there is increasing use of computer vision techniques in smart cities that involve automating the detection of violations such as speeding, running red lights or stop signs, wrong\u2010way driving, and making illegal turns. Traffic Flow Analysis Traffic flow analysis has been studied extensively for intelligent transportation systems (ITS) using invasive methods (tags, under-pavement coils, etc.) and non-invasive methods such as cameras. With the rise of computer vision and AI, video analytics can now be applied to ubiquitous traffic cameras, which can generate a vast impact in ITS and smart cities. The traffic flow can be observed using computer vision means and measure some of the variables required by traffic engineers. Parking Occupancy Detection Visual parking space monitoring is used with the goal of parking lot occupancy detection. Especially in smart cities, computer vision applications power decentralized and efficient solutions for visual parking lot occupancy detection based on a deep Convolutional Neural Network (CNN). There exist multiple datasets for parking lot detection, such as PKLot\u00a0and\u00a0CNRPark-EXT. Furthermore, video-based parking management systems have been implemented using stereoscopic imaging (3D) or thermal cameras. The advantage of camera-based parking lot detection is the scalability for large-scale use, inexpensive maintenance, and installation, especially since it is possible to reuse security cameras. Automated License Plate Recognition (ALPR) Many modern transportation and public safety systems rely on recognizing and extracting license plate information from still images or videos. Automated license plate recognition (ALPR) has in many ways transformed the public safety and transportation industries. Such number plate recognition systems enable modern tolled roadway solutions, providing tremendous operational cost savings via automation and even enabling completely new capabilities in the marketplace (such as police cruiser\u2010mounted license plate reading units). OpenALPR is a popular automatic number-plate recognition library based on optical character recognition (OCR) on images or video feeds of vehicle registration plates. Vehicle re-identification With improvements in person re-identification, smart transportation, and surveillance systems aim to replicate this approach for vehicles using vision-based vehicle re-identification. Conventional methods to provide a unique vehicle ID are usually intrusive (in-vehicle tag, cellular phone, or GPS). For controlled settings such as at a toll booth, automatic number-plate recognition (ANPR) is probably the most suitable technology for the accurate identification of individual vehicles. However, license plates are subject to change and forgery, and ALPR cannot reflect salient specialties of the vehicles, such as marks or dents. Non-intrusive methods such as image-based recognition have high potential and demand but are still far from mature for practical usage. Most existing vision-based vehicle re-identification techniques are based on vehicle appearances such as color, texture, and shape. Today, the recognition of subtle, distinctive features such as vehicle make or year model is still an unresolved challenge. Pedestrian Detection The detection of pedestrians is crucial to intelligent transportation systems (ITS). Use cases range from self-driving cars to infrastructure surveillance, traffic management, transit safety and efficiency, and law enforcement. Pedestrian detection involves many types of sensors, such as traditional CCTV or IP cameras, thermal imaging devices, near\u2010infrared imaging devices, and onboard RGB cameras. A person detection algorithm, or people detector, can be based on infrared signatures, shape features, gradient features, machine learning, or motion features. Pedestrian detection relying on deep convolution neural networks (CNN) has made significant progress, even with the detection of heavily occluded pedestrians. Traffic Sign Detection Computer Vision applications are useful for traffic sign detection and recognition. Vision techniques are applied to segment traffic signs from different traffic scenes (using image segmentation) and employ deep learning algorithms to recognize and classify traffic signs. Collision Avoidance Systems Vehicle detection and lane detection form an integral part of most autonomous vehicle advanced driver assistance systems (ADAS). Deep neural networks have been used recently to investigate deep learning and its use for autonomous collision avoidance systems. Road Condition Monitoring Computer vision-based defect detection and condition assessment are developed to monitor concrete and asphalt civil infrastructure. Pavement condition assessment provides information to make more cost-effective and consistent decisions regarding the management of pavement networks. Generally, pavement distress inspections are performed using sophisticated data collection vehicles and/or foot-on-ground surveys. A Deep Machine Learning Approach to develop an asphalt pavement condition index was developed to provide a human-independent, inexpensive, efficient, and safe way of automated pavement distress detection via Computer Vision. Another application is the visual inspection of roads to detect road potholes and allocate road maintenance to reduce related vehicle accidents. Infrastructure Condition Assessment To ensure civil infrastructure\u2019s safety and serviceability, it is essential to visually inspect and assess its physical and functional condition. Systems for Computer Vision-based civil infrastructure inspection and monitoring automatically convert image and video data into actionable information. Smart inspection applications identify structural components, characterize visible damage, and detect changes from reference images. Such monitoring applications include static measurement of strain and displacement and dynamic measurement of displacement for modal analysis. Driver Attentiveness Detection Distracted driving detection \u2013 such as daydreaming, cell phone usage, and looking at something outside the car \u2013 accounts for a large proportion of road traffic fatalities worldwide. Artificial intelligence helps understand driving behaviors and find solutions to mitigate road traffic incidents. Road surveillance technologies observe passenger compartment violations, for example, in deep learning-based seat belt detection in road surveillance. In\u2010vehicle driver monitoring technologies focus on visual sensing, analysis, and feedback. Driver behavior can be inferred both directly from inward driver\u2010facing cameras and indirectly from outward scene\u2010facing cameras or sensors. Techniques based on driver-facing video analytics detect the face and eyes with algorithms for gaze direction, head pose estimation, and facial expression monitoring. Face detection algorithms have been able to detect attentive vs. inattentive faces. Deep Learning algorithms can detect differences between focused and unfocused eyes, as well as signs of driving under the influence. Multiple vision-based applications for real-time distracted driver posture classification with multiple deep learning methods (RNN and CNN) are used in real-time distraction detection. Computer Vision in Retail Read our retail industry report here. Customer Tracking Deep learning algorithms can process the video streams in real time to analyze customer footfall in retail stores. Camera-based methods allow re-using the video stream of common, inexpensive security surveillance cameras. Machine learning algorithms detect people anonymously and contactless to analyze time spent in different areas, waiting times, queueing time, and assess the service quality. Customer behavior analytics can improve retail store layouts, increase customer satisfaction, and quantify key metrics across multiple locations. People Counting Computer Vision algorithms learn from training data to detect humans and count them in real-time. People counting technology helps stores collect data about their stores\u2019 success. During the COVID-19 pandemic, it tracked customers when a limited number of people could enter stores. Theft Detection Retailers can detect suspicious behavior, such as loitering or accessing off-limits areas, using computer vision algorithms that autonomously analyze the scene. Waiting Time Analytics To prevent impatient customers and endless waiting lines, retailers are implementing queue detection technology. Queue detection uses cameras to track and count the number of shoppers in a line. Once a threshold of customers is reached, the system sounds an alert for clerks to open new checkouts. Social Distancing To ensure adherence to safety precautions, companies implement distance detectors. A camera tracks employee or customer movement and uses depth sensors to assess the distance between them. Then, depending on their position, the system draws a red or green circle around the person. Learn more about Social Distancing Monitoring with deep learning. Computer Vision in Sports For a comprehensive report, explore our article about computer vision in sports. Player Pose Tracking AI vision recognizes patterns between human body movement and poses over multiple frames in video footage or streams. For example, human pose estimation has been applied to real-world videos of swimmers where single stationary cameras film above and below the water surface. A popular open-source tool for this is Openpose, useful for real-time keypoint detection. Those video recordings can quantitatively assess the athletes\u2019 performance without manual body part annotation in each video frame. Thus, Convolutional Neural Networks automatically infer the required pose information and detect athletes\u2019 swimming styles. Markerless Motion Capture Cameras use pose estimation to track the motion of the human skeleton without using traditional optical markers and specialized cameras. This is essential in sports capture, where players cannot play with the burden of additional performance capture attire or devices. Performance Assessment Automated detection and recognition of sport-specific movements overcome the limitations associated with manual performance analysis methods (subjectivity, quantification, reproducibility). Computer Vision data inputs can be used in combination with the data of body-worn sensors and wearables. Popular use cases are swimming analysis, golf swing analysis, over-ground running analytics, alpine skiing, and the detection and evaluation of cricket bowling. Multi-Player Pose Tracking Using Computer Vision algorithms, the human pose and body movement of multiple team players can be calculated from both monocular (single-camera footage), and multi-view (footage of multiple cameras) sports video datasets. 2D or 3D Pose of multiple players in sports is useful in performance analysis, motion capture, and novel applications in broadcast and immersive media. Stroke Recognition Computer vision applications are capable of detecting and classifying strokes (for example, classifying\u00a0strokes in table tennis). Movement recognition or classification involves interpretations and labeled predictions of the identified instance. I.e., differentiating tennis strokes as forehand or backhand). Stroke recognition aims to provide tools to analyze table tennis games and to improve sports skills more efficiently. Real-Time Coaching Computer Vision-based sports video analytics help to improve resource efficiency and reduce feedback times for time-constraint tasks. Coaches and athletes involved in time-intensive notational tasks, including post-swim race analysis, can review rapid, objective feedback before the next race. Self-training systems for sports exercise is a similar recently emerging computer vision research topic. While self-training is essential in sports exercise, a practitioner may progress to a limited extent without a coach\u2019s instruction. For example, a yoga self-training application aims to instruct the practitioner to perform yoga poses correctly. Thus, assisting in rectifying poor postures and preventing injury. In addition, vision-based self-training systems can give instructions on how to adjust body posture. Sports Team Analysis Professional sports analysts regularly perform analysis to gain strategic and tactical insights into player and team behavior. However, manual video analysis is typically time-consuming, where the analysts need to memorize and annotate scenes. Computer Vision techniques extract trajectory data from video material and apply movement analysis techniques to analyze: Ball Tracking Real-time object tracking detects and captures object movement patterns. Ball trajectory data is fundamental in evaluating players\u2019 performance and analysis of game strategies. Hence, tracking of ball movement detects and then tracks the ball across multiple video frames. Ball tracking is important in sports with large fields to help interpret and analyze a sports game and tactics faster. Goal-Line Technology Camera-based systems can determine the validity of a goal to support the decision-making of referees. Unlike sensors, the AI vision-based method is noninvasive and does not require changes to typical football devices. Such Goal-Line Technology systems are based on high-speed cameras with images to triangulate the ball\u2019s position. A ball detection algorithm that analyzes candidate ball regions to recognize the ball pattern. Event Detection in Sports We can detect complex events from unstructured videos, like scoring a goal, near misses, or other non-scoring actions. This technology is useful for real-time event detection in sports broadcasts, applicable to a wide range of field sports. Highlight Generation Producing sports highlights is labor-intensive work requiring some degree of specialization. This is especially true for sports with complex rules and extensive play periods (e.g., Cricket). Automatic Cricket highlight generation can use event-driven and excitement-based features to recognize and clip important match events. Another application is the automatic curation of golf highlights using multimodel excitement features with Computer Vision. Sports Activity Scoring Deep Learning methods can assess athletes\u2019 action quality (Deep Features for Sports Activity Scoring). For example, automatic sports activity scoring for diving, figure skating, or vaulting. ScoringNet is a 3D model CNN network application for sports activity scoring. For example, a diving scoring application works by assessing the quality score of a diving performance of an athlete. It analyzes the position of the athlete\u2019s feet to determine: AI Vision Industry Guides and Applications of Computer Vision Projects Organizations use deep and machine learning technology to create countless computer vision algorithms and applications across industry lines. Read our industry guides to find more industry-specific applications and get computer vision ideas from real-world case studies. Enterprise Computer AI Vision Algorithms and Applications At viso.ai, we power Viso Suite, the most complete end-to-end computer vision platform. Industry leaders, Fortune 100, and governmental organizations develop and deploy their computer vision applications with software infrastructure from viso.ai. Viso Suite provides full-scale features to rapidly build, deploy, and scale enterprise-grade computer vision applications. Viso helps to overcome integration hassles, privacy, security, and scalability challenges \u2013 without writing code from scratch. We provide all the computer vision services and AI vision experience you\u2019ll need. Get a demo of Viso Suite by AI experts and schedule a demo to see the key features. All-in-one platform to build, deploy, and scale computer vision applications viso.ai Product Features Industries Resources About Privacy Overview"}
{"url": "https://www.ibm.com/topics/computer-vision", "title": "What is Computer Vision? | IBM", "content": "Computer vision is a field of artificial intelligence (AI) that uses machine learning and neural networks to teach computers and systems to derive meaningful information from digital images, videos and other visual inputs\u2014and to make recommendations or take actions when they see defects or issues. \u00a0 If AI enables computers to think, computer vision enables them to see, observe and understand.\u00a0 Computer vision works much the same as human vision, except humans have a head start. Human sight has the advantage of lifetimes of context to train how to tell objects apart, how far away they are, whether they are moving or something is wrong with an image. Computer vision trains machines to perform these functions, but it must do it in much less time with cameras, data and algorithms rather than retinas, optic nerves and a visual cortex. Because a system trained to inspect products or watch a production asset can analyze thousands of products or processes a minute, noticing imperceptible defects or issues, it can quickly surpass human capabilities. Computer vision is used in industries that range from energy and utilities to manufacturing and automotive\u2014and the market is continuing to grow. It is expected to reach USD 48.6 billion by 2022.1 With ESG disclosures starting as early as 2025 for some companies, make sure that you're prepared with our guide. Register for the playbook on smarter asset management Computer vision needs lots of data. It runs analyses of data over and over until it discerns distinctions and ultimately recognize images. For example, to train a computer to recognize automobile tires, it needs to be fed vast quantities of tire images and tire-related items to learn the differences and recognize a tire, especially one with no defects. Two essential technologies are used to accomplish this: a type of machine learning called deep learning and a convolutional neural network (CNN). Machine learning uses algorithmic models that enable a computer to teach itself about the context of visual data. If enough data is fed through the model, the computer will \u201clook\u201d at the data and teach itself to tell one image from another. Algorithms enable the machine to learn by itself, rather than someone programming it to recognize an image. A CNN helps a machine learning or deep learning model \u201clook\u201d by breaking images down into pixels that are given tags or labels. It uses the labels to perform convolutions (a mathematical operation on two functions to produce a third function) and makes predictions about what it is \u201cseeing.\u201d The neural network runs convolutions and checks the accuracy of its predictions in a series of iterations until the predictions start to come true. It is then recognizing or seeing images in a way similar to humans. Much like a human making out an image at a distance, a CNN first discerns hard edges and simple shapes, then fills in information as it runs iterations of its predictions. A CNN is used to understand single images. A recurrent neural network (RNN) is used in a similar way for video applications to help computers understand how pictures in a series of frames are related to one another. Scientists and engineers have been trying to develop ways for machines to see and understand visual data for about 60 years. Experimentation began in 1959 when neurophysiologists showed a cat an array of images, attempting to correlate a response in its brain. They discovered that it responded first to hard edges or lines and scientifically, this meant that image processing starts with simple shapes like straight edges.2 At about the same time, the first computer image scanning technology was developed, enabling computers to digitize and acquire images. Another milestone was reached in 1963 when computers were able to transform two-dimensional images into three-dimensional forms. In the 1960s, AI emerged as an academic field of study and it also marked the beginning of the AI quest to solve the human vision problem. 1974 saw the introduction of optical character recognition (OCR) technology, which could recognize text printed in any font or typeface.3\u00a0Similarly, intelligent character recognition (ICR) could decipher hand-written text that is using neural networks.4\u00a0Since then, OCR and ICR have found their way into document and invoice processing, vehicle plate recognition, mobile payments, machine conversion and other common applications. In 1982, neuroscientist David Marr established that vision works hierarchically and introduced algorithms for machines to detect edges, corners, curves and similar basic shapes. Concurrently, computer scientist Kunihiko Fukushima developed a network of cells that could recognize patterns. The network, called the Neocognitron, included convolutional layers in a neural network. By 2000, the focus of study was on object recognition; and by 2001, the first real-time face recognition applications appeared. Standardization of how visual data sets are tagged and annotated emerged through the 2000s. In 2010, the ImageNet data set became available. It contained millions of tagged images across a thousand object classes and provides a foundation for CNNs and deep learning models used today. In 2012, a team from the University of Toronto entered a CNN into an image recognition contest. The model, called AlexNet, significantly reduced the error rate for image recognition. After this breakthrough, error rates have fallen to just a few percent.5 Access videos, papers, workshops and more. There is a lot of research being done in the computer vision field, but it doesn't stop there. Real-world applications demonstrate how important computer vision is to endeavors in business, entertainment, transportation, healthcare and everyday life. A key driver for the growth of these applications is the flood of visual information flowing from smartphones, security systems, traffic cameras and other visually instrumented devices. This data could play a major role in operations across industries, but today goes unused. The information creates a test bed to train computer vision applications and a launchpad for them to become part of a range of human activities: Many organizations don\u2019t have the resources to fund computer vision labs and create deep learning models and neural networks. They may also lack the computing power that is required to process huge sets of visual data. Companies such as IBM are helping by offering computer vision software development services. These services deliver pre-built learning models available from the cloud\u2014and also ease demand on computing resources. Users connect to the services through an application programming interface (API) and use them to develop computer vision applications. IBM has also introduced a computer vision platform that addresses both developmental and computing resource concerns. IBM Maximo\u00ae Visual Inspection includes tools that enable subject matter experts to label, train and deploy deep learning vision models\u2014without coding or deep learning expertise. The vision models can be deployed in local data centers, the cloud and edge devices. While it\u2019s getting easier to obtain resources to develop computer vision applications, an important question to answer early on is: What exactly will these applications do? Understanding and defining specific computer vision tasks can focus and validate projects and applications and make it easier to get started. Here are a few examples of established computer vision tasks: Put the power of computer vision into the hands of your quality and inspection teams. IBM Maximo Visual Inspection makes computer vision with deep learning more accessible to business users with visual inspection tools that empower. IBM Research is one of the world\u2019s largest corporate research labs. Learn more about research being done across industries. Learn about the evolution of visual inspection and how artificial intelligence is improving safety and quality. Learn more about getting started with visual recognition and IBM Maximo Visual Inspection. Explore resources and courses for developers. Read how Sund & Baelt used computer vision technology to streamline inspections and improve productivity. Learn how computer vision technology can improve quality inspections in manufacturing. Unleash the power of no-code computer vision for automated visual inspection with IBM Maximo Visual Inspection\u2014an intuitive toolset for labelling, training, and deploying artificial intelligence vision models. \n                            \n\n\n\n  \n    Sources:\n\n\n\n\n\n\n    \n\n\n                         1. https://www.forbes.com/sites/bernardmarr/2019/04/08/7-amazing-examples-of-computer-and-machine-vision-in-practice/#3dbb3f751018\u00a0(link resides outside ibm.com) 2.\u00a0https://hackernoon.com/a-brief-history-of-computer-vision-and-convolutional-neural-networks-8fe8aacc79f3 (link resides outside ibm.com) 3. Optical character recognition, Wikipedia\u00a0(link resides outside ibm.com) 4. Intelligent character recognition, Wikipedia\u00a0(link resides outside ibm.com) 5. A Brief History of Computer Vision (and Convolutional Neural Networks), Rostyslav Demush, Hacker Noon, February 27, 2019\u00a0(link resides outside ibm.com) 6. 7 Amazing Examples of Computer And Machine Vision In Practice, Bernard Marr, Forbes, April 8, 2019\u00a0(link resides outside ibm.com) 7. The 5 Computer Vision Techniques That Will Change How You See The World, James Le, Heartbeat, April 12, 2018\u00a0(link resides outside ibm.com)"}
{"url": "https://www.javatpoint.com/computer-vision-applications", "title": "Computer Vision Applications - javaTpoint", "content": "Python AI, ML and Data Science Java B.Tech and MCA Web Technology Software Testing Technical Interview Java Interview Web Interview Database Interview Company Interviews Computer Vision Tutorial Computer Vision Applications Computer vision is a subfield of AI (Artificial Intelligence), which enables machines to derive some meaningful information from any image, video, or other visual input and perform the required action on that information. Computer vision is like eyes for an AI system, which means if AI enables the machine to think, computer vision enables the machines to see and observe the visual inputs. Computer vision technology is based on the concept of teaching computers to process an image or a visual input at pixels and derive meaningful information from it. Nowadays, Computer vision is in great demand and used in different areas, including robotics, manufacturing, healthcare, etc. In this topic, we will discuss some popular applications of Computer Vision, but before that, let's first understand some common tasks that are performed by computer vision. Below are some common tasks for which computer vision can be used: Computer Vision Applications As per the increasing demand for AI and Machine Learning technologies, computer vision also has a great demand among different sectors. It has a massive impact on different industries, including retail, security, healthcare, automotive, agriculture, etc. Below are some most popular applications of computer vision: Above are some most common applications of Computer vision. Now let us discuss applications of computer vision across different sectors such as Retail, healthcare, etc. 1. Computer Vision in Healthcare The Healthcare industry is rapidly adopting new technologies and automation solutions, one of which is computer vision. In the healthcare industry, computer vision has the following applications: 2. Computer Vision in Transportation With the enhanced demand for the transportation sector, there has occurred various technological development in this industry, and one of such technologies is Computer vision. Below are some popular applications of computer vision in the transportation industry: 3. Computer Vision in Manufacturing In the manufacturing industry, the demand for automation is at its peak. Many tasks have already been automated, and other new technology innovations are in trend. For providing these automatic solutions, Computer vision is also widely used. Below are some most popular applications 4. Computer Vision in Agriculture In the agriculture sector, Machine Learning has made a great contribution with its models, including Computer vision. It can be used in areas such as crop monitoring, weather analysis, etc. Below are some popular cases of computer vision applications in Agriculture: 5. Computer Vision in Retail In the retail sector, computer vision system enables retailers to collect a huge volume of visual data and hence design better customer experiences with the help of cameras installed in stores. Some popular applications of computer vision in the retail industry are given below: Learn Important Tutorial B.Tech / MCA Preparation We provides tutorials and interview questions of all technology like java tutorial, android, java frameworks  Contact info  G-13, 2nd Floor, Sec-3, Noida, UP, 201301, India [email\u00a0protected]. Follow us Latest Post PRIVACY POLICY Tutorials Interview Questions Online Compiler"}
{"url": "https://www.geeksforgeeks.org/what-is-reinforcement-learning/", "title": "Reinforcement learning - GeeksforGeeks", "content": "Reinforcement learning Reinforcement Learning: An Overview Reinforcement Learning (RL) is a branch of machine learning focused on making decisions to maximize cumulative rewards in a given situation. Unlike supervised learning, which relies on a training dataset with predefined answers, RL involves learning through experience. In RL, an agent learns to achieve a goal in an uncertain, potentially complex environment by performing actions and receiving feedback through rewards or penalties. Key Concepts of Reinforcement Learning How Reinforcement Learning Works RL operates on the principle of learning optimal behavior through trial and error. The agent takes actions within the environment, receives rewards or penalties, and adjusts its behavior to maximize the cumulative reward. This learning process is characterized by the following elements: Example: Navigating a Maze The problem is as follows: We have an agent and a reward, with many hurdles in between. The agent is supposed to find the best possible path to reach the reward. The following problem explains the problem more easily.\u00a0\u00a0  The above image shows the robot, diamond, and fire. The goal of the robot is to get the reward that is the diamond and avoid the hurdles that are fired. The robot learns by trying all the possible paths and then choosing the path which gives him the reward with the least hurdles. Each right step will give the robot a reward and each wrong step will subtract the reward of the robot. The total reward will be calculated when it reaches the final reward that is the diamond.\u00a0 Main points in Reinforcement learning \u2013\u00a0 Difference between Reinforcement learning and Supervised learning:\u00a0 Types of Reinforcement:\u00a0 Elements of Reinforcement Learning i) Policy: Defines the agent\u2019s behavior at a given time. ii) Reward Function: Defines the goal of the RL problem by providing feedback. iii) Value Function: Estimates long-term rewards from a state. iv) Model of the Environment: Helps in predicting future states and rewards for planning. Example: CartPole Environment in OpenAI Gym The CartPole environment is a classic reinforcement learning problem where the goal is to balance a pole on a cart by applying forces to the left or right. Output: Explanation: Application of Reinforcement Learnings\u00a0 i) Robotics: Automating tasks in structured environments like manufacturing. ii) Game Playing: Developing strategies in complex games like chess. iii) Industrial Control: Real-time adjustments in operations like refinery controls. iv) Personalized Training Systems: Customizing instruction based on individual needs. Advantages and Disadvantages of Reinforcement Learning Advantages: 1. Reinforcement learning can be used to solve very complex problems that cannot be solved by conventional techniques. 2. The model can correct the errors that occurred during the training process.\u00a0 3. In RL, training data is obtained via the direct interaction of the agent with the environment 4. Reinforcement learning can handle environments that are non-deterministic, meaning that the outcomes of actions are not always predictable. This is useful in real-world applications where the environment may change over time or is uncertain. 5. Reinforcement learning can be used to solve a wide range of problems, including those that involve decision making, control, and optimization. 6. Reinforcement learning is a flexible approach that can be combined with other machine learning techniques, such as deep learning, to improve performance. Disadvantages: 1. Reinforcement learning is not preferable to use for solving simple problems. 2. Reinforcement learning needs a lot of data and a lot of computation 3. Reinforcement learning is highly dependent on the quality of the reward function. If the reward function is poorly designed, the agent may not learn the desired behavior. 4. Reinforcement learning can be difficult to debug and interpret. It is not always clear why the agent is behaving in a certain way, which can make it difficult to diagnose and fix problems. Conclusion Reinforcement learning is a powerful technique for decision-making and optimization in dynamic environments. Its applications range from robotics to personalized learning systems. However, the complexity of RL requires careful design of reward functions and significant computational resources. By understanding its principles and applications, one can leverage RL to solve intricate real-world problems. You can also read our recent article on Implementation \u2013 Reinforcement Learning Algorithm  P Similar Reads  What kind of Experience do you want to share?"}
{"url": "https://www.simplilearn.com/tutorials/machine-learning-tutorial/reinforcement-learning", "title": "What Is Reinforcement Learning | Types of Reinforcement Learning", "content": "Tutorial Playlist  Machine Learning Tutorial: A Step-by-Step Guide for Beginners  An Introduction To Machine Learning  What is Machine Learning and How Does It Work?  Machine Learning Steps: A Complete Guide  Top 10 Machine Learning Applications in 2024  Different Types of Machine Learning: Exploring AI's Core  A Beginner's Guide to Supervised & Unsupervised Learning in AI  Everything You Need to Know About Feature Selection  Linear Regression in Python  Everything You Need to Know About Classification in Machine Learning  An Introduction to Logistic Regression in Python  Understanding the Difference Between Linear vs Logistic Regression  The Best Guide On How To Implement Decision Tree In Python  Random Forest Algorithm  Understanding Naive Bayes Classifier  The Best Guide to Confusion Matrix  How to Leverage KNN Algorithm in Machine Learning?  K-Means Clustering Algorithm: Applications, Types, Demos and Use Cases  PCA in Machine Learning: Your Complete Guide to Principal Component Analysis  What is Cost Function in Machine Learning  The Ultimate Guide to Cross-Validation in Machine Learning  An Easy Guide to Stock Price Prediction Using Machine Learning  What Is Reinforcement Learning: A Complete Guide  What Is Q-Learning: The Best Guide to Understand Q-Learning  The Best Guide to Regularization in Machine Learning  Everything You Need to Know About Bias and Variance  The Complete Guide on Overfitting and Underfitting in Machine Learning  Mathematics for Machine Learning - Important Skills You Must Possess  A One-Stop Guide to Statistics for Machine Learning  Embarking on a Machine Learning Career? Here\u2019s All You Need to Know  How to Become a Machine Learning Engineer?  Top 45 Machine Learning Interview Questions and Answers for 2025  Explaining the Concepts of Quantum Computing  Supervised Machine Learning: All You Need to Know  10 Machine Learning Platforms to Revolutionize Your Business  What Is Boosting in Machine Learning ?: A Comprehensive Guide  Machine Learning vs. Neural Networks: Understanding the Differences  Unlocking the Future: 5 Compelling Reasons to Master Machine Learning in 2024  Feature Engineering  How to Create a Fake News Detection System?  Automated Machine Learning: A Quick Guide What Is Reinforcement Learning Lesson 22 of 40By Simplilearn  Machine Learning Tutorial: A Step-by-Step Guide for Beginners  An Introduction To Machine Learning  What is Machine Learning and How Does It Work?  Machine Learning Steps: A Complete Guide  Top 10 Machine Learning Applications in 2024  Different Types of Machine Learning: Exploring AI's Core  A Beginner's Guide to Supervised & Unsupervised Learning in AI  Everything You Need to Know About Feature Selection  Linear Regression in Python  Everything You Need to Know About Classification in Machine Learning  An Introduction to Logistic Regression in Python  Understanding the Difference Between Linear vs Logistic Regression  The Best Guide On How To Implement Decision Tree In Python  Random Forest Algorithm  Understanding Naive Bayes Classifier  The Best Guide to Confusion Matrix  How to Leverage KNN Algorithm in Machine Learning?  K-Means Clustering Algorithm: Applications, Types, Demos and Use Cases  PCA in Machine Learning: Your Complete Guide to Principal Component Analysis  What is Cost Function in Machine Learning  The Ultimate Guide to Cross-Validation in Machine Learning  An Easy Guide to Stock Price Prediction Using Machine Learning  What Is Reinforcement Learning: A Complete Guide  What Is Q-Learning: The Best Guide to Understand Q-Learning  The Best Guide to Regularization in Machine Learning  Everything You Need to Know About Bias and Variance  The Complete Guide on Overfitting and Underfitting in Machine Learning  Mathematics for Machine Learning - Important Skills You Must Possess  A One-Stop Guide to Statistics for Machine Learning  Embarking on a Machine Learning Career? Here\u2019s All You Need to Know  How to Become a Machine Learning Engineer?  Top 45 Machine Learning Interview Questions and Answers for 2025  Explaining the Concepts of Quantum Computing  Supervised Machine Learning: All You Need to Know  10 Machine Learning Platforms to Revolutionize Your Business  What Is Boosting in Machine Learning ?: A Comprehensive Guide  Machine Learning vs. Neural Networks: Understanding the Differences  Unlocking the Future: 5 Compelling Reasons to Master Machine Learning in 2024  Feature Engineering  How to Create a Fake News Detection System?  Automated Machine Learning: A Quick Guide Table of Contents Reinforcement Learning (RL) is an interesting domain of artificial intelligence that simulates the learning process by trial and error, mimicking how humans and animals learn from the consequences of their actions. At its core, RL involves an agent that makes decisions in a dynamic environment to achieve a set of objectives, aiming to maximize cumulative rewards. Unlike traditional machine learning paradigms, where models learn from a fixed data set, RL agents learn from continuous feedback and are refined as they interact with their environment.  Learn Core AI Engineering Skills and Tools   What Is Reinforcement Learning? Reinforcement Learning (RL) is a branch of machine learning that teaches agents how to make decisions by interacting with an environment to achieve a goal. In RL, an agent learns to perform tasks by trying different strategies to maximize cumulative rewards based on feedback received through its actions. Benefits of Reinforcement Learning 1. Autonomous Learning Ability One of the primary benefits of reinforcement learning is its ability to enable autonomous learning. This allows systems to improve over time based on their interactions with the environment without requiring explicit instructions. This makes reinforcement learning particularly effective in handling complex, unpredictable scenarios, such as those in robotics, gaming, and autonomous vehicles. 2. Personalized Solutions Reinforcement learning facilitates highly personalized solutions, such as adaptive recommendation systems that evolve with user behavior. Its capability to optimize resource allocation benefits various industries, leading to cost savings and efficiency improvements. The scalability of reinforcement learning allows it to be applied across various domains, from simple tasks to complex systems, without significant modifications. 3. Minimal Human Intervention\u00a0 Reinforcement learning requires minimal supervision, reducing the need for extensive labeled data and manual intervention. It also encourages the exploration of new strategies, potentially uncovering more effective solutions that traditional methods might miss. Need for Reinforcement Learning\u00a0 Reinforcement Learning (RL) addresses several unique challenges and needs in machine learning and artificial intelligence, making it indispensable for various applications. Here are some of the key reasons that underline the need for Reinforcement Learning: 1. Decision Making in Uncertain Environments RL is particularly well-suited for scenarios where the environment is complex and uncertain, and the consequences of decisions unfold over time. This is common in real-world situations such as robotic navigation, stock trading, or resource management, where actions now affect future opportunities and outcomes. 2. Learning from Interaction Unlike supervised learning, RL does not require labeled input/output pairs. Instead, it learns from the consequences of its actions through trial and error. This aspect is crucial in environments where it is impractical or impossible to provide the correct decision-making examples beforehand. 3. Development of Autonomous Systems RL enables creation of truly autonomous systems that can improve their behavior over time without human intervention. This is essential for developing systems like autonomous vehicles, drones, or automated trading systems that must operate independently in dynamic and complex environments. 4. Optimization of Performance RL optimizes an objective over time, making it ideal for applications that enhance performance metrics, such as reducing costs, increasing efficiency, or maximizing profits in various operations. 5. Adaptability and Flexibility RL agents can adapt their strategies based on the feedback from the environment. This adaptability is vital in applications where conditions change dynamically, such as adapting to new financial market conditions or adjusting strategies in real-time strategy games. 6. Complex Chain of Decisions RL can handle situations where decisions are not isolated but part of a sequence that leads to a long-term outcome. This capability is important in scenarios like healthcare treatment planning, where a series of treatment decisions cumulatively affects a patient's health outcome. 7. Balancing Exploration and Exploitation RL algorithms are designed to balance exploration (trying untested actions to discover new knowledge) and exploitation (using known information to achieve rewards). This balance is crucial in many fields, such as e-commerce for recommending new products vs. popular ones or in energy management for experimenting with new resource allocations to find the most efficient strategies. 8. Personalization In environments where personalized feedback is crucial, such as personalized learning or individualized marketing strategies, RL can tailor strategies based on individual interactions and preferences, continually improving the personalization based on ongoing engagement.  Advance Your Career With Top AI Engineering Skills  Supervised vs Unsupervised vs Reinforcement Learning Here's a comparative table outlining the key differences between Supervised Learning, Unsupervised Learning, and Reinforcement Learning: Aspect Supervised Learning Unsupervised Learning Reinforcement Learning Definition Learning from labeled data to predict outcomes for new data. Learning from unlabeled data to identify patterns and structures. Learning to make decisions by performing actions in an environment and receiving rewards or penalties. Data Requirement Requires a dataset with input-output pairs. Data must be labeled. Works with unlabeled data. No need for input-output pairs. No predefined dataset; learns from interactions with the environment through trial and error. Output A predictive model that maps inputs to outputs. Model that identifies the data's patterns, clusters, associations, or features. Policy or strategy that specifies the action to take in each state of the environment. Feedback Direct feedback (correct output is known). No explicit feedback. The algorithm infers structures. Indirect feedback (rewards or penalties after actions, not necessarily immediate). Goal Minimize the error between predicted and actual outputs. Discover the underlying structure of the data. Maximize cumulative reward over time. Examples Image classification, spam detection, regression tasks. Clustering, dimensionality reduction, market basket analysis. Video game AI, robotic control, dynamic pricing, personalized recommendations. Learning Approach Learns from examples provided during training. Learns patterns or features from data without specific guidance. Learns from the consequences of its actions rather than from direct instruction. Evaluation Typically evaluated on a separate test set using accuracy, precision, recall, etc. Evaluated based on metrics like silhouette score, within-cluster sum of squares, etc. Evaluated based on the amount of reward it can secure over time in the environment. Challenges Requires a large amount of labeled data, which can be expensive or impractical. Difficult to validate results as there is no true benchmark. Interpretation is often subjective. Requires a balance between exploration and exploitation and can be challenging in environments with sparse rewards. How Reinforcement Learning Works Reinforcement Learning (RL) operates similarly to how we learn through reinforcement, a concept rooted in behavioral psychology. Imagine a child learning that helping with chores earns praise while misbehaving leads to negative reactions. Over time, the child figures out which actions lead to positive outcomes. In RL, the algorithm learns in much the same way. It tries different actions to see which ones lead to positive or negative outcomes, ultimately figuring out the best way to achieve a desired goal. Key Concepts in Reinforcement Learning Basics of the Algorithm Reinforcement Learning is built on a framework known as the Markov Decision Process (MDP). In this process, time is divided into steps. At each step, the agent takes an action that changes the state of the environment. The current state depends on the sequence of previous actions. As the agent interacts with the environment through trial and error, it learns rules or strategies\u2014called policies\u2014to guide its actions toward maximizing rewards. A key challenge is the exploration-exploitation trade-off: deciding whether to explore new actions to discover better outcomes or stick with actions already known to yield high rewards. Types of Reinforcement Learning Algorithm Here's a breakdown of the types of reinforcement learning algorithms: 1. Model-Based RL In model-based reinforcement learning algorithm, the agent builds a model of the environment's dynamics. This model predicts the next state and the reward given the current state and action. The agent uses this model to plan actions by simulating possible future scenarios before deciding on the best action. This type of RL is appropriate for environments where building an accurate model is feasible, allowing for efficient exploration and planning. 2. Model-Free RL Model-free reinforcement learning algorithm does not require a model of the environment. Instead, the agent learns directly from interactions with the environment by trial and error. The agent learns to associate actions with rewards and uses this experience to improve decision-making over time. This type of reinforcement learning is suitable for complex environments where modeling the environment's dynamics is difficult or impossible. Value-Based Methods and Policy-Based Methods typically come under Model-Free Reinforcement Learning. Here's how they relate:  Learn Core AI Engineering Skills and Tools  Reinforcement Learning Terminologies Reinforcement Learning (RL) involves a variety of terms and concepts that are fundamental to understanding and implementing RL algorithms. Here\u2019s a list of some important terms commonly used in RL: 1. Agent The decision-maker in an RL setting interacts with the environment by performing actions based on its policy to maximize cumulative rewards. 2. Environment The external system with which the agent interacts during the learning process. It responds to the agent's actions by presenting new states and rewards. 3. State A description of the current situation in the environment. States can vary in complexity from simple numerical values to complex sensory inputs like images. 4. Action A specific step or decision taken by the agent to interact with the environment. The set of all possible actions available to the agent is known as the action space. 5. Reward A scalar feedback signal received by the agent from the environment indicates an action's effectiveness. The agent's goal is to maximize the sum of these rewards over time. 6. Policy (\u03c0) A strategy or rule that defines the agent\u2019s way of behaving at a given time. A policy maps states to actions, determining what action to take in each state. 7. Value Function A function that estimates how good it is for the agent to be in a particular state (State-Value Function) or how good it is to perform a particular action in a particular state (Action-Value Function). The \"goodness\" is defined in terms of expected future rewards. 8. Q-function (Action-Value Function) A function that estimates the total amount of rewards an agent can expect to accumulate over the future, starting from a given state and taking a particular action under a specific policy. 9. Model In model-based RL, the model predicts the next state and reward for each action taken in each state. In model-free RL, the agent learns directly from the experience without this model. 10. Exploration The act of trying new actions to discover more about the environment. Exploration helps the agent to learn about rewards associated with lesser-known actions. 11. Exploitation Using the known information to maximize the reward. Exploitation leverages the agent's current knowledge to perform the best-known action to gain the highest reward. 12. Discount Factor (\u03b3) A factor used in calculating the present value of future rewards. It determines the importance of future rewards. A discount factor close to 0 makes the agent short-sighted (more focused on immediate rewards), while a factor close to 1 makes it far-sighted (considering long-term rewards). 13. Temporal Difference (TD) Learning A method in RL where learning happens based on the difference between estimated values of the current state and the next state. It blends ideas from Monte Carlo methods and dynamic programming. 14. Monte Carlo Methods These methods learn directly from complete experience episodes without requiring a model of the environment. They average the returns received after visits to a particular state to estimate its value. 15. Bellman Equation A fundamental equation in dynamic programming that provides recursive relationships for the value functions, helping to decompose the decision-making process into simpler subproblems.  Become an AI Engineer in 11 Months  Reinforcement Learning Models 1. Traditional Reinforcement Learning Models Traditional reinforcement learning models are based on the foundational principles of RL, where an agent learns to make decisions through trial and error by interacting with an environment. These models often rely on tabular methods, like Q-learning and SARSA (detailed below), which use a table or matrix to store and update the values of different actions in various states.\u00a0 2. Deep Reinforcement Learning Models Deep reinforcement learning (Deep RL) models combine the principles of traditional RL with deep learning, allowing the agent to handle complex environments with high-dimensional inputs, such as images or continuous action spaces. Deep RL models are powerful in handling complex and large-scale problems, such as playing video games, robotics, and autonomous driving. They can process high-dimensional data and learn features automatically without manual feature engineering. Instead of using tables to store values, Deep RL models utilize neural networks to approximate the value functions or policies, explained in detail below: Reinforcement Learning in Python Implementing Reinforcement Learning (RL) in Python typically involves using specific libraries that facilitate the creation, manipulation, and visualization of RL models. Here\u2019s a guide on how to start with RL in Python, including a reinforcement learning example using one of the most popular libraries for RL, gym, from OpenAI. Step 1: Setting Up Your Environment Before you start, make sure you have Python installed on your machine. You will also need to install a few packages, primarily gym, an open-source library provided by OpenAI that offers various environments to test and develop RL algorithms. pip install gym Step 2: Importing Libraries After installing gym, you can start by importing it along with other necessary libraries: import gym import numpy as np Step 3: Creating the Environment One of the basic gym environments is the \"CartPole-v1,\" where the goal is to keep a pole balanced on a cart by moving the cart left or right. env = gym.make('CartPole-v1') Step 4: Implementing a Simple Agent We\u2019ll implement a very basic RL agent that randomly decides to move left or right without any learning involved to demonstrate the interaction with the environment. for episode in range(5):\u00a0 # Run 5 episodes \u00a0\u00a0\u00a0\u00a0state = env.reset()\u00a0 # Reset the environment for a new episode \u00a0\u00a0\u00a0\u00a0done = False \u00a0\u00a0\u00a0\u00a0step_count = 0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0while not done: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0env.render()\u00a0 # Render the environment to visualize the cart and pole \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0action = env.action_space.sample()\u00a0 # Randomly pick an action (0 or 1) \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0state, reward, done, info = env.step(action)\u00a0 # Execute the action \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0step_count += 1\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0if done: \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0print(f\"Episode {episode + 1} finished after {step_count} steps.\") \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0break env.close()\u00a0 # Close the environment Step 5: Adding Learning To make this reinforcement learning example into a learning agent, you would typically incorporate an RL algorithm like Q-learning, Deep Q-Networks (DQN), or policy gradients. These algorithms help the agent to learn from the outcomes of its actions rather than making random decisions. Reinforcement Learning Use Cases Reinforcement Learning (RL) is a powerful branch of machine learning used across various domains to optimize decision-making processes and improve performance over time based on feedback. Here are several key use cases where RL has been successfully applied: 1. Gaming and Simulations 2. Autonomous Vehicles 3. Finance 4. Healthcare 5. Robotics 6. Energy Systems 7. Supply Chain and Logistics 8. Advertising and Marketing 9. Natural Language Processing Dialogue Systems: RL is used in conversational agents to improve the quality of responses and the ability to handle a conversation through learning from user interactions. 10. Education Adaptive Learning Platforms: Customizing learning experiences to the needs of individual students, adapting the difficulty and topics to optimize learning outcomes. Applications of Reinforcement Learning Here's a concise list of key applications: Learn Core AI Engineering Skills and Tools Challenges with Reinforcement Learning Here are some of the key challenges with reinforcement learning: 1. Sample Efficiency RL algorithms often require a large number of interactions with the environment to learn effective policies. This can be particularly challenging in real-world scenarios where gathering data is expensive or time-consuming. 2. Credit Assignment Problem In RL, the agent may receive rewards after a series of actions, making it difficult to determine which actions were responsible for the outcome. This problem, known as the credit assignment problem, complicates the overall learning process. 3. Delayed Rewards In several environments, rewards are sparse or delayed, meaning the agent may perform many actions before receiving feedback. This makes it hard for the agent to learn which actions are beneficial. 4. Non-Stationary Environments Sometimes, the environment may change over time, making previously learned policies suboptimal or even harmful. 5. Lack of Interpretability The policies learned by RL agents, especially when using deep learning, can be difficult to interpret. This lack of transparency can be problematic in critical applications like healthcare or finance. Conclusion Reinforcement Learning (RL) stands out as a powerful branch of machine learning that empowers agents to make optimal decisions through trial and error, learning directly from their interactions with the environment. Unlike traditional forms of machine learning, RL does not require a predefined dataset; instead, it thrives on reward-based learning, making it highly adaptable to a wide array of complex and dynamic environments. RL's applications are diverse and transformative, from mastering games that challenge human intelligence to navigating the intricacies of autonomous vehicles and optimizing energy systems. Do you want to specialize in RL and understand its principles? Enrol in Simplilearn\u2019s Artificial Engineer Master\u2019s Program. FAQs 1. Why is it called Reinforcement Learning? Reinforcement Learning (RL) is named so because the learning process is driven by reinforcing agents' behaviors through rewards and penalties. Agents learn to optimize their actions based on feedback from the environment, continually improving their performance in achieving their goals. 2. Which is best for reinforcement learning? The best approach for reinforcement learning depends on the specific problem. Techniques like Q-learning, Deep Q-Networks (DQN), and Proximal Policy Optimization (PPO) are popular. Deep Q-Networks are especially effective for problems with high-dimensional input spaces, like video games. 3. Is reinforcement learning ML or DL? Reinforcement learning is a branch of Machine Learning (ML). When it incorporates deep learning models, such as neural networks, to process complex inputs and optimize decisions, it is called Deep Reinforcement Learning (DRL). 4. Who invented RL? Reinforcement learning as a formal framework was primarily developed by Richard S. Sutton and Andrew G. Barto, with their significant contributions in their book \"Reinforcement Learning: An Introduction\" published in 1998. 5. What is one advantage of using reinforcement learning? One major advantage of reinforcement learning is its ability to make decisions in complex, uncertain environments where explicit programming of all possible scenarios is impractical. It excels in adaptive problem-solving, continually learning to improve its strategies based on outcomes. Find our Post Graduate Program in AI and Machine Learning Online Bootcamp in top cities: About the Author Simplilearn is one of the world\u2019s leading providers of online training for Digital Marketing, Cloud Computing, Project Management, Data Science, IT, Software Development, and many other emerging technologies. Recommended Resources What Is Q-Learning: The Best Guide to Understand Q-Learning Machine Learning Career Guide: A Playbook to Becoming a Machine Learning Engineer 10 Essential Machine Learning Terms You Need to Know Discover the Differences Between AI vs. Machine Learning vs. Deep Learning What is Machine Learning and How Does It Work? Machine Learning Interview Guide \u00a9 2009 -2024- Simplilearn Solutions. Follow us! Company Work with us Discover For Businesses Learn On the Go! Trending Post Graduate Programs Trending Master Programs Trending Courses Trending Categories Trending Resources"}
{"url": "https://www.gov.uk/guidance/understanding-artificial-intelligence-ethics-and-safety", "title": "\n      Understanding artificial intelligence ethics and safety - GOV.UK\n  ", "content": "Cookies on GOV.UK We use some essential cookies to make this website work. We\u2019d like to set additional cookies to understand how you use GOV.UK, remember your settings and improve government services. We also use cookies set by other sites to help us deliver content from their services. You have accepted additional cookies. You can change your cookie settings at any time. You have rejected additional cookies. You can change your cookie settings at any time. \n        Navigation menu\n       \n                  Services and information\n                 \n                  Government activity\n                 Departments, agencies and public bodies News stories, speeches, letters and notices Detailed guidance, regulations and rules Reports, analysis and official statistics Consultations and strategy Data, Freedom of Information releases and corporate reports \n            Search\n           \n    Understanding artificial intelligence ethics and safety\n   \n    Understand how to use artificial intelligence ethically and safely\n \n      Contents\n This guidance is part of a wider collection about using artificial intelligence (AI) in the public sector. The Office for Artificial Intelligence (OAI) and the Government Digital Service (GDS) have produced the following chapter\u2019s guidance in partnership with The Alan Turing Institute\u2019s public policy programme. This chapter is a summary of The Alan Turing Institute\u2019s detailed guidance, and readers should refer to the full guidance when implementing these recommendations. AI has the potential to make a substantial impact for individuals, communities, and society. To make sure the impact of your AI project is positive and does not unintentionally harm those affected by it, you and your team should make considerations of AI ethics and safety a high priority. This section introduces AI ethics and provides a high-level overview of the ethical building blocks needed for the responsible delivery of an AI project. The following guidance is designed to complement and supplement the Data Ethics Framework. The Data Ethics Framework is a tool that should be used in any project. Who this guidance is for This guidance is for everyone involved in the design, production, and deployment of an AI project such as: Ethical considerations will arise at every stage of your AI project. You should use the expertise and active cooperation of all your team members to address them. Understanding what AI ethics is AI ethics is a set of values, principles, and techniques that employ widely accepted standards to guide moral conduct in the development and use of AI systems. The field of AI ethics emerged from the need to address the individual and societal harms AI systems might cause. These harms rarely arise as a result of a deliberate choice - most AI developers do not want to build biased or discriminatory applications or applications which invade users\u2019 privacy. The main ways AI systems can cause involuntary harm are: The field of AI ethics mitigates these harms by providing project teams with the values, principles, and techniques needed to produce ethical, fair, and safe AI applications. Varying your governance for projects using AI The guidance summarised in this chapter and presented at length in The Alan Turing Institute\u2019s further guidance on AI ethics and safety is as comprehensive as possible. However, not all issues discussed will apply equally to each project using AI. An AI model which filters out spam emails, for example, will present fewer ethical challenges than one which identifies vulnerable children. You and your team should formulate governance procedures and protocols for each project using AI, following a careful evaluation of social and ethical impacts. Establish ethical building blocks for your AI project You should establish ethical building blocks for the responsible delivery of your AI project. This involves building a culture of responsible innovation as well as a governance architecture to bring the values and principles of ethical, fair, and safe AI to life. Building a culture of responsible innovation To build and maintain a culture of responsibility you and your team should prioritise 4 goals as you design, develop, and deploy your AI project. In particular, you should make sure your AI project is: Prioritising these goals will help build a culture of responsible innovation. To make sure they are fully incorporated into your project you should establish a governance architecture consisting of a: Start with a framework of ethical values You should understand the framework of ethical values which support, underwrite, and motivate the responsible design and use of AI. The Alan Turing Institute calls these \u2018the SUM Values\u2019: These values: You can read further guidance on SUM Values in The Alan Turing Institute\u2019s comprehensive guidance on AI ethics and safety. Establish a set of actionable principles While the SUM Values can help you consider the ethical permissibility of your AI project, they are not specifically catered to the particularities of designing, developing, and implementing an AI system. AI systems increasingly perform tasks previously done by humans. For example, AI systems can screen CVs as part of a recruitment process. However, unlike human recruiters, you cannot hold an AI system directly responsible or accountable for denying applicants a job. This lack of accountability of the AI system itself creates a need for a set of actionable principles tailored to the design and use of AI systems. The Alan Turing Institute calls these the \u2018FAST Track Principles\u2019: Carefully reviewing the FAST Track Principles helps you: Fairness If your AI system processes social or demographic data, you should design it to meet a minimum level of discriminatory non-harm. To do this you should: Accountability You should design your AI system to be fully answerable and auditable. To do this you should: Sustainability The technical sustainability of these systems ultimately depends on their safety, including their accuracy, reliability, security, and robustness. You should make sure designers and users remain aware of: Transparency Designers and implementers of AI systems should be able to: To assess these criteria in depth, you should consult The Alan Turing Institute\u2019s guidance on AI ethics and safety. Build a process-based governance framework The final method to make sure you use AI ethically, fairly, and safely is building a process-based governance framework. The Alan Turing Institute calls it a \u2018PBG Framework\u2019. Its primary purpose is to integrate the SUM Values and the FAST Track Principles across the implementation of AI within a service. Building a good PBG Framework for your AI project will provide your team with an overview of: You may find it useful to consider further guidance on allocating responsibility and governance for AI projects. \u00a0Related guides Updates to this page \n      Sign up for emails or print this page\n   \n        Related content\n       Collection Is this page useful? Help us improve GOV.UK Don\u2019t include personal or financial information like your National Insurance number or credit card details. Help us improve GOV.UK \n        To help us improve GOV.UK, we\u2019d like to know more about your visit today.\n        Please fill in this survey (opens in a new tab).\n       Services and information Government activity Support links"}
{"url": "https://www.aisafetybook.com/", "title": "AI Safety, Ethics, and Society Textbook", "content": "Introduction to AI Safety, Ethics, and Society Rapid progress in AI raises questions about how the deployment of advanced AI systems will shape society, both for better and for worse. How do we ensure that AI is deployed in ways that are ethical and have positive societal impact? How can we understand and mitigate risks from advanced AI systems? \u00a0 This course, developed by Dan Hendrycks, director of the Center for AI Safety, aims to provide an accessible introduction to students, practitioners and others looking to better understand these issues. The course textbook is available to read online here and is forthcoming in print with Taylor & Francis. Print copies of the book can be pre-ordered here.  Ensuring that AI systems are safe is more than just a machine learning problem - it is a societal challenge that cuts across traditional disciplinary boundaries. This course takes a holistic approach drawing on insights from engineering, economics and other relevant fields. The course aims to foster a thoughtful and nuanced understanding of AI safety, equipping participants with the tools and insights needed to navigate this rapidly evolving field. Key topics covered include: We review a variety of AI risks that have the potential to lead to catastrophic societal outcomes. These risks are organised into four categories: malicious use, in which individuals or groups intentionally use AIs to cause harm; AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs, describing the inherent difficulty in controlling AI\u00a0systems that may outperform humans in many tasks. For each category of risk, we look at various specific hazards falling under this category and stories that illustrate how such risks might play out. To understand the risks that artificial intelligence (AI) poses and to learn what measures we can take to mitigate them, it is essential to understand the technology itself: how it works, how it is used, and where its strengths and weaknesses lie. We describe key concepts in machine learning (ML), the approach that powers most modern AI systems, with a particular focus on the technologies that power Large Language Models such as OpenAI's GPT-4 or Google's Gemini. We introduce the phenomenon of scaling laws, where AI systems' performance improves predictably as the amounts of data and computation used during their training are increased, and consider what this implies for future AI progress. ML systems have grown more competent and general as the field of deep learning has matured. Reasoning about the behavior and internal structure of such systems can be challenging, especially since some failure modes arise only once an AI system is sufficiently sophisticated. We discuss some of the fundamental technical challenges around monitoring, robustness and control of AI systems. Current AI systems lack transparency and can exhibit surprising emergent capabilities. They are vulnerable to adversarial examples, Trojans and other attacks. These challenges in turn may make it hard to control AI\u00a0systems and prevent unintended behaviour such as deception. When conducting research to advance AI safety, it is important to consider the risk of inadvertently accelerating AI\u00a0capabilities in a way that undermines the overall goal of better understanding and controlling AI\u00a0systems. It is useful to consider AI safety within the broader context of safety engineering. This provides fundamental principles for identifying and managing risks, drawing general lessons from the management of other systems such as airplanes or nuclear power plants. Principles of safe design, crucial for improving a system's safety and controllability, are discussed, particularly in relation to AI systems. Any competent form of risk management needs to consider tail events with low probability but high impact. We explore the concepts of tail events and black swans\u2014essentially unpredictable unknowns. We consider how these concepts can be applied as part of strategies to mitigate unforeseeable risks from AI. AI systems and the societies they operate within belong to the class of complex systems. This has important implications for ensuring AI safety. Complex systems exhibit surprising behaviors and defy conventional analysis methods that examine individual components in isolation. We explore how unintended side effects often result from interventions in complex systems. To develop effective strategies for AI safety, it is crucial to adopt approaches that account for the unique properties of complex systems and enable us to anticipate and address AI risks.  Defining the principles that advanced AI systems should follow is no trivial task. There are many plausible candidates, such as complying with users' preferences, following the law, or doing what is ethical. While each of these proposals has its attractions, they all face limitations as well, and may come into conflict with each other. We explore these questions and consider what it would mean to design an AI\u00a0system that promotes the wellbeing of users and of society as a whole. We also discuss how AI systems should deal with situations where there is uncertainty about the right course of action. Addressing risks posed by individual AI systems alone is insufficient, as many challenges in AI safety come from the interaction of multiple AI developers, nations or other actors pursuing their self-interest. The principles of Game Theory can help us to understand how rational self-interest can lead to collectively undesirable results. Theories of conflict in international relations help us to understand under what circumstances the developers of advanced AI systems or the systems themselves might come into conflict with each other, which could lead to violent consequences. Lastly, the concept of Generalized Darwinism, which expands evolution by natural selection beyond biology, may also provide a useful lens to understand how the diffusion of AI systems in society could lead to an erosion of human control over AI. To design effective policies to capture AI's benefits and manage its risks, we need to consider some fundamental variables such as the speed of progress in AI capabilities, and the breadth of access to highly powerful AI systems. We outline several possible scenarios regarding the speed of AI development and review arguments for and against a dramatic acceleration of economic growth due to AI progress. We also explore how varying concentrations of power, both in the number of AIs and the access to these AIs, can alter the risks and benefits we face. We then discuss potential governance approaches at different level to ensure AI development is well-regulated and beneficial. We examine how corporations can be held to safety standards and how national and international bodies can implement effective legal oversight of AI activities. Some forms of international cooperation may be needed to mitigate competitive pressures.  Citation:Dan Hendrycks. Introduction to AI Safety, Ethics and Society. Taylor & Francis, (forthcoming). ISBN: 9781032798028. URL: www.aisafetybook.com Cookies Notice: This website uses cookies to identify pages that are being used most frequently. This helps us analyze data about web page traffic and improve our website. We only use this information for the purpose of statistical analysis and then the data is removed from the system. We do not and will never sell user data. Read more about our cookie policy on our privacy policy. Please contact us if you have any questions."}
{"url": "https://www.simplilearn.com/10-algorithms-machine-learning-engineers-need-to-know-article", "title": "10 Types of Machine Learning Algorithms and Models", "content": "Table of Contents Top \u200c10\u00a0 \u200cMachine\u200c \u200cLearning Algorithms? Types of Machine Learning Algorithms List of Popular Machine Learning Algorithms Supervised vs. Unsupervised vs. Reinforcement Learning Algorithms When to Use Supervised, Unsupervised, or Reinforcement Learning Factors to Consider When Choosing a Machine Learning Algorithm Conclusion FAQs 10 Types of Machine Learning Algorithms and Models Table of Contents Top \u200c10\u00a0 \u200cMachine\u200c \u200cLearning Algorithms? Types of Machine Learning Algorithms List of Popular Machine Learning Algorithms Supervised vs. Unsupervised vs. Reinforcement Learning Algorithms When to Use Supervised, Unsupervised, or Reinforcement Learning Factors to Consider When Choosing a Machine Learning Algorithm Conclusion FAQs In a world where nearly all manual tasks are being automated, the definition of manual is changing. There are now many different types of Machine Learning algorithms, some of which can help computers play chess, perform surgeries, and get smarter and more personal. We are living in an era of constant technological progress, and looking at how computing has advanced over the years, we can predict what\u2019s to come in the days ahead.\u00a0 In the fast-evolving field of machine learning, understanding the right algorithms is crucial for any aspiring engineer or data scientist. This article highlights the top 10 machine learning algorithms that every machine learning engineer should be familiar with to build effective models and derive meaningful insights from data. Become a AI & Machine Learning Professional Post Graduate Program in AI and Machine Learning Artificial Intelligence Engineer Here's what learners are saying regarding our programs: Akili Yang Personal Financial Consultant, OCBC Bank The live sessions were quite good; you could ask questions and clear doubts. Also, the self-paced videos can be played conveniently, and any course part can be revisited. The hands-on projects were also perfect for practice; we could use the knowledge we acquired while doing the projects and apply it in real life.  Indrakala Nigam Beniwal Technical Consultant, Land Transport Authority (LTA) Singapore I completed a Master's Program in Artificial Intelligence Engineer with flying colors from Simplilearn. Thanks to the course teachers and others associated with designing such a wonderful learning experience.  Top \u200c10\u00a0 \u200cMachine\u200c \u200cLearning Algorithms? Below is the list of the top 10 commonly used Machine Learning Algorithms: Types of Machine Learning Algorithms 1. Supervised Learning Supervised learning algorithms are trained using labeled data, which means the input data is tagged with the correct output. The goal of these algorithms is to learn a mapping from inputs to outputs, making it possible to predict the output for new data. Common supervised learning algorithms include: Linear Regression: Used for predicting continuous outcomes. It models the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data. Logistic Regression: Used for binary classification tasks (e.g., predicting yes/no outcomes). It estimates probabilities using a logistic function. Decision Trees: These models predict the value of a target variable by learning simple decision rules inferred from the data features. Random Forests: An ensemble of decision trees, typically used for classification and regression, improving model accuracy and overfitting control. Support Vector Machines (SVM): Effective in high-dimensional spaces, SVM is primarily used for classification but can also be used for regression. Neural Networks: These are powerful models that can capture complex non-linear relationships. They are widely used in deep learning applications. Become a AI & Machine Learning Professional Post Graduate Program in AI and Machine Learning Artificial Intelligence Engineer Here's what learners are saying regarding our programs: Akili Yang Personal Financial Consultant, OCBC Bank The live sessions were quite good; you could ask questions and clear doubts. Also, the self-paced videos can be played conveniently, and any course part can be revisited. The hands-on projects were also perfect for practice; we could use the knowledge we acquired while doing the projects and apply it in real life.  Indrakala Nigam Beniwal Technical Consultant, Land Transport Authority (LTA) Singapore I completed a Master's Program in Artificial Intelligence Engineer with flying colors from Simplilearn. Thanks to the course teachers and others associated with designing such a wonderful learning experience.  2. Unsupervised Learning Unsupervised learning algorithms are used with data sets without labeled responses. The goal here is to infer the natural structure present within a set of data points. Common unsupervised learning techniques include: Clustering: Algorithms like K-means, hierarchical clustering, and DBSCAN group a set of objects in such a way that objects in the same group are more similar to each other than to those in other groups. Association: These algorithms find rules that describe large portions of your data, such as market basket analysis. Principal Component Analysis (PCA): A statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables. Autoencoders: Special type of neural network used to learn efficient codings of unlabeled data. 3. Reinforcement Learning Reinforcement learning algorithms learn to make a sequence of decisions. The algorithm learns to achieve a goal in an uncertain, potentially complex environment. In reinforcement learning, an agent makes decisions by following a policy based on which actions to take, and it learns from the consequences of these actions through rewards or penalties. Q-learning: This is a model-free reinforcement learning algorithm that learns the value of an action in a particular state. Deep Q-Networks (DQN): It combines Q-learning with deep neural networks, allowing the approach to learn successful policies directly from high-dimensional sensory inputs. Policy Gradient Methods: These methods optimize the parameters of a policy directly as opposed to estimating the value of actions. Monte Carlo Tree Search (MCTS): Used in decision processes for finding optimal decisions by playing out scenarios, notably used in games like Go. These categories provide a broad overview of the most common types of machine learning algorithms. Each has its strengths and ideal use cases, making them better suited for certain types of tasks over others. Become a AI & Machine Learning Professional Post Graduate Program in AI and Machine Learning Artificial Intelligence Engineer Here's what learners are saying regarding our programs: Akili Yang Personal Financial Consultant, OCBC Bank The live sessions were quite good; you could ask questions and clear doubts. Also, the self-paced videos can be played conveniently, and any course part can be revisited. The hands-on projects were also perfect for practice; we could use the knowledge we acquired while doing the projects and apply it in real life.  Indrakala Nigam Beniwal Technical Consultant, Land Transport Authority (LTA) Singapore I completed a Master's Program in Artificial Intelligence Engineer with flying colors from Simplilearn. Thanks to the course teachers and others associated with designing such a wonderful learning experience.  List of Popular Machine Learning Algorithms 1. Linear Regression To understand the working functionality of Linear Regression, imagine how you would arrange random logs of wood in increasing order of their weight. There is a catch; however \u2013 you cannot weigh each log. You have to guess its weight just by looking at the height and girth of the log (visual analysis) and arranging them using a combination of these visible parameters. This is what linear regression in machine learning is like. In this process, a relationship is established between independent and dependent variables by fitting them to a line. This line is known as the regression line and is represented by a linear equation Y= a *X + b. In this equation: The coefficients a & b are derived by minimizing the sum of the squared difference of distance between data points and the regression line. 2. Logistic Regression Logistic Regression is used to estimate discrete values (usually binary values like 0/1) from a set of independent variables. It helps predict the probability of an event by fitting data to a logit function. It is also called logit regression. These methods listed below are often used to help improve logistic regression models: 3. Decision Tree Decision Tree algorithm in machine learning is one of the most popular algorithm in use today; this is a supervised learning algorithm that is used for classifying problems. It works well in classifying both categorical and continuous dependent variables. This algorithm divides the population into two or more homogeneous sets based on the most significant attributes/ independent variables. 4. SVM (Support Vector Machine) Algorithm SVM algorithm is a method of a classification algorithm in which you plot raw data as points in an n-dimensional space (where n is the number of features you have). The value of each feature is then tied to a particular coordinate, making it easy to classify the data. Lines called classifiers can be used to split the data and plot them on a graph. 5. Naive Bayes Algorithm A Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. Even if these features are related to each other, a Naive Bayes classifier would consider all of these properties independently when calculating the probability of a particular outcome. A Naive Bayesian model is easy to build and useful for massive datasets. It's simple and is known to outperform even highly sophisticated classification methods. 6. KNN (K- Nearest Neighbors) Algorithm This algorithm can be applied to both classification and regression problems. Apparently, within the Data Science industry, it's more widely used to solve classification problems. It\u2019s a simple algorithm that stores all available cases and classifies any new cases by taking a majority vote of its k neighbors. The case is then assigned to the class with which it has the most in common. A distance function performs this measurement. KNN can be easily understood by comparing it to real life. For example, if you want information about a person, it makes sense to talk to his or her friends and colleagues! Things to consider before selecting K Nearest Neighbours Algorithm:\u00a0 7. K-Means It is an unsupervised learning algorithm that solves clustering problems. Data sets are classified into a particular number of clusters (let's call that number K) in such a way that all the data points within a cluster are homogenous and heterogeneous from the data in other clusters. How K-means forms clusters: Become a AI & Machine Learning Professional Post Graduate Program in AI and Machine Learning Artificial Intelligence Engineer Here's what learners are saying regarding our programs: Akili Yang Personal Financial Consultant, OCBC Bank The live sessions were quite good; you could ask questions and clear doubts. Also, the self-paced videos can be played conveniently, and any course part can be revisited. The hands-on projects were also perfect for practice; we could use the knowledge we acquired while doing the projects and apply it in real life.  Indrakala Nigam Beniwal Technical Consultant, Land Transport Authority (LTA) Singapore I completed a Master's Program in Artificial Intelligence Engineer with flying colors from Simplilearn. Thanks to the course teachers and others associated with designing such a wonderful learning experience.  8. Random Forest Algorithm A collective of decision trees is called a Random Forest. To classify a new object based on its attributes, each tree is classified, and the tree \u201cvotes\u201d for that class. The forest chooses the classification having the most votes (over all the trees in the forest). Each tree is planted & grown as follows: 9. Dimensionality Reduction Algorithms In today's world, vast amounts of data are being stored and analyzed by corporates, government agencies, and research organizations. As a data scientist, you know that this raw data contains a lot of information - the challenge is to identify significant patterns and variables. Dimensionality reduction algorithms like Decision Tree, Factor Analysis, Missing Value Ratio, and Random Forest can help you find relevant details. 10. Gradient Boosting Algorithm and AdaBoosting Algorithm Gradient Boosting Algorithm and AdaBoosting Algorithm are boosting algorithms used when massive loads of data have to be handled to make predictions with high accuracy. Boosting is an ensemble learning algorithm that combines the predictive power of several base estimators to improve robustness. In short, it combines multiple weak or average predictors to build a strong predictor. These boosting algorithms always work well in data science competitions like Kaggle, AV Hackathon, CrowdAnalytix. These are the most preferred machine learning algorithms today. Use them, along with Python and R Codes, to achieve accurate outcomes. You can also watch our in-demand video on top Machine Learning Algorithms.  Supervised vs. Unsupervised vs. Reinforcement Learning Algorithms Let\u2019s look at how supervised, unsupervised, and reinforcement learning really stack up across a few key areas. Data Labeling In supervised learning, you have labeled data at your disposal, meaning the answers are already known for each example, making it easier to train the model. Unsupervised learning, on the other hand, doesn\u2019t come with labels, so the algorithm has to figure out patterns on its own. Reinforcement learning also skips labeled data; instead, it learns by taking actions, getting feedback through rewards or penalties, and using that feedback to keep improving. Goal Orientation Supervised learning has a clear goal in mind; you\u2019re trying to predict specific outcomes using labeled data. Unsupervised learning isn\u2019t as structured; it\u2019s more about exploring the data to uncover hidden patterns or clusters. With reinforcement learning, the goal is all about maximizing rewards over time, adjusting actions based on past mistakes and successes to do better as it goes along. Learning Approach In supervised learning, it involves giving the model numerous examples with a known result and the model is trained to achieve the results through such examples. Unsupervised learning takes the algorithm in a different role: discovering structure within the data. For example, finding clusters or associations. Reinforcement learning in its approach is relatively different, it is more fluid in that it evolves through interacting with the environment and learning as it progresses through its strategy. Application Scenarios Supervised learning is best suited for tasks such as outcome forecasting and pattern recognition. It involves classification and prediction. On the other hand, unsupervised learning is most useful in identifying groups within the data, detecting outliers, or reducing the dimensionality of the data. Reinforcement learning is particularly useful in areas where real-time decisions are required such as robotics, games, etc. in which performance can be enhanced through experience. Become a AI & Machine Learning Professional Post Graduate Program in AI and Machine Learning Artificial Intelligence Engineer Here's what learners are saying regarding our programs: Akili Yang Personal Financial Consultant, OCBC Bank The live sessions were quite good; you could ask questions and clear doubts. Also, the self-paced videos can be played conveniently, and any course part can be revisited. The hands-on projects were also perfect for practice; we could use the knowledge we acquired while doing the projects and apply it in real life.  Indrakala Nigam Beniwal Technical Consultant, Land Transport Authority (LTA) Singapore I completed a Master's Program in Artificial Intelligence Engineer with flying colors from Simplilearn. Thanks to the course teachers and others associated with designing such a wonderful learning experience.  When to Use Supervised, Unsupervised, or Reinforcement Learning Supervised learning works best when labeled data is readily available, and you need precise predictions. It\u2019s often used in spam detection, stock price prediction, and medical diagnosis.\u00a0 Unsupervised learning is great when exploring new data to find patterns or clusters, such as customer segmentation or anomaly detection.\u00a0 Reinforcement learning is suitable for scenarios involving continuous learning, like training a robot to navigate or optimizing game strategies, where feedback is given over time. Factors to Consider When Choosing a Machine Learning Algorithm \u00a0Let\u2019s explore what to consider when making choosing a machine learning algorithm:\u00a0 Type of Data The first thing to look at is determining the type of data that you have. For instance, labeled datasets or those with defined outputs can be entrusted in the hands of supervised methods. On the other hand, in the case of unlabeled data, unsupervised approaches are required to locate hidden structures. In scenarios where learning is carried out through interactions, reinforcement learning seems to be a useful candidate. Complexity of the Problem After that, evaluate the complexity of the problem you are trying to solve. In tasks that are less complex, simpler algorithms can do the job. However, if you\u2019re tackling a more complex issue with intricate relationships, you might want to use more advanced methods, like neural networks or ensemble techniques. Just be prepared for a bit more effort and tuning. Computational Resources Another important factor is the computational power at your disposal. Some algorithms, like deep learning models, can be resource-intensive and require powerful hardware. If you're working with limited resources, simpler algorithms like logistic regression or k-nearest neighbors can still deliver solid results without putting too much strain on your system. Interpretability vs. Accuracy Finally, think about whether you need an algorithm that\u2019s easy to understand or one that prioritizes accuracy, even if it\u2019s a bit of a black box. Decision trees and linear regression are generally easier to interpret, making them great for explaining to stakeholders. In contrast, more complex models like neural networks might give you better accuracy but can be harder to explain. Become a AI & Machine Learning Professional Post Graduate Program in AI and Machine Learning Artificial Intelligence Engineer Here's what learners are saying regarding our programs: Akili Yang Personal Financial Consultant, OCBC Bank The live sessions were quite good; you could ask questions and clear doubts. Also, the self-paced videos can be played conveniently, and any course part can be revisited. The hands-on projects were also perfect for practice; we could use the knowledge we acquired while doing the projects and apply it in real life.  Indrakala Nigam Beniwal Technical Consultant, Land Transport Authority (LTA) Singapore I completed a Master's Program in Artificial Intelligence Engineer with flying colors from Simplilearn. Thanks to the course teachers and others associated with designing such a wonderful learning experience.  Conclusion IMastering these Machine Learning Algorithms are a great way to build a career in machine learning. The field is proliferating, and the sooner you understand the scope of machine learning tools, the sooner you'll be able to provide solutions to complex work problems. However, if you are experienced in the field and want to boost your career, you can take-up the Post Graduate Program in AI and Machine Learning in partnership with Purdue University collaborated with IBM. This program gives you an in-depth knowledge of Python, Deep Learning algorithm with the Tensor flow, Natural Language Processing, Speech Recognition, Computer Vision, and Reinforcement Learning. Explore and enroll today!  FAQs 1. What is an algorithm in machine learning? Algorithms in machine learning are mathematical procedures and techniques that allow computers to learn from data, identify patterns, make predictions, or perform tasks without explicit programming. These algorithms can be categorized into various types, such as supervised learning, unsupervised learning, reinforcement learning, and more. 2. What are the three types of machine learning algorithms? The three basic machine learning algorithms are: 3. What are the 4 machine learning algorithm? The 4 machine learning algorithms are: 4. Which ML algorithm is best for prediction? The best ML algorithm for prediction depends on variety of factors such as the nature of the problem, the type of data, and the specific requirements. Popular algorithms for prediction tasks include Support Vector Machines, Random Forests, and Gradient Boosting methods. However, the choice of an algorithm should be based on experimentation and evaluation of the specific problem and dataset at hand. 5. What is the difference between supervised and unsupervised learning algorithms? The primary difference between supervised and unsupervised learning lies in the type of data used for training. Supervised learning algorithms use labeled data, where the target output is known, to learn patterns and make predictions. Unsupervised learning algorithms work with unlabeled data, relying on intrinsic patterns and relationships to group data points or discover hidden structures. 6. Is CNN a machine learning algorithm? A convolutional neural network (CNN or convnet) is a type of artificial neural network used for various tasks, especially with images and videos. It's a part of machine learning and works with different kinds of data. Our AI & ML Courses Duration And Fees AI & Machine Learning Courses typically range from a few weeks to several months, with fees varying based on program and institution. Cohort Starts: 2 Dec, 2024 Cohort Starts: 10 Dec, 2024 Cohort Starts: 10 Dec, 2024 Cohort Starts: 12 Dec, 2024 Cohort Starts: 17 Dec, 2024 Cohort Starts: 17 Dec, 2024 Recommended Reads Machine Learning Interview Guide What is Epoch in Machine Learning? Different Types of Machine Learning: Exploring AI's Core Machine Learning Career Guide: A Playbook to Becoming a Machine Learning Engineer What is Machine Learning and How Does It Work An Introduction To Machine Learning Get Affiliated Certifications with Live Class programs Post Graduate Program in AI and Machine Learning Artificial Intelligence Engineer Caltech Post Graduate Program in AI and Machine Learning \u00a9 2009 -2024- Simplilearn Solutions. Follow us! Company Work with us Discover For Businesses Learn On the Go! Trending Post Graduate Programs Trending Master Programs Trending Courses Trending Categories Trending Resources"}
{"url": "https://www.ibm.com/topics/machine-learning-algorithms", "title": "What Is a Machine Learning Algorithm? | IBM", "content": "A machine learning algorithm\u00a0is a set of rules or processes used by an AI system to conduct tasks\u2014most often to discover new data insights and patterns, or to predict output values from a given set of input variables. Algorithms enable machine learning (ML) to learn. Industry analysts agree on the importance of machine learning and its underlying algorithms. From Forrester, \u201cAdvancements in machine-learning algorithms bring precision and depth\u00a0to marketing data analysis that helps marketers understand how marketing details\u2014such as platform, creative, call to action, or messaging\u2014impact marketing performance.1\u201d While Gartner states that, \u201cMachine learning is at the core of many successful AI\u00a0applications, fueling its enormous traction in the market.2\u201d Most often, training ML algorithms on more data will provide more accurate answers than training on less data. Using statistical methods, algorithms are trained to determine classifications or make predictions, and to uncover key insights in data mining projects. These insights can subsequently improve your decision-making to boost key growth metrics.\n Use cases\u00a0for machine learning algorithms include the ability to analyze data to identify trends and predict issues before they occur.3 More advanced AI can enable more personalized support, reduce response times, provide speech recognition\u00a0and improve customer satisfaction. The industries that particularly benefit from machine learning algorithms to create new\u00a0content\u00a0from vast amounts of data include supply chain management, transportation and logistics, retail and manufacturing4\u2014all embracing generative AI, with its ability to automate tasks, enhance efficiency and provide valuable insights, even to beginners.\n Learn key benefits of generative AI and how organizations can incorporate generative AI and machine learning into their business.    Register for the guide on foundation models Deep learning\u00a0is a specific application of the advanced functions provided by machine learning algorithms. The distinction is in how each algorithm learns. \"Deep\" machine learning\u00a0 models can use your labeled datasets, also known as supervised learning, to inform its algorithm, but it doesn\u2019t necessarily require labeled data. Deep learning can ingest unstructured data in its raw form (such as text or images), and it can automatically determine the set of features which distinguish different categories of data from one another. This eliminates some of the human intervention required and enables the use of larger data sets. The easiest way to think about artificial intelligence, machine learning, deep learning and neural networks is to think of them as a series of AI systems\u00a0from largest to smallest, each encompassing the next. Artificial intelligence (AI) is the overarching system. Machine learning is a subset of AI. Deep learning is a subfield of machine learning, and neural networks make up the backbone of deep learning algorithms. It\u2019s the number of node layers, or depth, of neural networks that distinguishes a single neural network from a deep learning algorithm, which must have more than three. A paper from UC\u00a0Berkeley breaks out the learning system\u00a0of a machine learning algorithm into three main parts.5 3.\u00a0\u00a0\u00a0 A model optimization process: If the model can fit better to the data points in the training set, then weights are adjusted to reduce the discrepancy between the known example and the model estimate. The algorithm will repeat this \u201cevaluate and optimize\u201d process, updating weights autonomously until a threshold of accuracy has been met.\u00a0\u00a0 Supervised learning in particular uses a training set to teach models to yield the desired output. This training dataset includes inputs and correct outputs, which enables the model to learn over time. The algorithm measures its accuracy through the loss function, adjusting until the error has been sufficiently minimized. There are four types of machine learning algorithms: supervised, unsupervised, semi-supervised, and reinforcement.\u00a0 Depending on your budget, need for speed and precision required, each type and variant has its own advantages.\u00a0Advanced\u00a0 machine learning algorithms require multiple technologies\u2014including deep learning,\u00a0neural networks\u00a0and\u00a0natural language processing\u2014and are able to use both unsupervised and supervised learning.6\u00a0The following are the most popular and commonly used algorithms. Supervised learning can be separated into two types of problems when data mining:\u00a0classification and regression. Various algorithms and computations techniques are used in supervised machine learning processes, often calculated through use of programs such as Python. Supervised learning algorithms include: Unlike supervised learning, unsupervised learning\u00a0uses unlabeled data. From that data, the algorithm\u00a0discovers patterns\u00a0that help solve clustering or association problems. This is particularly useful when subject matter experts are unsure of common properties within a data set. Common clustering algorithms are hierarchical, K-means, Gaussian mixture models and Dimensionality Reduction Methods such as PCA\u00a0and t-SNE. Semi-supervised\u00a0learning algorithms In this case, learning occurs when only part of the given input data has been labeled\u2014giving the algorithm a bit of a \u201chead start.\u201d This approach can combine the best of both\u00a0worlds10\u2014improved accuracy associated with supervised machine learning and the ability to make use of cost-effective unlabeled data, as in the case of unsupervised machine learning. Reinforcement algorithms\n In this case, the algorithms are trained just as humans learn\u2014through rewards and penalties\u2014which are measured and tracked by a reinforcement learning agent11\u00a0which has a general understanding of the probability of successfully moving the score up vs. moving\u00a0it down. Through trial and error, the agent learns to take actions that lead to the most favorable outcomes over time. Reinforcement learning is often used12\u00a0 in resource management, robotics and video games. Design complex neural networks. Experiment at scale to deploy optimized learning models within IBM Watson Studio. Analyze data and build analytics and predictive models of future outcomes. Uncover risks and opportunities for your business. NLP is AI that speaks the language of your business. Build solutions that drive 383 percent ROI over three years with IBM Watson Discovery. Learn the fundamental concepts for AI and generative AI, including prompt engineering, large language models and the best open-source projects. IBM again recognized as a Leader in the 2023 Gartner\u00ae Magic Quadrant\u2122 for Enterprise Conversational AI. Learn the tools businesses use to efficiently run and manage AI models and empower their data scientist with technology that can help optimize their data-driven decision making. Explore how machine learning lets you continually learn from data and predict the future. Four strategies to scale AI with a strong data foundation. AI technology has been rapidly evolving over the last couple of decades. Learn how businesses are implementing AI today. Build an AI strategy for your business on one collaborative AI and data platform\u2014IBM watsonx. Train, validate, tune and deploy AI models to help you scale and accelerate the impact of AI with trusted data across your business. \n                            \n\n\n\n  \n    Footnotes\n\n\n\n\n\n\n    \n\n\n                         All footnote links below reside outside of IBM. 1 Forrester: Use Marketing Analytics To Support Your 2023 Marketing Strategy\u00a0 2 Gartner: What Is Artificial Intelligence? 3 Gartner Peer Community: How will AI help facilitate desk and IT support teams?\u00a0 4 IDC: Generative AI: Exploring Trends and Use Cases Across Asia/Pacific Supply Chains\u00a0 5 Berkeley School of information: \u00a0What Is Machine Learning (ML)? 6 Gartner Glossary: Machine Learning\n\r\n7 TechTarget: What are machine learning algorithms?\n 8 GeeksforGeeks: Hierarchical Clustering in Data Mining\n 9 Stanford University: K Means\n 10 Booz Allen: How do machines learn? 11 G2: Reinforcement Learning: How Machines Learn From Their Mistakes\u00a0 12 TechTarget: What is machine learning and how does it work?\u00a0"}
{"url": "https://www.geeksforgeeks.org/machine-learning-algorithms/", "title": "Machine Learning Algorithms", "content": "Machine Learning Algorithms Machine learning algorithms are computational models that allow computers to understand patterns and forecast or make judgments based on data without explicit programming. These algorithms form the foundation of modern artificial intelligence and are used in various applications, including image and speech recognition, natural language processing, recommendation systems, fraud detection, autonomous cars, etc.  Table of Content This Machine learning Algorithms article will cover all the essential algorithms of machine learning like Support vector machine, decision-making, logistics regression, naive bayees classifier, random forest, k-mean clustering, reinforcement learning, vector, hierarchical clustering, xgboost, adaboost, logistics, etc. Types of Machine Learning Algorithms There are four types of machine learning algorithms 1. Supervised Learning Supervised learning involves training a model on labeled data, where the desired output is known. The model learns to map inputs to outputs based on the provided examples. A. Classification 1. Logistic Regression 2. Support Vector Machines (SVM) 3. k-Nearest Neighbors (k-NN) 4. Naive Bayes 5. Decision Trees 6. Random Forest 7. Gradient Boosting (e.g., XGBoost, LightGBM, CatBoost) 8. Neural Networks (e.g., Multilayer Perceptron) B. Regression 1. Linear Regression 2. Ridge Regression 3. Lasso Regression 4. Support Vector Regression (SVR) 5. Decision Trees Regression 6. Random Forest Regression 7. Gradient Boosting Regression 8. Neural Networks Regression 2. Unsupervised Learning Unsupervised learning works with unlabeled data and aims to find hidden patterns or intrinsic structures in the input data. A. Clustering 1. k-Means 2. Hierarchical Clustering 3. DBSCAN (Density-Based Spatial Clustering of Applications with Noise) 4. Gaussian Mixture Models (GMM) B. Dimensionality Reduction 1. Principal Component Analysis (PCA) 2. t-Distributed Stochastic Neighbor Embedding (t-SNE) 3. Linear Discriminant Analysis (LDA) 4. Independent Component Analysis (ICA) 5. UMAP (Uniform Manifold Approximation and Projection) C. Association 1. Apriori Algorithm 2. Eclat Algorithm 3. Reinforcement Learning Reinforcement learning involves training agents to make a sequence of decisions by rewarding them for good actions and penalizing them for bad ones. A. Model-Free Methods 1. Q-Learning 2. Deep Q-Network (DQN) 3. SARSA (State-Action-Reward-State-Action) 4. Policy Gradient Methods (e.g., REINFORCE) B. Model-Based Methods 1. Deep Deterministic Policy Gradient (DDPG) 2. Proximal Policy Optimization (PPO) 3. Trust Region Policy Optimization (TRPO) C. Value-Based Methods 1. Monte Carlo Methods 2. Temporal Difference (TD) Learning 4. Ensemble Learning Ensemble learning combines multiple models to improve performance by leveraging the strengths of each model. 1. Bagging (e.g., Random Forest) 2. Boosting (e.g., AdaBoost, Gradient Boosting) 3. Stacking  \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014- 1. Supervised Learning Algorithm Supervised learning is a type of machine learning algorithms where we used labeled dataset to train the model or algorithms. The goal of the algorithm is to learn a mapping from the input data to the output labels, allowing it to make predictions or classifications on new, unseen data. Supervised Machine Learning Algorithms Some of the Supervised Machine Learning Algorithms can be used used for both Classification & Regression with a little bit of modification Metrics for Classification & Regression Algorithms: Cross Validation Technique: Optimization Technique: 2. Unsupervised Learning Algorithm Unsupervised Learning is a type of machine learning algorithms where the algorithms are used to find the patterns, structure or relationship within a dataset using unlabled dataset. It explores the data\u2019s inherent structure without predefined categories or labels. Unsupervised Machine Learning Algorithms 3. Reinforcement Learning Reinforcement Learning is a type of machine learning algorithms where an agent learns to make successive decisions by interacting with its surroundings. The agent receives the feedback in the form of incentives or punishments based on its actions. The agent\u2019s purpose is to discover optimal tactics that maximize cumulative rewards over time through trial and error. Reinforcement learning is frequently employed in scenarios in which the agent must learn how to navigate an environment, play games, manage robots, or make judgments in uncertain situations. Reinforcement Learning Discover the fundamental concepts driving machine learning by learning the top 10 algorithms, such as linear regression, decision trees, and neural networks. Machine Learning Algorithm \u2013 FAQs 1. What is an algorithm in Machine Learning? Machine learning algorithms are techniques based on statistical concepts that enable computers to learn from data, discover patterns, make predictions, or complete tasks without the need for explicit programming. These algorithms are broadly classified into the three types, i.e supervised learning, unsupervised learning, and reinforcement learning. 2. What are types of Machine Learning? There are mainly three types of machine learning: 3. Which ML algorithm is best for prediction? The ideal machine learning method for prediction is determined by a number of criteria, including the nature of the problem, the type of data, and the unique requirements. Support Vector Machines, Random Forests, and Gradient Boosting approaches are popular for prediction workloads. The selection of an algorithm, on the other hand, should be based on testing and evaluation of the specific problem and dataset at hand. 4. What\u200c \u200care\u200c \u200cthe\u200c \u200c10 \u200cPopular\u200c \u200cMachine\u200c \u200cLearning Algorithms?\u200c Below is the list of Top 10 commonly used Machine Learning (ML) Algorithms: Similar Reads  What kind of Experience do you want to share?"}
{"url": "https://www.simplilearn.com/tutorials/deep-learning-tutorial/deep-learning-frameworks", "title": "Top 8 Deep Learning Frameworks You Should Know in 2024", "content": "Tutorial Playlist  Deep Learning Tutorial for Beginners  Discover Deep Learning: AI's Game-Changing Technology!  The Best Introduction to Deep Learning - A Step by Step Guide  Top Deep Learning Applications Used Across Industries  What is Neural Network: Overview, Applications, and Advantages  Neural Networks Tutorial  Top 8 Deep Learning Frameworks You Should Know in 2024  Top 10 Deep Learning Algorithms You Should Know in 2025  An Introduction To Deep Learning With Python  What is Tensorflow: Deep Learning Libraries and Program Elements Explained  How To Install TensorFlow on Ubuntu  What Is TensorFlow 2.0? The Best Guide to Understand TensorFlow  TensorFlow Tutorial for Beginners: Your Gateway to Building Machine Learning Models  Convolutional Neural Network Tutorial  Recurrent Neural Network (RNN) Tutorial for Beginners  The Best Introduction to What GANs Are  What Is Keras? The Best Introductory Guide to Keras  Frequently asked Deep Learning Interview Questions and Answers  The Ultimate Guide to Building Powerful Keras Image Classification Models  What Is Ethernet? A Look Into the Basics of Network Communication  The Best Guide to Understand Everything About the Google Summer of Code  The Best Guide to Understand GraphQL  SSL Handshake: From Zero to Hero  How to Download and Install Junit  KNN in Python: Learn How to Leverage KNN Algorithms  How to Become a Machine Learning Engineer  Intro to Deep Belief Network (DBN) in Deep Learning Top 8 Deep Learning Frameworks You Should Know in 2024 Lesson 6 of 26By Simplilearn  Deep Learning Tutorial for Beginners  Discover Deep Learning: AI's Game-Changing Technology!  The Best Introduction to Deep Learning - A Step by Step Guide  Top Deep Learning Applications Used Across Industries  What is Neural Network: Overview, Applications, and Advantages  Neural Networks Tutorial  Top 8 Deep Learning Frameworks You Should Know in 2024  Top 10 Deep Learning Algorithms You Should Know in 2025  An Introduction To Deep Learning With Python  What is Tensorflow: Deep Learning Libraries and Program Elements Explained  How To Install TensorFlow on Ubuntu  What Is TensorFlow 2.0? The Best Guide to Understand TensorFlow  TensorFlow Tutorial for Beginners: Your Gateway to Building Machine Learning Models  Convolutional Neural Network Tutorial  Recurrent Neural Network (RNN) Tutorial for Beginners  The Best Introduction to What GANs Are  What Is Keras? The Best Introductory Guide to Keras  Frequently asked Deep Learning Interview Questions and Answers  The Ultimate Guide to Building Powerful Keras Image Classification Models  What Is Ethernet? A Look Into the Basics of Network Communication  The Best Guide to Understand Everything About the Google Summer of Code  The Best Guide to Understand GraphQL  SSL Handshake: From Zero to Hero  How to Download and Install Junit  KNN in Python: Learn How to Leverage KNN Algorithms  How to Become a Machine Learning Engineer  Intro to Deep Belief Network (DBN) in Deep Learning Table of Contents In today\u2019s world, more and more organizations are turning to machine learning and artificial intelligence (AI) to improve their business processes and stay ahead of the competition.\u00a0 The growth of machine learning and AI has enabled organizations to provide smart solutions and predictive personalizations to their customers. However, not all organizations can implement machine learning and AI for their processes due to various reasons. This is where the services of various deep learning frameworks come in. These are interfaces, libraries, or tools, which are generally open-source that people with little to no knowledge of machine learning and AI can easily integrate. Deep learning frameworks can help you upload data and train a deep learning model that would lead to accurate and intuitive predictive analysis.\u00a0  Become a Data Scientist with Hands-on Training!  TensorFlow Google\u2019s Brain team developed a Deep Learning Framework called TensorFlow, which supports languages like Python and R, and uses dataflow graphs to process data. This is very important because as you build these neural networks, you can look at how the data flows through the neural network.\u00a0 TensorFlow\u2019s machine learning models are easy to build, can be used for robust machine learning production, and allow powerful experimentation for research. With TensorFlow, you also get TensorBoard for data visualization, which is a large package that generally goes unnoticed. TensorBoard simplifies the process for visually displaying data when working with your shareholders. You can use the R and Python visualization packages as well. \u00a0 Initial release: November 9, 2015 Stable release: 2.4.1 / January 21, 2021 Written in: Python, C++, CUDA Platform: Linux, macOS, Windows, Android, JavaScript Type: Machine learning library Repository github.com/tensorflow/tensorflow License: Apache License 2.0 Website www.tensorflow.org Keras Francois Chollet originally developed Keras, with 350,000+ users and 700+ open-source contributors, making it one of the fastest-growing deep learning framework packages.\u00a0 Keras supports high-level neural network API, written in Python. What makes Keras interesting is that it runs on top of TensorFlow, Theano, and CNTK.\u00a0 Keras is used in several startups, research labs, and companies including Microsoft Research, NASA, Netflix, and Cern.\u00a0 Other Features of Keras: \u00a0 Initial release: March 27, 2015 Stable release: 2.4.0 / June 17, 2020 Platform: Cross-platform Type: Neural networks Repository github.com/keras-team/keras License: Massachusetts Institute of Technology (MIT) Website https://keras.io/ PyTorch Adam Paszke, Sam Gross, Soumith Chintala, and Gregory Chanan authored PyTorch and is primarily developed by Facebook's AI Research lab (FAIR). It\u2019s built on the Lua-based scientific computing framework for machine learning and deep learning algorithms. PyTorch employed Python, CUDA, along with C/C++ libraries, for processing and was designed to scale the production of building models and overall flexibility. If you\u2019re well-versed with C/C++, then PyTorch might not be too big of a jump for you. PyTorch is widely used in large companies like Facebook, Twitter, and Google.\u00a0 Other Features of the Deep Learning Framework Include: \u00a0 Initial release: September 2016 Stable release: 1.7.1 / December 10, 2020 Platform: IA-32, x86-64 Type: Library for machine learning and deep learning Repository github.com/pytorch/pytorch License: Berkeley Software Distribution (BSD) Website https://pytorch.org/ Theano The University de Montreal developed Theano, written in Python and centers around NVIDIA CUDA, allowing users to integrate it with GPS. The Python library allows users to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays.\u00a0 \u00a0 Initial release: 2007 Stable release: 1.0.5 / July 27, 2020 Platform: Linux, macOS, Windows Type: Machine learning library Repository github.com/pytorch/pytorch License: The 3-Clause Berkeley Software Distribution (BSD) Website http://www.deeplearning.net/software/theano/  Deeplearning4j (DL4J) A machine learning group that includes the authors Adam Gibson Alex D. Black, Vyacheslav Kokorin, Josh Patterson developed this Deep Learning Framework Deeplearning4j. Written in Java, Scala, C++, C, CUDA, DL4J supports different neural networks, like CNN (Convolutional Neural Network), RNN (Recurrent Neural Network), and LSTM (Long Short-Term Memory).\u00a0 After Skymind joined the Eclipse Foundation in 2017, DL4J was integrated with Hadoop and Apache Spark. It brings AI to business environments for use on distributed CPUs and GPUs. Other Features of DL4J Include: \u00a0 Initial release: 2014 Stable release: 1.0.0-beta6 / September 10, 2019 Platform: Cross-platform Type: Natural Language Processing(NLP), Deep Learning, Machine Vision, Artificial Intelligence(AI) Repository github.com/deeplearning4j/deeplearning4j License: Apache License 2.0 Website www.deeplearning4j.org Caffe Developed at BAIR or Berklee Artificial Intelligence Research and created by Yangqing Jia, Caffe stands for Convolutional Architecture for Fast Feature Embedding. Caffe is written in C++ with a Python Interface and is generally used for image detection and classification.\u00a0 Other Features and Uses of Caffe: \u00a0 Stable release: 1.0 / April 18, 2017 Type: Library for Deep Learning Repository https://github.com/BVLC/caffe Website http://caffe.berkeleyvision.org/ Chainer Developed by PreferredNetworks in collaborations with IBM, Intel, Microsoft, and Nvidia, Chainer is written purely in Python. Chainer runs on top of Numpy and CuPy Python libraries and provides several extended libraries, like Chainer MN, Chainer RL, Chainer CV, and many other libraries.\u00a0 Other Features of Chainer: \u00a0 Author: Seiya Tokui Initial release: June 9, 2015 Stable release: 7.7.0 / July 30, 2020 Platform: Cross-Platform Type: Deep Learning library Repository https://github.com/chainer/chainer License: Massachusetts Institute of Technology (MIT) Website https://chainer.org/ Microsoft CNTK Microsoft Research developed CNTK, a deep learning framework that builds a neural network as a series of computational steps via a direct graph. CNTK supports interfaces such as Python and C++ and is used for handwriting, speech recognition, and facial recognition.\u00a0 Other Features of Microsoft CNTK Include: \u00a0 Initial release: January 25, 2016 Stable release: 2.7.0 / April 26, 2019 Type: Library for Machine Learning and Deep Learning Repository https://github.com/Microsoft/CNTK License: Massachusetts Institute of Technology (MIT) Website https://www.microsoft.com/en-us/cognitive-toolkit/  Become a Data Scientist with Hands-on Training!  Master Deep Learning Framework Concepts with Simplilearn All of these deep learning packages have their own advantages, benefits, and uses. It\u2019s not mandatory that you stick to a single framework\u2014you can jump back and forth between most.\u00a0 To learn more about deep learning frameworks, you should enroll in our world-class Data Science Bootcamp, delivered in partnership with IBM. Explore and enroll today! Find our Post Graduate Program in AI and Machine Learning Online Bootcamp in top cities: About the Author Simplilearn is one of the world\u2019s leading providers of online training for Digital Marketing, Cloud Computing, Project Management, Data Science, IT, Software Development, and many other emerging technologies. Recommended Programs Post Graduate Program in AI and Machine Learning  Caltech Post Graduate Program in AI and Machine Learning  Professional Certificate Course in Generative AI and Machine Learning  *Lifetime access to high-quality, self-paced e-learning content. Recommended Resources Different Types of Machine Learning: Exploring AI's Core Machine Learning Career Guide: A Playbook to Becoming a Machine Learning Engineer Course Review: Training for a Career in AI and Machine Learning Discover the Differences Between AI vs. Machine Learning vs. Deep Learning How to Become a Machine Learning Engineer Machine Learning Interview Guide \u00a9 2009 -2024- Simplilearn Solutions. Follow us! Company Work with us Discover For Businesses Learn On the Go! Trending Post Graduate Programs Trending Master Programs Trending Courses Trending Categories Trending Resources"}
{"url": "https://developer.nvidia.com/deep-learning-frameworks", "title": "Deep Learning Frameworks | NVIDIA Developer", "content": "Deep Learning Frameworks Deep learning (DL) frameworks offer building blocks for designing, training, and validating deep neural networks through a high-level programming interface. Widely-used DL frameworks, such as PyTorch, JAX, TensorFlow, PyTorch Geometric, DGL, and others, rely on GPU-accelerated libraries, such as cuDNN, NCCL, and DALI to deliver high-performance, multi-GPU-accelerated training. View all Frameworks\u00a0\u00a0Deep Learning Frameworks FAQs  NVIDIA-Optimized DL Frameworks Developers, researchers, and data scientists can get easy access to NVIDIA optimized DL framework containers with DL examples that are performance-tuned and tested for NVIDIA GPUs. This eliminates the need to manage packages and dependencies or build DL frameworks from source. Containerized DL frameworks, with all dependencies included, provide an easy place to start developing common applications, such as conversational AI, natural language understanding (NLU), recommenders, and computer vision. Visit the NVIDIA NGC\u2122 catalog to learn more. PyTorch PyTorch is a Python package that provides two high-level features: Reuse your favorite Python packages, such as numpy, scipy and Cython, to extend PyTorch when needed. Model Deployment For high performance inference deployment for PyTorch trained models: Learning Resources TensorFlow TensorFlow is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) that flow between them. This flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device without rewriting code. For visualizing TensorFlow results, TensorFlow offers TensorBoard, a suite of visualization tools. Model Deployment For high performance inference deployment for TensorFlow trained models: Learning Resources JAX JAX is a Python library designed for high-performance numerical computing and machine learning research. JAX can automatically differentiate native Python and implement the NumPy API. With just a few lines of code change, JAX enables distributed training across multi-node, multi-GPU systems, with accelerated performance through XLA. Learning Resources PaddlePaddle PaddlePaddle provides an intuitive and flexible interface for loading data and specifying model structures. It supports CNN, RNN, and multiple variants, and easily configures complicated deep models. PaddlePaddle also provides extremely optimized operations, memory recycling, and network communication, and makes it easy to scale heterogeneous computing resources and storage to accelerate the training process. Model Deployment For high performance inference deployment for PaddlePaddle trained models: Learning Resources MATLAB MATLAB makes DL easy for engineers, scientists, and domain experts. With tools and functions for managing and labeling large data sets, MATLAB also offers specialized toolboxes for working with machine learning, neural networks, computer vision, and automated driving. With just a few lines of code, MATLAB allows you to create and visualize models, and deploy models to servers and embedded devices without being an expert. MATLAB also enables users to automatically generate high performance CUDA code for DL and vision applications from MATLAB code. Model Deployment For high performance inference deployment of MATLAB trained models, use MATLAB GPU Coder to automatically generate TensorRT-optimized inference engines from cloud to embedded deployment environments. Learning Resources     "}
{"url": "https://www.geeksforgeeks.org/deep-learning-frameworks/", "title": "7 Best Deep Learning Frameworks You Should Know in 2024 - GeeksforGeeks", "content": "7 Best Deep Learning Frameworks You Should Know in 2024 As we are now moving at this speed in technology, we\u2019re becoming more adaptive toward it. In recent years, there has been a lot of noise about Deep Learning, especially in the field of technology (to be specific, data science), and it\u2019s been widely used today in various industries. The Deep Learning Market is expected to be\u00a0around US$ 25.50 Billion by 2025\u00a0 Table of Content Deep Learning, Machine Learning has become one of the most significant weapons in technology today including self-driving cars, automated tasks, AI-based voice-overs, and whatnot, it is widely operating in almost every domain to make work-life balance simpler and more advanced. Why this has become an urge of this technology and most demanding in every corner of the world? To help you with that, we\u2019re here with this article to discuss the anomaly of deep learning and the frameworks that are being widely used today.\u00a0 What is a Deep Learning Framework? A deep learning framework is a software library or tool that includes a set of APIs (Application Programming Interfaces), abstractions, and tools to assist developers in building and training deep learning models. Deep learning frameworks could help you upload data and train a deep learning model that would lead to accurate and intuitive predictive analysis. These frameworks simplify the process of creating and deploying neural networks, allowing researchers and engineers to focus on complicated machine-learning tasks. The course on Complete Machine Learning and Data Science Course gives you access to the course explaining ML and AI concepts such as Regression, Classification, and Clustering, and you will get to learn all about NLP.\u00a0 7 Best Deep Learning Frameworks You Should Know in 2024  Explore these deep-learning frameworks designed to advance your projects and boost your results. Whether you\u2019re a beginner eager to work on your first project or an experienced developer trying to find something new in AI innovation, this list provides you with the knowledge to choose the perfect framework for your needs. Let us move on to check out the 7 best deep learning frameworks you should know in 2024. 1. TensorFlow TensorFlow is one of the most popular, open-source libraries that is being heavily used for numerical computation deep learning. Google introduced it in 2015 for their internal RnD work but later when they saw the capabilities of this framework, they decided to make it open and the repository is available at TensorFlow Repository. As you\u2019ll see, learning deep learning is pretty complex but making certain implementations is far easier, and by such frameworks, it\u2019s even smoother to process the desired outcomes. How Does it Work? This framework allows you to create dataflow graphs and structures to specify how data travels through a graph with the help of inputs as tensors (also known as a multi-dimensional graph). Tensor Flow allows users to prepare a flowchart and based on their inputs, it generates the output. Applications of Tensor Flow: 2.\u00a0PyTorch The most famous, that even powers \u201cTesla Auto-Pilot\u201d is none other than Pytorch which works on deep learning technology. It was first introduced in 2016 by a group of people (Adam Paszke, Sam Gross, Soumith Chintala, and Gregory Chanan), under Facebook\u2019s AI lab. The interesting part about PyTorch is that both C++ & Python can use it but Python\u2019s interface is the most polished. Not so surprisingly, Pytorch is being backed by some of the top giants in the tech industry (Google, Salesforce, Uber, etc.). It was introduced to achieve two major goals, the first is to remove the requirement of NumPy (so that it can power GPU with tensor) and the second is to offer an automatic differentiation library (that is useful to implement neural networks). How Does it Work? This framework uses a computational dynamic graph right after the declaration of variables. Besides this, it uses Python\u2019s basic concepts like loops, structures, etc. We have often used NLP functions in our smartphones (such as Apple\u2019s Siri or Google Assistant), and they all use deep learning algorithms known as RNN or Recurrent Neural Network. Applications of PyTorch: 3.\u00a0Keras Since we\u2019ve been talking about deep learning and its complexity, Keras is another highly productive library that focuses on solving deep learning problems. Besides this, Keras also helps engineers to take full advantage of the scalability and cross-platform capabilities to apply within their projects. It was first introduced in 2015 under the ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System) project. Keras is an open-source platform and is being actively used as a part of Python\u2019s interface in machine learning and deep neural learning. Today, big tech giants like Netflix, Uber, etc. are using Keras actively to improve their scalability. How Does it Work? The architecture of Keras has been designed in such a way that it acts as a high-level neural network (written in Python). Besides this, It works as a wrapper for low-level libraries (such as TensorFlow or Theano) and high-level neural network libraries. It was introduced with the concept of performing fast testing and experiments before going on the full scale. Applications of Keras: 4.\u00a0Theano To define any mathematical expressions in deep learning, we use Python\u2019s library Theano. It was named after a great Greek mathematician \u201cTheano\u201d. It was released in 2007 by MILA (Montreal Institute for Learning Algorithms) and Theano uses a host of clever code optimizations to deliver as much performance at maximum caliber from your hardware. Besides this, there are two salient features at the core of any deep-learning library: These two features enable us to work with a big bucket of data. Moreover, Theano proposes automatic differentiation which is a very useful feature and can also solve numeric optimization on a big picture than deep learning complex issues. How Does it Work? If you talk about its working algorithm, Theano itself is effectively dead, but the deep learning frameworks built on top of Theano, are still functioning which also include the more user-friendly frameworks- Keras, Lasagne, and Blocks that offer a high-level framework for fast prototyping and model testing in deep learning and machine learning algorithms. Applications of Theano: 5. Deeplearning4j (DL4J) Deeplearning4j (DL4J) is a free tool, a deep learning framework for building applications using Java and Scala. It was created by Skymind the reason behind its popularity is that it works well with existing Java-based systems, thanks to its compatibility with the Java Virtual Machine (JVM). DL4J lets developers make and use strong models for things like recognizing images and speech, understanding language, and making predictions. How Does it Work? Deeplearning4j (DL4J) helps developers by providing a set of libraries for Java and Scala programmers to build and deploy deep learning models. It takes advantage of the Java Virtual Machine (JVM) for compatibility and supports various neural network architectures. DL4J makes easy tasks like image and speech recognition, natural language processing, and predictive analytics. Its emphasis on distributed computing enables efficient training of large-scale models across multiple machines. Application of Deeplearning4j (DL4J) 6. Scikit-learn Originating from the notion SciPy Toolkit was designed to operate and handle high-performance linear algebra. Firstly, it was introduced back in 2007 during the Google Summer of Code project by David Cournapeau. This model is designed on various frameworks such as NumPy, SciPy, and Matplotlib and has been written in Python. The objective of sci-kit-learn is to offer some of efficient tools for Deep learning, Machine learning, and statistical modeling that enlist: Moreover, it offers two different varieties of algorithms (supervised and unsupervised). How Does it Work? The sole purpose of introducing this library is to achieve the level of robustness and support required for use in production systems, which means a deep focus on concerns (that include ease of use, code quality, collaboration, documentation, and performance). Although the interface is Python, c-libraries are an advantage for performance (such as NumPy) for arrays and matrix operations. Application of Scikit-learn 7. \u00a0Sonnet Sonnet is a high-level toolkit for creating sophisticated neural network architectures in TensorFlow. This deep learning framework is built on top of TensorFlow. Sonnet seeks to construct and generate Python objects that correspond to certain parts of a neural network. These objects are then individually linked to the computational TensorFlow graph. This approach of independently building Python objects and attaching them to a graph simplifies the construction of high-level structures. This is one of the greatest deep-learning frameworks available. How Does it Work? Sonnet, crafted by DeepMind, is a user-friendly deep learning framework used for constructing neural networks with TensorFlow. It simplifies the creation of models through high-level abstractions, modular design, and efficient parameter management. Sonnet also works well with TensorFlow, making it easier for scientists and developers to create and train smart systems for different jobs. It\u2019s like a friendly assistant that makes constructing and optimizing sophisticated models straightforward. Application of Sonnet Must Read Conclusion Once you understand the basics of the main deep learning frameworks, you can confidently select the one that fits your project needs. It\u2019s crucial to identify the most suitable framework for your particular use or implementation. Each deep learning framework, like TensorFlow, PyTorch, Keras, Sonnet etc comes with its unique advantages. You are not bound to one framework in your building of deep learning models journey; feel free to switch between them based on your project requirements. This flexibility ensures you can optimize your approach and achieve the best results. Similar Reads  What kind of Experience do you want to share?"}
{"url": "https://health.clevelandclinic.org/ai-in-healthcare", "title": "AI in Healthcare: Benefits and Examples", "content": "Advertisement How AI Is Being Used to Benefit Your Healthcare Artificial intelligence and machine learning are being integrated into chatbots, patient rooms, diagnostic testing, research studies and more \u2014 all to improve innovation, discovery and patient care The age of artificial intelligence (AI) and machine learning has arrived. And with it, comes the promise to revolutionize healthcare. There are projections that AI in healthcare will become a $188 billion industry worldwide by 2030. But what will that actually look like? How might AI be used in a medical context? And what can you expect from AI when it comes to your own personal healthcare? Advertisement Cleveland Clinic is a non-profit academic medical center. Advertising on our site helps support our mission. We do not endorse non-Cleveland Clinic products or services. Policy Already, healthcare providers, surgeons and researchers are using AI to develop new drugs and treatments, diagnose complex conditions more efficiently and improve patients\u2019 access to critical care \u2014 and this is only the beginning. Our experts share how AI is being used in healthcare systems right now and what we can expect down the line as the innovation and experimentation continues. What are the benefits of using AI in healthcare and hospitals? Artificial intelligence describes the use of computers to do certain jobs that once required human intelligence. Examples include recognizing speech, making decisions and translating between different languages. Machine learning is a branch of AI that focuses on computer programming. It uses extremely large datasets and algorithms to learn how to do complex tasks and solve problems similar to the way a human would. When used together, AI and machine learning can help us be more efficient and effective than ever before. These tools are being used with thousands of datasets to improve our ability to research various diseases and treatment options. These tools are also used behind the scenes, even before patients arrive onsite for care, to improve the patient experience. From radiology to neurology, emergency response services, administrative services and beyond, AI is changing the way we take care of ourselves and each other. In many ways, these innovations are forcing us to confront age-old questions: How can we continue to push ourselves to be better at what we already do well? And what\u2019s left to learn as we embrace groundbreaking technology? Advertisement \u201cAI is no longer just an interesting idea, but it\u2019s being used in a real-life setting,\u201d says Cleveland Clinic\u2019s Chief Digital Officer Rohit Chandra, PhD. \u201cToday, there\u2019s a decent chance a computer can read an MRI or an X-ray better than a human, so it\u2019s relatively advanced in those use-cases. But then at the other extreme, you\u2019ve got generative AI like ChatGPT and all sorts of cool stuff you hear about in the media that\u2019s fascinating technology, but less mature. The potential for it is there and it\u2019s also quite promising.\u201d To that end, Cleveland Clinic has become a founding member of a global effort to create an AI Alliance \u2014 an international community of researchers, developers and organizational leaders all working together to develop, achieve and advance the safe and responsible use of AI. The AI Alliance, started by IBM and Meta, now includes over 90 leading AI technology and research organizations to support and accelerate open, safe and trusted generative AI research and development. Cleveland Clinic will lead the effort to accelerate and enhance the ethical use of AI in medical research and patient care. An example of Cleveland Clinic\u2019s commitment to AI innovation is the Discovery Accelerator, a 10-year strategic partnership between IBM and Cleveland Clinic, focused on accelerating biomedical discovery. \u201cBiomedical research is changing from a discipline that was once exclusively reliant on experiments in a lab done on a bench with animal models or biological samples to a discipline that involves heavy and fast computational tools,\u201d says the Accelerator\u2019s executive lead and Cleveland Clinic\u2019s Chief Resource Information Officer Lara Jehi, MD. \u201cThat shift has happened because the data we now have at our disposal is way more than what we had even just 10 years ago,\u201d she continues. \u201cWe can now measure in detail the genetic composition of every single cell in the human body. We can measure in detail how that genetic composition is translating itself to proteins that our body is making, and how those proteins are influencing the function of different organs in our body.\u201d AI and machine learning are being integrated into every step of the patient care process \u2014 from research to diagnosis, treatment and aftercare. And that means the field of healthcare is forever changing. These kinds of changes require new approaches to medical science and new skill sets for incoming nurses, doctors and surgeons interested in working in the medical field. How fast is this technology moving? If we took our understanding of how the human body worked just 10 years ago and compared it to our understanding of how it works today with our new AI measurement tools, Dr. Jehi says that we\u2019d have a completely different outlook on how the human body works. Advertisement \u201cThe advances in AI would be like taking a fuzzy black and white picture from the 1800s and comparing it to one from an iPhone 14 Pro with high definition and color,\u201d she illustrates. \u201cThis is the difference with the scale and the resolution of the data that we have to work with now.\u201d So, what does AI and machine learning use look like in practice? Well, depending on the area of focus, medical specialty and what\u2019s needed, AI can be used in a variety of ways to impact and improve patient outcomes. Diagnostics Broken bones, breast cancer, brain bleeds \u2014 these conditions and many others, no matter how complex, need the right kind of tools to make a diagnosis. And often, a patient\u2019s journey depends on receiving the right diagnosis. \u201cIn radiology, technology and computers are used every day by doctors to identify diseases before anyone else,\u201d shares diagnostic radiologist Po-Hao Chen, MD. \u201cIn many cases, a radiologist is the first one to call the disease when it happens.\u201d But how does AI fit into diagnostic testing? Well, let\u2019s revisit the definition of machine learning. Let\u2019s say you show a computer program a series of X-rays that may or may not show bone fractures. After reviewing those photos, the program tries to guess which ones are bone fractures. When it gets some of those answers wrong, you give it the correct answers. Then, you feed it another series of X-rays and have it rerun the program again with that new knowledge. Over time, the program gets better at identifying what\u2019s a bone fracture and what\u2019s not. Each time this process occurs, it\u2019s able to make those decisions faster, more efficiently and more effectively. Advertisement Now, imagine that same process, but with hundreds or thousands of other datasets and other conditions. You can probably see how AI can help pinpoint and identify findings with the help of a radiologist\u2019s expertise. \u201cIt works like a second pair of eyes, like a shoulder-to-shoulder partner,\u201d says Dr. Chen. \u201cThe combined team of human plus AI is when you get the best performance.\u201d The radiologists of the future will have a very different skill set compared to radiologists who excel today, notes Dr. Chen. And that future skill set will involve a significant portion of AI know-how. \u201cIt wasn\u2019t that long ago when almost all radiology was done on physical film that you held in your hand,\u201d he adds. \u201cAs radiology became computerized, doctors had to enhance their skill set. AI is changing digital radiology the same way digital radiology changed film.\u201d Breast cancer Breast cancer radiology has shown promising results using AI, according to breast cancer radiologist Laura Dean, MD. \u201cEveryone\u2019s breast tissue is like their fingerprint or their handprint,\u201d she clarifies. \u201cIn other words, breast cancer can look very different from one patient to another. So, what we look for are very subtle changes in the appearance of the patient\u2019s own breast pattern. This is where we are really seeing an advantage of using AI in our interpretations.\u201d Advertisement Breast cancer experts widely agree that annual screening mammography beginning at age 40 provides the most life-saving benefits. \u201cIn breast imaging exams, we\u2019re looking to see if the patterns in someone\u2019s breast tissue look stable. A very important part of mammography interpretation is pattern recognition,\u201d explains Dr. Dean. \u201cAre there areas that are new or changing or different? Are there areas where the tissue just looks a little bit different or is there a unique finding in the breast?\u201d It\u2019s up to the radiologist to review the 3D images and search for areas of density, calcifications (which can be early signs of cancer), architectural distortion (areas where tissue looks like it\u2019s pulling the surrounding tissue) and other areas of concern. \u201cA lot of cancers are really, really subtle. They can be really hard to see, depending on the patient\u2019s breast tissue, the type of breast cancer, how the tissue is evolving and how the cancer is developing,\u201d Dr. Dean notes. \u201cIf every breast cancer were one of those obvious textbook spiculated masses with calcifications, it would make my job a lot easier. But as our technology continues to improve, many of the cancers we\u2019re seeing now are really, really subtle. Those subtle cancers are the areas where I think AI has shown a lot of promise.\u201d There are now several AI-detection programs available for use in mammography. The first one to get approval from the U.S. Food and Drug Administration is iCAD\u2019s ProFound AI, which can compare a patient\u2019s mammography against a learned dataset to pinpoint and circle areas of concern and potential cancerous regions. When the AI identifies these areas, the program also highlights its confidence level that those findings could be malignant. For example, a confidence level of 92% means that in the dataset of known cancers from which the algorithm has trained, 92% of those that look like the case at hand were ultimately proven to be cancerous. \u201cThe first step is identifying the finding, and then, using all of my expertise and my diagnostic criteria to determine if it\u2019s a real finding,\u201d explains Dr. Dean. \u201cIf it\u2019s something that I think looks suspicious, then it warrants diagnostic imaging. We bring the patient back, do additional diagnostic views and try to see if that finding is reproducible \u2014 can we still see it? Where is it in the breast? And then, we have other tools such as targeted ultrasound where we would home in right on that area and see if there is a mass there, what the breast tissue looks like and then do a biopsy if needed.\u201d One benefit of AI programs is that they can function like a second set of eyes or a second reader. It improves the overall accuracy of the radiologist by decreasing callback rates and increasing specificity. \u201cWe are seeing that the AI can guide the radiologist to work up a finding they might not have otherwise seen,\u201d she says. That\u2019s especially important when you consider that earlier detection is crucial to helping identify cancers at the lowest possible stage, especially for aggressive molecular subtypes of breast cancer. Earlier detection may also help decrease the rate of interval cancers, or those that develop between mammogram screenings. \u201cI think it\u2019s really beneficial to look at how AI is helping in the so-called near-miss cases. These are findings that are really hard to see for even a very experienced radiologist,\u201d he continues. \u201cIn general, radiologists should be calling back less with the help of AI. And that\u2019s the point: AI helps us tease out which cases are truly negative and which cases are truly suspicious and need to come back for further testing.\u201d Triage Improving access to patient care can be critical, especially for emergencies. While we continue to work against bias in healthcare, AI is being used to triage medical cases by bumping those considered most critical to the top of the care chain. \u201cWe do it on a disease-by-disease case,\u201d says Dr. Chen. \u201cWe identify diseases that need to be caught as early as possible and then we develop or bring in technology to do that. One instance we\u2019re doing that is with stroke.\u201d Stroke Time is brain tissue \u2014 so every minute counts when someone is having a stroke. \u201cIt\u2019s not all or nothing. It\u2019s a process that happens over time,\u201d explains Dr. Chen. \u201cThe problem is that that timeframe is measured in minutes. Every minute that a patient doesn\u2019t receive care or doesn\u2019t receive intervention, a little bit more of their brain becomes irreversibly damaged.\u201d And that\u2019s especially true when you have what\u2019s called a large vessel occlusion, a kind of ischemic stroke that occurs when a major artery in the brain is blocked. That kind of stroke is treatable if it\u2019s discovered in the right amount of time. Now, out in the field, if EMS gets a call that they\u2019re dealing with a possible stroke, they have the capability to trigger a stroke alert. This alert sets off a cascade of management events that prepares a team for a patient\u2019s arrival and treatment plan \u2014 available surgeons are alerted, beds are made available, rooms are prepped for surgery, and so on. \u201cWe add AI to the front end of that process,\u201d he further explains. \u201cWhen patients who have a suspected stroke receive a scan, AI now reviews those images before any human has an opportunity to even open the scan on their computer.\u201d As soon as the brain scan is taken, the image is sent to a server where the program, Viz.ai, analyzes it fast and efficiently using its neural network to arrive at a preliminary diagnosis. \u201cThe AI is cutting down precious minutes by being the first and fastest agent in this process to review those images,\u201d says Dr. Chen. \u201cIf you can find a patient that\u2019s having a stroke that can be treated, then it makes absolute sense to do everything you possibly can do to mobilize resources to treat it.\u201d If a large vessel occlusion is found, the program begins coordinating care. It\u2019s integrated into scheduling software, so it knows who\u2019s on call and which doctors need to be notified right away. \u201cThe AI software kicks off a series of communications to make sure everyone in the chain \u2014 all the doctors, neurosurgeons, neurologists, radiologists and so on \u2014 are aware that this is happening and we\u2019re able to expedite care,\u201d he continues. Complex measurements A patient\u2019s journey often doesn\u2019t begin and end with diagnosis and treatment. Often, the journey involves watching, waiting and revisiting a diagnosis. For example, in the case of lung cancer, it\u2019s common for oncologists to begin tracking the growth of nodules before they\u2019re proven to be cancerous. \u201cThat\u2019s the whole point of doing screening programs,\u201d says Dr. Chen. \u201cThe ones that grow are more likely to be cancer. The ones that don\u2019t grow are more likely to be benign. That\u2019s why they\u2019re important to track over time. And most of that work is done manually by trained radiologists who go through every nodule that they can see in the lung. They track it, measure it and report on it.\u201d That kind of work can be tedious and time-consuming. That\u2019s why it\u2019s a focus area for using AI. \u201cWe are actively looking at and trying to deploy a solution that can do the detection and measurement of these nodules in the lung automatically,\u201d he adds. \u201cThat would help with the consistency and reproducibility of those measurements now with different kinds of cancer.\u201d Managing tasks and patient services Like the scheduling software, AI is being utilized in small and large ways to free up physicians\u2019 time behind the scenes and to help increase patients\u2019 access to care. In his 2024 State of the Clinic address, Cleveland Clinic\u2019s CEO and President Tom Mihaljevic, MD, highlighted several practical areas AI is already being used both in and out of the exam room. Among them: Broadly speaking, AI can also be beneficial when it comes to virtual appointments. Studies show that AI monitoring tools have been beneficial when it comes to seeing if patients are using medications like inhalers or insulin pens the way they\u2019re prescribed and providing much-needed guidance when questions arise. The future of AI in healthcare The future of AI in healthcare, notes Dr. Jehi, is perhaps brightest in the realm of research. \u201cI\u2019ve learned throughout this process that there is a lot more to be learned by using AI,\u201d she says. As an epilepsy specialist, Dr. Jehi researches how machine learning has changed epilepsy surgery as we know it. Traditionally, if a patient with epilepsy continues to have seizures and isn\u2019t responding to medication treatment, surgery becomes the next best option. As part of the surgical procedure, a surgeon would find the spot in the brain that\u2019s triggering the seizures, make sure that spot isn\u2019t critical for their functioning and then safely remove it. \u201cThe way we used to make those decisions, we\u2019d do a bunch of tests, we\u2019d measure brainwaves, we\u2019d take a picture of the brain, we\u2019d look at how the radiologist or the EEG doctor interpreted the results, and then, we\u2019d take the test results,\u201d she shares. \u201cBased on our own human experience, we\u2019d decide if we want to do the surgery or not. But we were very limited in our ability to build collective knowledge.\u201d In essence, Dr. Jehi explains that doctors were stuck in a vacuum. They knew the expertise they\u2019d gained over the years had been valuable on an individual level, but without looking at the bigger picture, it was hard to tell who would respond best to which surgical technique if they were coming in as a first-time patient. Now, machine learning has filled in that gap in collective knowledge by pulling together all this patient data and distilling it down into one location. Doctors can access that information all in one place and use it to research the disease and the effectiveness of different treatment options, and use that information to inform their practice. \u201cFrom the patient perspective, nothing really much changes. They\u2019re still getting the tests that they need for the clinical decision to be made,\u201d she enthuses. \u201cThat is the beauty of what AI offers. It\u2019s a task for us to get exponentially more insight from the same type of clinical data that we always had but we just didn\u2019t know what to do with. AI is allowing us to deep dive into those tests and get more insights than just what our superficial initial interpretation was.\u201d Currently, Dr. Jehi is working to improve specialized AI predictive models that can accurately guide medical and surgical epilepsy decision-making. \u201cWe are doing research to come up with a way to reduce these complex AI models to simpler tools that could be more easily integrated in clinical care,\u201d she notes. Dr. Jehi and other researchers have also identified biomarkers with the help of machine learning that determine which patients have a higher risk for epilepsy reoccurring after having surgery. And work is currently being done to fully automate detecting and locating brain segments that need to be removed during epilepsy surgery. Right now, Dr. Jehi is focusing on understanding how a patient\u2019s genetic composition and brain plays into their epilepsy. How do they respond to epilepsy based on a number of factors? How do they respond to epilepsy surgery? And are these factors related to how well their surgery works down the road? \u201cWe\u2019ve been completely overlooking how nature works,\u201d she says. \u201cUntil now, we haven\u2019t really analyzed how the genetic makeup of individuals factors into all of this. With my research, we have a lot of evidence that makes us believe that genetic makeup is actually quite important in driving surgical outcomes.\u201d With AI and machine learning, Dr. Jehi hopes to continue pushing this research to the next level by looking at increasingly larger groups of patients. Our AI journey As we continue to improve our understanding of AI and further our pursuit of innovation and discovery, it\u2019s up to healthcare providers around the world to question how best to utilize the tools at their disposal. Already, the World Health Organization (WHO) has issued additional guidelines for safe and ethical AI use in the healthcare space \u2014 a continued effort that builds off their original 2021 guidelines but with added caution around large language models like ChatGPT and Bard. But when AI is used to further research and improve patient care with ethics and safety as the foundation of those efforts, its potential for the future of healthcare knows no bounds. \u201cI see AI as a path forward that helps us make sure that no data is left behind,\u201d encourages Dr. Jehi. \u201cWhen we\u2019re doing research and we\u2019re developing a new predictive model, or we want to better understand how a disease progresses, or we want to develop a new drug, or we want to just generate new knowledge \u2014 that\u2019s what research is. It\u2019s the generation of new knowledge. The more data that we can put in, the more our chances are of finding something new and of those things actually being meaningful.\u201d Learn more about our editorial process. Advertisement Related Articles Mammogram vs. Breast Ultrasound: Which One Is Best? One is the gold standard for breast screening, while the other is used as a  complementary diagnostic tool What To Expect With HER2-Low Metastatic Breast Cancer HER2-low is less aggressive than HER2-positive and HER2-negative, but the new designation allows for targeted therapies that may be helpful Self-Care Is Important When You\u2019re Living With HER2-Negative Metastatic Breast Cancer Taking care of yourself extends beyond symptom management and includes things like passion projects and meaningful moments HER2-Low Metastatic Breast Cancer: Finding Community Support groups, financial assistance and survivorship programs are all readily available How Fast Can HER2-Positive Breast Cancer Spread? It depends on factors like the stage of the cancer, your age and other risks Living With HER2-Positive Brain Metastases Receiving this diagnosis can be scary, but there are ways to manage symptoms and reduce stress Following a Healthy Diet When You Have HER2-Negative Metastatic Breast Cancer Eating well can help support your immune system and maintain strength  What Are the Differences in Left vs. Right Brain Strokes? Strokes in the left side of the brain are more common and the effects are typically more noticeable Trending Topics Home Remedies for an Ear Infection: What To Try and What To Avoid Not all ear infections need antibiotics \u2014 cold and warm compresses and changing up your sleep position can help 52 Foods High In Iron Pump up your iron intake with foods like tuna, tofu and turkey Is Starting Your Day With Lemon Water Healthy? A glass of lemon water in the morning can help with digestion and boost vitamin C levels, and may even help get you into a better routine Health Categories to Explore Other Popular Categories Cleveland Clinic Site Information & Policies Resources"}
{"url": "https://courses.minnalearn.com/en/courses/emerging-technologies/robotics-and-automation/introduction-to-robotics-and-automation/", "title": "Introduction to robotics and automation", "content": "I. Introduction to robotics and automation Robotics is a field of science working with machines that perform tasks based on predetermined and adaptive programs and algorithms in an automatic or semi-automatic way. These machines \u2013 commonly called robots \u2013 are either controlled by humans or work entirely under the supervision of a computer application and algorithms. Robotics is a comprehensive concept that includes the building, planning and programming of robots. These robots are in direct contact with the physical world \u2013 and they have often been used to perform monotonous and repetitive tasks instead of human beings. Robots can be categorised based on their size, application domain or purpose, and we will discuss this later. Robotics vs automation Automation is a much broader concept than robotics. It means that specific parts of a process or an entire process is performed without human intervention. Instead, the process is operated only by predefined or adaptive computer applications and electrical or mechanical machines. The predefined applications refer to algorithms, in which all the operations are predefined and executed independently regardless of any unforeseen changes in the environment. Adaptive automation means that the algorithm can change its behaviour according to changes in the process or environment.  Robotics goes hand-in-hand with automation, as in most cases robots are part of an automated system. Although there are times when robots are used with little or even no automation \u2013 and you can also have automation without robotics \u2013 the two are like twins that have a lot in common, but each with their own distinct personality.   If there are machines that perform tasks for us, why do we even work? The fact is that the capabilities of robots are limited. Even if robots look smart, these machines are typically only good at some very narrow application domain. Even if we apply an armada of robots for the many tasks in our life, these domains do not overlap in a way that makes a complete system that can completely replace humans. In other words, as these robots can only operate in a very limited domain \u2013 and we are nowhere close to achieving general machine intelligence \u2013 you should not be afraid of evil robots or robot domination. The types of robots  Robots can be classified in different ways. We\u2019ll look at four main methods of categorisation:  Size  Application domain Purpose Number Size When looking at size, the following categories exist:  Nanorobots or nanobots: nanorobots are made of nanomaterials and range in size from 0.1 to 10 micrometres (to get an idea of how small that is, a human red blood cell is about 5-10 micrometres). Nanobots are in the early research stages \u2013 mostly, the concept is being discussed for use in medicine and many more years of hard work are needed to make them a possible solution. One vision for nanorobots is to inject them into a patient\u2019s body to identify and cure diseases. Microbots, millibots and minibots: these robots are very small but still larger than nanobots \u2013 and they actually already exist. Microbots, millibots and minibots are smaller than 1 mm, 1 cm and 10 cm, respectively. The smallest flying robot is RoboBee, with a wingspan of 1.2 cm and a weight of 80 milligrams. The wings can flap 120 times per second and the robot can be controlled remotely. The goal of such a small device is to form a flying swarm for search and rescue, or artificial pollination.   Small and mid-sized robots: these robots are typically under 100 cm (small) or about the size of a human (100\u2013200 cm, mid-sized). Most household robots, toys and social robots, humanoids (robots that have a similar appearance to humans \u2013 the Transformers of comic books and films being a common example), and digital personal assistants are this size. Small and mid-size robots are the kind we see and meet most of the time \u2013 in movies and in real life.   Large robots: these robots are bigger than us. Much bigger. There are some large humanoid robots, even up to 8\u201310 meters. However, humanoid large robots are typically made for research purposes, or just for fun. In fact, most large robots don't look like humans \u2013 they are made for automation in manufacturing, construction, agriculture, autonomous driving and navigation.  Application domain It is also possible to categorise robots according to their application domain, dividing them into personal and industrial robots.  Personal robots are used in our daily life and are designed to be useful for individual or family use. Non-technical people can operate personal robots to perform repetitive and perhaps boring tasks to save time or to entertain us. Household robots, social robots, digital personal assistants and toys are the most common personal robots. Industrial robots are robust and are created to perform specific tasks in a pre-programmed manner in manufacturing, construction or agriculture, for instance. Applications include assembly, disassembly, mounting, screw tightening, welding, painting, visual inspection, and so on. Industrial robots are outstanding at one specific task: these are fast, precise and reliable machines. Without industrial robots, we wouldn't really have today's level of technological development.  Purpose  Another possible categorisation for robots is purpose. Robots can have a specific and a general purpose. But what does that mean? Task-specific robots: these machines perform a particular task or a sequence of possible tasks. It can be as simple as a robot arm that moves objects from A to B, but it can be as complex as a social robot with an advanced natural language interface. The construction and behaviour of these robots cannot be changed; they follow predefined programs according to their original purpose. Household robots and industrial robots are among such machines.  General-purpose robots: in this case, the task of the robot is not predefined. Various components of the robots can be bought separately, and these components can be assembled in different ways in order to solve specific tasks. The components may include robot arms, wheels, cameras, step motors and additional sensors and actuators. These robots may also have wireless connections, such as Wi-Fi and Bluetooth. The \"brain\" of the robot \u2013 which is generally a small computer \u2013 can be  \u201ctrained\u201d  to perform different tasks with different components using custom applications written in computer programming languages. Common programmable small computers \u2013 also called embedded systems \u2013 are the Nvidia Jetson and Jetson Nano, Raspberry Pi, and Arduino. These embedded systems have general-purpose input and output connections (GPIOs) to which sensors and actuators can be connected using a standard communication interface.   Other general-purpose robots include a prebuilt body comprising sensors (like cameras and microphones) and actuators (like arms and legs). By developing different computer applications, the robot can perform different specific tasks. Examples of such robots include Softbank Robotics\u2019 Nao, Pepper and Romeo, or Boston Dynamics\u2019 robot \u2018dog\u2019, named Spot.  Number Robots can also be categorised according to how many there are: Single robots: a single robot works on its own. It has a duty which it performs based on a predefined program. The predefined program might involve advanced technologies which make it able to adapt to its environment, and the robot might be connected to the internet, but the robot is still alone. Even if there are several single robots in one place, they are still \u2019alone\u2019 as they cannot communicate with each other.  Robots in teams: robots can work in teams, just like humans. Often a task is done in sequence by several robots. Think about video recordings of how cars are assembled. The chassis is welded, then comes the doors, then the car is painted, front and rear windows are next, and so on. All of these steps are performed by different robots that can only do that particular task.  Swarm robotics: robots can also work in a swarm. In this case, a large number of simple robots are controlled collaboratively. Individual robots in the swarm are not particularly valuable, but the swarm itself can perform important tasks. Just think about bees in nature. A single bee cannot do much, but without millions of bees working in swarms, humans probably wouldn\u2019t even exist. Possible applications of swarm robotics are exploration and rescue, microbiology, surveillance and pollination. However, at the time of writing (2021) swarm robotics is mostly in the research phase.  The evolution of robots  The word robot comes from the Czech word \"robota\", which means \u201cserf work\u201d in Czech. Karel \u010capek\u2019s 1920 play, where machines take over the world, made the word \u201crobot\u201d widely known. But humanity has always been interested in the rethinking of human existence. Even before the 20th century there were several attempts to recreate a human being and legends telling of people who had succeeded. One of the most famous ideas belongs to the 16th century alchemist Paracelsus. He stated that a small human-like being (called a homunculus) could be created in a flask using only chemical procedures. Later in the 16th century, the word golem entered public consciousness. Based on a folk story, the golem was made of clay and would serve people if someone inserted a special parchment into its mouth or forehead. The story says that after a while, the golem confronted its creator and eventually turned against him.  Looking at the history of robotics, there is a universal interest in imbuing robots with humanity or some human attributes. This interest generally has three main conditions: the robot has to be similar to a human being in some way (in appearance, in thinking, etc.)  the robot has to be better at something (stronger, smarter, etc.)  the robot has to be completely under the control of its creator  There was a milestone in the history of robotics when machines that were stronger than people appeared. Machines that replaced a human's contribution to work appeared during the first industrial revolution around 1769. At that time the main purpose was to reduce costs and the time spent on production and to increase the quantity of products without human interaction. Automation became the main concept at that time. With automation, several processes can be completed without any human intervention. As work was done by machines, it led people to find new ways of working and living. Machines do not get tired like people do, so machines can work 24/7. The risk of error and the amount of waste also decreased with automation.  Robots are also characterised by controlled accuracy and effectiveness. In the 1800s, computer technology was not present. However, people were able to create large machines to perform complex tasks. After 1950, there has been an important development in robotics.  Shakey  The first industrial robot \"Unimate\" was created and used in automobile factories as a replacement for the manual workforce, and the first general-purpose mobile robot \"Shakey\" was also created. Shakey had built-in cameras and touch sensors and was able to interact with its environment. Various positive statements can be made about the existence of robots, but humanity is not 100% satisfied. The labour market is under continuous pressure from people willing to work, as workers doing repetitive tasks could be replaced with machines. A certain fear always appears in connection with robots about them replacing the human workforce, or if robots may have more control over humans than desired.  As robots get more realistic, another fear arises. People generally tolerate robot-like robots. Our brain can easily categorise robot-like humanoid robots like we categorise industrial robots in manufacturing. People may get confused and even frustrated when meeting an overly realistic robot. In this case, we know that it's a robot. However, the brain can't really deal with this fact due to its realistic appearance. Its skin, movement and even speech is very similar to that of a person, but our brain struggles with the categorisation: is it really a robot? Does it have thoughts? Can or should I trust it? These fears could be a reason for creating the \"rules of the robots\" in the middle of the 20th century, also known as The Three Laws or Asimov's Laws, created and named after the author Isaac Asimov. The three laws are:  A robot may not injure a human being or, through inaction, allow a human being to come to harm. A robot must obey the orders given it by human beings except where such orders would conflict with the First Law. A robot must protect its own existence as long as such protection does not conflict with the First or Second Law. (Asimov 1950) Another milestone in the history of robotics was when the first remote-controlled mobile robot discovered the moon's surface around 1970. A bit later, in 1986, Honda started a project to create humanoid robots that look similar to people. The evolution continued, and robots appeared in more and more fields like healthcare, manufacturing and logistics. The evolution of robots is still an ongoing process and now robots are present in our daily lives. Robots are in homes (vacuum cleaners), in workplaces (assembly robots) and in healthcare (social robots in patient treatment or surgical robots), for instance.  Humanity is in the fourth industrial revolution, which integrates the hottest emerging technologies, like robotics, IoT, 5G, artificial intelligence, and many more, to take the industry to new levels. Sign up to solve exercises  Follow us The course has been created with the Digital SkillUp initiative, which is the brand name of the outputs created under the European Digital Academy project, funded by the European Commission with support of the European Parliament. The project is dedicated to making basic knowledge on emerging technologies available and accessible to all citizens and SMEs. It aims to create an online training space that will offer learning content and opportunities on topics like AI, Blockchain, robotics, cybersecurity and IoT for everyone. This project has received funding from the European Union.  This communication reflects only the author\u2019s view. It does not represent the view of the European Commission, and the EC is not responsible for any use that may be made of the information it contains."}
{"url": "https://www.blog.trainindata.com/data-science-fundamentals/", "title": "Data Science Fundamentals: A beginner\u2019s guide - Train in Data's Blog", "content": "This site uses cookies We use cookies to recognize your repeated visits and preferences, to measure the effectiveness of our blogs and find out if users find what they're searching \t\tfor. By continuing using this site, you consent to the use of these cookies. Learn More. Data Science Fundamentals: A beginner\u2019s guide  by Noor Ul Huda | Mar 18, 2024 | Data Science In today\u2019s digital world, data is valuable, and data science is all about using it smartly to learn new things and make better decisions. If you are embarking on a journey into data science and feel like stepping into a vast and mysterious landscape, I get you. I was there. That\u2019s why, in this article, I\u2019ll take you through the data science fundamentals, so you know exactly what you need to get started and advance in this field. There are three pillars of data science: We\u2019ll start by breaking down these core pillars, from the mathematical and statistical analysis foundations, to the crucial computer science skills you\u2019ll need. Then, we\u2019ll explore how data scientists apply these fundamentals to tackle real-world problems. Let\u2019s get started\u2026 \u00a0 What is Data Science? Data science is about using data to find answers or solve problems. It involves collecting, analyzing, and interpreting large amounts of data to gain insights and make informed decisions. In practical terms, data science combines techniques from various fields, such as statistics and computer science, with domain knowledge to make sense of data. It involves tasks like cleaning and preparing data, exploring patterns and relationships within the data, and building predictive models to forecast future outcomes or identify trends. By harnessing the power of data, data scientists can uncover valuable insights that help businesses improve their operations, develop new products, or better understand their customers. Ultimately, data science enables us to leverage the wealth of information available in the modern world to drive innovation and make smarter choices. \u00a0 Data Science Fundamentals Stepping into data science requires a solid foundation built on four essential pillars: domain knowledge, mathematical or statistical skills, computer science knowledge and communication skills.  The core pillars of data science: math skills, programming skills, communication skills and programming skills. These pillars define the core competencies of a data scientist and also shape the way we approach problems, interpret data, and communicate insights. Let\u2019s look at them in more detail. \u00a0 Domain Knowledge Domain knowledge, simply put, is what you know about a specific area, like healthcare or finance, or a topic, like fraud detection or energy demand. Understanding the industry or field you\u2019re working in is crucial because it informs the questions you ask, the data you collect, and the problems you aim to solve. Without a deep understanding of the domain, data scientists might miss the nuances of the business, leading to insights that may not be accurate or actionable. \u00a0 Math Skills Math skills for data science involve understanding basic concepts like statistics and algebra, which help in analyzing data and building models to make predictions or find patterns. From linear algebra and multivariable calculus to statistics and probability, these skills enable data scientists to understand and make predictions through the use of complex models and machine learning algorithms. \u00a0 Computer Science  Computer science provides the programming skills needed to manipulate, process, and analyze large volumes of data efficiently using computational techniques. Data scientists use programming languages like Python and R to explore and analyze vast datasets and train machine learning algorithms. They use other languages like SQL or MongoDB to understand and explore relational and non-relational databases. Knowledge of distributed computing with tools like Apache Hadoop and Spark is necessary for processing large datasets, making computer science skills indispensable for tackling the technical challenges in data science. \u00a0 Communication and Visualization Skills The ability to communicate findings effectively is what transforms data science from a technical exercise to a strategic asset. This pillar ensures that the hard work put into analyzing the data translates into actionable insights that can guide decision-making and strategy.  Together, these four pillars form the foundation of a skilled data scientist. They combine the ability to understand the context (domain knowledge), crunch the numbers (math skills), leverage technology (computer science), and share insights (communication and visualization skills). Mastery in these areas enables data scientists to navigate the vast seas of data and extract valuable insights that can propel businesses and organizations forward. \u00a0 What Do Data Scientists Do? Imagine stepping into the shoes of a data scientist. Your day is like embarking on a digital adventure, where data is both your map and your mystery. From the moment you start your day, you\u2019re on a quest to turn raw, unstructured data into valuable insights that can drive decisions and strategies. Here\u2019s a glimpse into what a day in the life of a data scientist might look like, broken down into digestible, human-like explanations: \u00a0 Problem Definition Every adventure begins with a purpose. For a data scientist, it starts with understanding the problem at hand. Is it about predicting which customers might leave a service (customer churn) or spotting unusual patterns that could indicate fraud? This step is about setting the course for your journey. \u00a0 Data Collection Next, you gather your tools \u2013 in this case, data from various sources. It could be from online platforms, internal databases, or sensors. Think of it as packing your backpack with everything you might need for your expedition. \u00a0 Data Cleaning Data is rarely ready-to-use. This step involves cleaning up the data \u2013 removing duplicates, fixing errors, and getting rid of anything that doesn\u2019t help with your quest. It\u2019s like clearing rocks and branches off a trail before you start hiking. \u00a0 Exploratory Data Analysis Now, you take a closer look at the data to understand what you\u2019re working with. By analyzing trends, patterns, and outliers, you\u2019re surveying the land before diving deeper. This could involve creating visual graphs or running statistical tests. \u00a0 Data Preprocessing Before you can analyze the data in-depth, you need to get it in the right format. This might mean adjusting scales, normalizing data, or categorizing information. It\u2019s akin to packing your backpack in an organized way, so everything is accessible when you need it.  Model Building With your data ready, you start building machine learning models. This is where the magic happens. You\u2019ll use techniques from simple regression to complex neural networks to find answers hidden in your data. It\u2019s the heart of your adventure, where you use your tools to uncover secrets in the data landscape. \u00a0 Model Evaluation After building your models, you need to see if they\u2019re leading you in the right direction. By evaluating their performance with metrics, you\u2019re essentially checking your compass to ensure you\u2019re on the right path to solving your problem. \u00a0 Deployment Once you\u2019re confident in your model, it\u2019s time to put it into action. If you\u2019ve built a system that recommends products to online shoppers, now it starts suggesting items to users in the real world. This is where your journey makes a tangible impact. \u00a0 Feedback and Improvement The journey doesn\u2019t end with deployment. You keep an eye on how your models are performing, gathering feedback, and making improvements. It\u2019s an ongoing process of learning from the journey, adapting, and preparing for the next adventure. \u00a0 How to get started? Getting started with data science involves a series of steps to build the foundational knowledge and in-demand skills. Data science combines several disciplines, including statistics, mathematics, programming, and domain expertise, to analyze and interpret complex data. In the next section, I will break down the specific skills and data science tools learners need to be successful data scientists, data analysts, or business intelligence analysts. These skills and tools are a pre-requisite to working efficiently and effectively and to providing industry-standard solutions. \u00a0 Mathematics & Statistics A strong foundation in mathematics (especially calculus and linear algebra) and statistics is crucial for understanding and applying data science techniques. For this you can start separately, by enrolling yourself in data science courses that are available online on this subject on different platforms like Coursera and Udemy. To get started you can take this specialization on coursera: Mathematics for Machine Learning and Data Science Specialization. \u00a0 Programming Skills Make sure you learn a programming language that is widely used in data science. Python and R programming are the most popular choices due to their simplicity and the extensive libraries they offer for data analytics (like Pandas, NumPy, SciPy for Python; and dplyr, ggplot2 for R). For learning programming language you can take this course on Coursera named as \u201cPython for Data Science, AI & Development\u201c. Another course on Coursera by IBM named \u201cIntroduction to R Programming for Data Science\u201c, is great for new learners to start their journey in data science using R programming. \u00a0 Data Visualization Understand how to clean, manipulate, and prepare data for analysis. This involves handling missing values, and outliers, and making your data suitable for analysis. You can enroll in an excellent course on Exploratory data analysis by IBM. Make sure you check out our Feature Engineering for Machine Learning course as well. After that, learn about data visualization to find patterns, trends, and anomalies. Tools and libraries like Matplotlib, Seaborn (Python), and ggplot2 (R) are essential. \u00a0 Machine Learning Machine learning provides the tools and methodologies that allow data scientists to make predictions or decisions based on data, automate processes, and gain insights from big data. Organizations that effectively use data mining and machine learning can gain a competitive advantage by being more efficient, making better decisions, and offering innovative solutions that differentiate them in the marketplace. To kickstart your journey, I would recommend you take some data science and machine learning courses to improve data science skills. These online courses will give you a holistic understanding of the data science processes, from the basic concepts to the more advanced ones.  If you are already familiar with machine learning basics and would like to take your skills further, check out our Advanced Machine Learning specialization. \u00a0 Work on Projects Practical experience is key. Work on various data science projects to apply what you\u2019ve learned. This can include competitions on platforms like Kaggle or personal projects that interest you. You will get some hands-on experience from the data science courses that you take, but getting your skills to the next level requires working on bigger projects. By taking on projects, you\u2019ll get hands-on experience in data cleaning, exploratory data analysis, predictive modeling, and machine learning. You\u2019ll cement your leanings and be better prepared to answer questions when you get invited for a job interview. \u00a0 Data Science Applications Data science finds applications across a wide range of industries, revolutionizing how businesses operate and make decisions. For instance, in e-commerce, data science is used to personalize product recommendations based on a customer\u2019s browsing and purchase history. By analyzing past behavior and preferences, companies can tailor their offerings to individual customers, leading to higher engagement and increased sales. The same approach is used by Amazon and Netflix. Both of them heavily rely on recommendation systems powered by data science algorithms. So you might have observed that if you watch some movies of related genres, Netflix will start showing you other movies according to your current taste, how is it done? By collecting user data! They analyze user behavior, preferences, and interactions to suggest relevant movies, TV shows, products, and services to individual users. In healthcare, data science plays a vital role in improving patient outcomes and optimizing resource allocation. For example, predictive analytics can help identify patients at risk of developing certain diseases based on factors like genetics, lifestyle, and medical history. By intervening early and providing targeted interventions, healthcare providers can prevent or manage conditions more effectively, ultimately saving lives and reducing healthcare costs. \u00a0 Conclusion As we wrap up our journey into the world of data science, remember that it\u2019s all about taking big piles of data and finding the secrets hidden inside them to make smart decisions. Whether you\u2019re just starting, a keen learner, or looking to dive deeper, the path of data science is filled with exciting discoveries and challenges that can truly change the way we understand and interact with the world around us. It\u2019s a field that welcomes curious minds who love to ask questions and seek answers. So, grab your computer, get ready to learn, and who knows? You might just uncover something amazing that can make a real difference. Follow us\u00a0 \ud83d\udc47 Pages Courses Books Legal Privacy Policy Terms of use Impressum Our reviews on Trustpilot \u00a9 Train in Data 2024"}
{"url": "https://www.staffordglobal.org/blog/fundamental-data-science-concepts/", "title": "Data Science Concepts: The fundamentals of Data Science", "content": "MBA Degree Executive MBA University of Northampton Online MBA Edinburgh Napier University Global Online MBA University of Hull MBA Business Analytics University of Leicester View all MBA degrees Education Degree PGCEi University of Nottingham Master of Education University of Dundee Early Childhood Education University of Dundee MSc Inclusive Education University of Dundee View all Education degrees Popular Courses BSc Nursing Edinburgh Napier Univeristy MSc Engineering Management University of Hull MSc Artificial Intelligence University of Hull Doctor of Business Administration University of Northampton View all Courses Admissions Course Index Universities Refer a Friend   Course Fee Index   About Stafford Meet The Team Stafford History Partner With Us Graduation Gallery Learn Events & Webinar Student Testimonials Blog Online Learning Why Study Online Online Learning FAQ   Course Category     Video Library   Admissions Course Index Universities Refer a Friend   Course Fee Index   5 Fundamental Data Science Concepts Table of Contents Data Science, Data Analytics, and Business Analytics are complex subjects that interweave a myriad of concepts from mathematics, statistics, programming, computing, and management-level skills. This article covers five fundamental Data Science Concepts. What is Data Science? Data science is a scientific field that utilises structured and unstructured data, manipulating it through various processes and algorithms to extract purpose-specific knowledge. The Data Science Lifecycle The same set of data can be analysed by different industries for various purposes, resulting in specific sought-after knowledge. Despite the varied applications, the data science lifecycle is largely similar. \u00a0 Why is Data Science Important? Since the dawn of the internet, the amount of generated data has grown exponentially. Whether researching online, shopping, or socialising, every click, pause, or stop generates data. Depending on the industry, your data, and the data of people like you, is extremely valuable to companies that use this information to better understand customer preferences and buying habits, which helps them make better business decisions. This demand has led to a growing need for data science professionals, prompting universities to invest in programs such as bachelor\u2019s or master\u2019s degrees in data science. \u00a0 Data Science Concept #1: Machine Learning Machine Learning is a branch of Artificial Intelligence that involves programming a system to automatically perform specific tasks. The system then self-learns from data, performs pattern recognition, and makes decisions with little to no human intervention. In Data Science, Machine Learning is used to build predictive models. As a system is exposed to new data, the machine learning algorithm can independently process and adapt it to predict outcomes more accurately. These predictions are based not only on new data but also on all previous computations. Machine Learning is crucial for managing and working with Big Data. Data Science Concept #2: Algorithms Algorithms are specific sets of rules or processes used in calculations to solve problems or perform tasks. The simplest example of an algorithm is a recipe\u2014a set of instructions to achieve a particular outcome. In Data Science, many data models and analyses are accomplished using algorithms. These can either be automated to self-learn, as in the case of Machine Learning, or applied as simple macros in Excel to generate results based on the provided data. Data Science Concept #3: Statistical Models Statistical Models are mathematical representations that specify relationships between random and non-random variables. They analyse datasets by mathematically representing observed data to make inferences from samples. This concept is essential in Data Science. Models can be used to extract information or predict probable outcomes based on available data. Statistical models can be considered statistical assumptions, allowing Data Scientists to calculate the probability of an event occurring. A simple example is predicting the probable outcome of a dice roll. Data Science Concept #4: Regression Analysis Regression analysis is a statistical process that estimates the relationships between a dependent variable and one or more independent variables, providing a real number value representing a quantity on a line, such as temperature or sales turnover. In Data Science, regression analysis is used for statistical modeling to find trends in data and predict or forecast specific behaviors, such as forecasting monthly sales trends for the year based on past and current data. Regression analysis significantly overlaps with the Machine Learning field. Data Science Concept #5: Programming Computer programming languages are used to develop and build models for data analysis. Additionally, programming can clean, organise, and visualise data in understandable formats for stakeholders. Commonly used programming languages in Data Science include Python, R for statistics, and SQL for database management and creation. However, business analysts or individuals who specialise only in data interpretation and analysis do not often study these languages. What is Data Science Used For? The overall aim of data science is to help businesses and industries see the bigger picture and make better decisions. This is achieved through different analytics types, mainly descriptive analytics, diagnostic analytics, predictive analytics, and prescriptive analytics. Difference Between Data Scientist, Data Analyst, and Data Engineer With a degree such as a master\u2019s in data science, various career options become available. Here\u2019s how each profession describes their roles: As a data scientist, I delve into an organisation\u2019s data to extract and convey meaningful insights. My deep understanding of machine learning workflows and their application to real-world business scenarios guides me as I predominantly work with coding tools, conduct in-depth analyses, and frequently engage with big data tools. In my role as a Data Analyst, I interpret an organisation\u2019s data, transforming intricate datasets into actionable insights that drive business decisions. My expertise in mathematical and statistical analysis, combined with data visualisation tools, allows me to effectively communicate findings to both technical and non-technical stakeholders. As a Data Engineer, I serve as an architect, designing, constructing, and managing data infrastructure to facilitate efficient data analysis for Data Scientists. My focus spans data collection, storage, and processing, where I establish data pipelines that streamline the analytical process. Stafford offers Postgraduate\u00a0Online Data Science courses. Contact a\u00a0Higher Education Consultant\u00a0for a personal evaluation. Available Data Science Degrees include: Share: You may Also Like Jobs after completing an online Masters in Data Science  info@staffordglobal.org  \u00a9 2024 Stafford Global. All rights reserved. Degrees Explore Payment MBA Degree Executive MBA Online MBA Global Online MBA MBA Business Analytics View all MBA degrees Education Degree PGCEi Master of Education Early Childhood Education MSc Inclusive Education View all Education degrees Popular Courses BSc Nursing MSc Engg. Management MSc Artificial Intelligence DBA View all Courses Admissions Course Index Universities Refer a Friend About Stafford Meet The Team Stafford History Graduation Gallery Partner With Us Learn Events & Webinar Student Testimonials Blog Online Learning Why Study Online Online Learning FAQ Admissions Course Index Universities Refer a Friend"}
{"url": "https://www.ibm.com/topics/big-data-analytics", "title": "What is Big Data Analytics? | IBM", "content": "Published: 5 April 2024\nContributors: Tim Mucci, Cole Stryker\n Big data analytics refers to the systematic processing and analysis of large amounts of data and complex data sets, known as big data, to extract valuable insights. Big data analytics allows for the uncovering of trends, patterns and correlations in large amounts of raw data to help analysts make data-informed decisions. This process allows organizations to leverage the exponentially growing data generated from diverse sources, including internet-of-things (IoT) sensors, social media, financial transactions and smart devices to derive actionable intelligence through advanced analytic techniques. In the early 2000s, advances in software and hardware capabilities made it possible for organizations to collect and handle large amounts of unstructured data. With this explosion of useful data, open-source communities developed big data frameworks to store and process this data. These frameworks are used for distributed storage and processing of large data sets across a network of computers. Along with additional tools and libraries, big data frameworks can be used for: Four main data analysis methods\u00a0 \u2013 descriptive, diagnostic, predictive and prescriptive\u00a0 \u2013 are used to uncover insights and patterns within an organization's data. These methods facilitate a deeper understanding of market trends, customer preferences and other important business metrics. IBM named a Leader in the 2024 Gartner\u00ae Magic Quadrant\u2122 for Augmented Data Quality Solutions. Structured vs unstructured data What is data management? The main difference between big data analytics and traditional data analytics is the type of data handled and the tools used to analyze it. Traditional analytics deals with structured data, typically stored in relational databases. This type of database helps ensure that data is well-organized and easy for a computer to understand. Traditional data analytics relies on statistical methods and tools like structured query language (SQL) for querying databases. Big data analytics involves massive amounts of data in various formats, including structured, semi-structured and unstructured data. The complexity of this data requires more sophisticated analysis techniques. Big data analytics employs advanced techniques like machine learning and data mining to extract information from complex data sets. It often requires distributed processing systems like Hadoop to manage the sheer volume of data. These are the four methods of data analysis at work within big data:\n The \"what happened\" stage of data analysis. Here, the focus is on summarizing and describing past data to understand its basic characteristics. The \u201cwhy it happened\u201d stage. By delving deep into the data, diagnostic analysis identifies the root patterns and trends observed in descriptive analytics. The \u201cwhat will happen\u201d stage. It uses historical data, statistical modeling and machine learning to forecast trends. Describes the \u201cwhat to do\u201d stage, which goes beyond prediction to provide recommendations for optimizing future actions based on insights derived from all previous. The following dimensions highlight the core challenges and opportunities inherent in big data analytics.\n The sheer volume of data generated today, from social media feeds, IoT devices, transaction records and more, presents a significant challenge. Traditional data storage and processing solutions are often inadequate to handle this scale efficiently. Big data technologies and cloud-based storage solutions enable organizations to store and manage these vast data sets cost-effectively, protecting valuable data from being discarded due to storage limitations.\n Data is being produced at unprecedented speeds, from real-time social media updates to high-frequency stock trading records. The velocity at which data flows into organizations requires robust processing capabilities to capture, process and deliver accurate analysis in near real-time. Stream processing frameworks and in-memory data processing are designed to handle these rapid data streams and balance supply with demand. Today's data comes in many formats, from structured to numeric data in traditional databases to unstructured text, video and images from diverse sources like social media and video surveillance. This variety demans flexible data management systems to handle and integrate disparate data types for comprehensive analysis. NoSQL databases, data lakes and schema-on-read technologies provide the necessary flexibility to accommodate the diverse nature of big data.\n Data reliability and accuracy are critical, as decisions based on inaccurate or incomplete data can lead to negative outcomes. Veracity refers to the data's trustworthiness, encompassing data quality, noise and anomaly detection issues. Techniques and tools for data cleaning, validation and verification are integral to ensuring the integrity of big data, enabling organizations to make better decisions based on reliable information. Big data analytics aims to extract actionable insights that offer tangible value. This involves turning vast data sets into meaningful information that can inform strategic decisions, uncover new opportunities and drive innovation. Advanced analytics, machine learning and AI are key to unlocking the value contained within big data, transforming raw data into strategic assets. Data professionals, analysts, scientists and statisticians prepare and process data in a data lakehouse, which combines the performance of a data warehouse with the flexibility of a data lake to clean data and ensure its quality. The process of turning raw data into valuable insights encompasses several key stages: Under the Analyze umbrella, there are potentially many technologies at work, including data mining, which is used to identify patterns and relationships within large data sets; predictive analytics, which forecasts future trends and opportunities; and deep learning, which mimics human learning patterns to uncover more abstract ideas. Deep learning uses an artificial neural network with multiple layers to model complex patterns in data. Unlike traditional machine learning algorithms, deep learning learns from images, sound and text without manual help. For big data analytics, this powerful capability means the volume and complexity of data is not an issue. Natural language processing (NLP) models allow machines to understand, interpret and generate human language. Within big data analytics, NLP extracts insights from massive unstructured text data generated across an organization and beyond.\n Structured Data Structured data refers to highly organized information that is easily searchable and typically stored in relational databases or spreadsheets. It adheres to a rigid schema, meaning each data element is clearly defined and accessible in a fixed field within a record or file. Examples of structured data include: Structured data's main advantage is its simplicity for entry, search and analysis, often using straightforward database queries like SQL. However, the rapidly expanding universe of big data means that structured data represents a relatively small portion of the total data available to organizations. Unstructured Data Unstructured data lacks a pre-defined data model, making it more difficult to collect, process and analyze. It comprises the majority of data generated today, and includes formats such as: The primary challenge with unstructured data is its complexity and lack of uniformity, requiring more sophisticated methods for indexing, searching and analyzing. NLP, machine learning and advanced analytics platforms are often employed to extract meaningful insights from unstructured data. Semi-structured data Semi-structured data occupies the middle ground between structured and unstructured data. While it does not reside in a relational database, it contains tags or other markers to separate semantic elements and enforce hierarchies of records and fields within the data. Examples include: Semi-structured data is more flexible than structured data but easier to analyze than unstructured data, providing a balance that is particularly useful in web applications and data integration tasks. Ensuring data quality and integrity, integrating disparate data sources, protecting data privacy and security and finding the right talent to analyze and interpret data can present challenges to organizations looking to leverage their extensive data volumes. What follows are the benefits organizations can realize once they see success with big data analytics: Real-time intelligence One of the standout advantages of big data analytics is the capacity to provide real-time intelligence. Organizations can analyze vast amounts of data as it is generated from myriad sources and in various formats. Real-time insight allows businesses to make quick decisions, respond to market changes instantaneously and identify and act on opportunities as they arise. Better-informed decisions With big data analytics, organizations can uncover previously hidden trends, patterns and correlations. A deeper understanding equips leaders and decision-makers with the information needed to strategize effectively, enhancing business decision-making in supply chain management, e-commerce, operations and overall strategic direction. \u00a0 Cost savings Big data analytics drives cost savings by identifying business process efficiencies and optimizations. Organizations can pinpoint wasteful expenditures by analyzing large datasets, streamlining operations and enhancing productivity. Moreover, predictive analytics can forecast future trends, allowing companies to allocate resources more efficiently and avoid costly missteps. Better customer engagement Understanding customer needs, behaviors and sentiments is crucial for successful engagement and big data analytics provides the tools to achieve this understanding. Companies gain insights into consumer preferences and tailor their marketing strategies by analyzing customer data. Optimized risk management strategies Big data analytics enhances an organization's ability to manage risk by providing the tools to identify, assess and address threats in real time. Predictive analytics can foresee potential dangers before they materialize, allowing companies to devise preemptive strategies. As organizations across industries seek to leverage data to drive decision-making, improve operational efficiencies and enhance customer experiences, the demand for skilled professionals in big data analytics has surged. Here are some prominent career paths that utilize big data analytics: Data scientist Data scientists analyze complex digital data to assist businesses in making decisions. Using their data science training and advanced analytics technologies, including machine learning and predictive modeling, they uncover hidden insights in data. Data analyst Data analysts turn data into information and information into insights. They use statistical techniques to analyze and extract meaningful trends from data sets, often to inform business strategy and decisions. Data engineer Data engineers prepare, process and manage big data infrastructure and tools. They also develop, maintain, test and evaluate data solutions within organizations, often working with massive datasets to assist in analytics projects. Machine learning engineer Machine learning engineers focus on designing and implementing machine learning applications. They develop sophisticated algorithms that learn from and make predictions on data. Business intelligence analyst Business intelligence (BI) analysts help businesses make data-driven decisions by analyzing data to produce actionable insights. They often use BI tools to convert data into easy-to-understand reports and visualizations for business stakeholders. Data visualization specialist These specialists focus on the visual representation of data. They create data visualizations that help end users understand the significance of data by placing it in a visual context. Data architect Data architects design, create, deploy and manage an organization's data architecture. They define how data is stored, consumed, integrated and managed by different data entities and IT systems. IBM and Cloudera have partnered to create an industry-leading, enterprise-grade big data framework distribution plus a variety of cloud services and products \u2014 all designed to achieve faster analytics at scale. IBM Db2 Database on IBM Cloud Pak for Data combines a proven, AI-infused, enterprise-ready data management system with an integrated data and AI platform built on the security-rich, scalable Red Hat OpenShift foundation. IBM Big Replicate is an enterprise-class data replication software platform that keeps data consistent in a distributed environment, on-premises and in the hybrid cloud, including SQL and NoSQL databases. A data warehouse is a system that aggregates data from different sources into a single, central, consistent data store to support data analysis, data mining, artificial intelligence and machine learning. Business intelligence gives organizations the ability to get answers they can understand. Instead of using best guesses, they can base decisions on what their business data is telling them \u2014 whether it relates to production, supply chain, customers or market trends. Cloud computing is the on-demand access of physical or virtual servers, data storage, networking capabilities, application development tools, software, AI analytic tools and more\u2014over the internet with pay-per-use pricing. The cloud computing model offers customers flexibility and scalability compared to traditional infrastructure. Purpose-built data-driven architecture helps support business intelligence across the organization. IBM analytics solutions allow organizations to simplify raw data access, provide end-to-end data management and empower business users with AI-driven self-service analytics to predict outcomes. \u00a0"}
{"url": "https://www.coursera.org/articles/big-data-analytics", "title": "What Is Big Data Analytics? Definition, Benefits, and More | Coursera", "content": "What Is Big Data Analytics? Definition, Benefits, and More Big data analytics is behind some of the most significant industry advancements in the world today, including in health care, government, and finance. Learn more about working with big data and common tools to get started. Big data analytics uses advanced analytics on large collections of both structured and unstructured data to produce valuable insights for businesses. It is used widely across industries such as health care, education, insurance, artificial intelligence, retail, and manufacturing to understand what\u2019s working and what\u2019s not, and to improve processes, systems, and profitability.\u00a0 In this guide, you'll learn more about what big data analytics is, why it's important, and some common benefits. You'll also learn about types of analysis used in big data analytics, find a list of common tools used to perform it, and find suggested courses that can help you get started on your own data analytics professional journey. Start building your skills today Develop and strengthen your data analysis skills with Google's Data Analytics Professional Certificate. You'll be equipped with an immersive understanding of the practices and processes any associate data analyst needs on a day-to-day basis, including organizing data for analysis, completing analysis with spreadsheets and R programming, and presenting findings. What is big data analytics? Big data analytics is the process of collecting, examining, and analyzing large amounts of data to discover market trends, insights, and patterns that can help companies make better business decisions. This information is available quickly and efficiently so that companies can be agile in crafting plans to maintain their competitive advantage. Technologies such as business intelligence (BI) tools and systems help organizations take the unstructured and structured data from multiple sources. Users (typically employees) input queries into these tools to understand business operations and performance. Big data analytics uses the four data analysis methods to uncover meaningful insights and derive solutions. So, what makes data \u201cbig\u201d?   Big data is characterized by the five V's: volume, velocity, variety, variability, and value [1]. It\u2019s complex, so making sense of all of the data in the business requires both innovative technologies and analytical skills. Read more: What Is Big Data? A Layperson's Guide Example of big data analytics For example, big data analytics is integral to the modern health care industry. As you can imagine, thousands of patient records, insurance plans, prescriptions, and vaccine information need to be managed. It comprises huge amounts of structured and unstructured data, which can offer important insights when analytics are applied. Big data analytics does this quickly and efficiently so that health care providers can use the information to make informed, life-saving diagnoses.\u00a0 Why is big data analytics important?\u00a0 Data will become increasingly integral and transformative to day-to-day business operations, according to McKinsey & Company [2]. Big data analytics is important because it helps companies leverage their data to identify opportunities for improvement and optimization. Across different business segments, increasing efficiency leads to overall more intelligent operations, higher profits, and satisfied customers. Big data analytics helps companies reduce costs and develop better, customer-centric products and services. Data analytics helps provide insights that improve the way our society functions. In health care, big data analytics not only keeps track of and analyzes individual records, but plays a critical role in measuring public health outcomes on a global scale. It informs health ministries within each nation\u2019s government on how to proceed with public and population health policy and devises solutions for mitigating future society-wide health problems.\u00a0 Benefits of big data analytics There are quite a few advantages to incorporating big data analytics into a business or organization. These include:    Cost reduction: Big data can reduce costs in storing all of a business's data in one place. Tracking analytics also helps companies find ways to work more efficiently to cut costs wherever possible.\u00a0  Product development: Developing and marketing new products, services, or brands is much easier when based on data collected from customers\u2019 needs and wants. Big data analytics also helps businesses understand product viability and keep up with trends.  Strategic business decisions: The ability to constantly analyze data helps businesses make better and faster decisions, such as cost and supply chain optimization.  Customer experience: Data-driven algorithms help marketing efforts (targeted ads, as an example) and increase customer satisfaction by delivering an enhanced customer experience.  Risk management: Businesses can identify risks by analyzing data patterns and developing solutions for managing those risks.   Entertainment: Providing a personalized recommendation of movies and music according to a customer\u2019s individual preferences has been transformative for the entertainment industry (think Spotify and Netflix).  Education: Big data helps schools and educational technology companies alike develop new curriculums while improving existing plans based on needs and demands.  Health care: Monitoring patients\u2019 medical histories helps doctors detect and prevent diseases.  Government: Big data can be used to collect data from CCTV and traffic cameras, satellites, body cameras and sensors, emails, calls, and more, to help manage the public sector.  Marketing: Customer information and preferences can be used to create targeted advertising campaigns with a high return on investment (ROI).\u00a0  Banking: Data analytics can help track and monitor illegal money laundering.  Types of big data analytics (+ examples) There are four main types of big data analytics that support and inform different business decisions. 1. Descriptive analytics Descriptive analytics refers to data that can be easily read and interpreted. This data helps create reports and visualize information that can detail company profits and sales.\u00a0 Example: During the Covid-19 pandemic, a leading pharmaceuticals company conducted data analysis on its offices and research labs. Descriptive analytics helped them identify unutilized spaces and departments that were consolidated, saving the company millions of dollars. 2. Diagnostics analytics Diagnostics analytics helps companies understand why a problem occurred. Big data technologies and tools allow users to mine and recover data that helps dissect an issue and prevent it from happening in the future. \nExample: A clothing company\u2019s sales have decreased even though customers continue to add items to their shopping carts. Diagnostics analytics helped to understand that the payment page was not working properly for a few weeks. 3. Predictive analytics Predictive analytics looks at past and present data to make predictions. With artificial intelligence (AI), machine learning, and data mining, users can analyze data to predict market trends. \nExample: In the manufacturing sector, companies can use machine learning models trained on historical data to predict if or when a piece of equipment will malfunction or break down. 4. Prescriptive analytics Prescriptive analytics provides a solution to a problem, relying on AI and machine learning to gather data and use it for risk management.\u00a0 \nExample: Within the energy sector, utility companies, gas producers, and pipeline owners identify factors that affect the price of oil and gas in order to hedge risks. Leverage generative AI in your data science workflows with the Microsoft Copilot for Data Science Specialization. Copilot supercharges your data science workflow, automating tasks and generating code, so you can focus on the big picture. Big data analytics tools Harnessing all of that data requires tools. Thankfully, technology has advanced so that there are many intuitive software systems available for data analysts to use. \n Hadoop: An open-source framework that stores and processes big data sets. Hadoop is able to handle and analyze structured and unstructured data.\u00a0  Spark: An open-source cluster computing framework used for real-time processing and analyzing data.  Data integration software: Programs that allow big data to be streamlined across different platforms, such as MongoDB, Apache, Hadoop, and Amazon EMR.  Stream analytics tools: Systems that filter, aggregate, and analyze data that might be stored in different platforms and formats, such as Kafka.  Distributed storage: Databases that can split data across multiple servers and have the capability to identify lost or corrupt data, such as Cassandra.  Predictive analytics hardware and software: Systems that process large amounts of complex data, using machine learning algorithms to predict future outcomes, such as fraud detection, marketing, and risk assessments.  Data mining tools: Programs that allow users to search within structured and unstructured big data.  NoSQL databases: Non-relational data management systems ideal for dealing with raw and unstructured data.  Data warehouses: Storage for large amounts of data collected from many different sources, typically using predefined schemas.  Familiarize yourself with big data analytics tools Knowing how to use industry-standard tools like the ones mentioned above is essential. Whether you're pursuing a promotion or interested in obtaining a more advanced role, you can gain experience in commonly used tools online. Here are a few relevant options to consider:  Introduction to Big Data with Spark and Hadoop Google Cloud Big Data and Machine Learning Specialization Data Engineering, Big Data, and Machine Learning on GCP Specialization PostgreSQL for Everybody Specialization Microsoft Copilot for Data Science  Start advancing your data analytics skills today If you want to expand your big data analytics skill set to meet your career goals, you have options. For example, you can learn from an industry leader while earning a credential for your resume with IBM's Introduction to Data Analytics.  And, in just six months or less, you can learn in-demand, job-ready skills like data cleaning, analysis, and visualization with the Google Data Analytics Professional Certificate. You'll also gain hands-on experience with spreadsheets, SQL programming, and Tableau.               \n Article sources IBM. \u201cThe 5 V\u2019s of big data, https://www.ibm.com/blogs/watson-health/the-5-vs-of-big-data/.\u201d Accessed March 28, 2024. McKinsey & Company. \u201cThe data-driven enterprise of 2025, https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-data-driven-enterprise-of-2025.\" Accessed March 28, 2024.  Keep reading             Coursera Staff Editorial Team Coursera\u2019s editorial team is comprised of highly experienced professional editors, writers, and fact... This content has been made available for informational purposes only. Learners are advised to conduct additional research to ensure that courses and other credentials pursued meet their personal, professional, and financial goals.   Coursera Footer Get Started with AI Popular Career Certificates Popular Subjects Popular Resources Coursera Community More"}
{"url": "https://www.techtarget.com/searchbusinessanalytics/definition/big-data-analytics", "title": "What is big data analytics? | Definition from TechTarget", "content": "big data analytics  What is big data analytics? Big data analytics is the often complex process of examining big data to uncover information -- such as hidden patterns, correlations, market trends and customer preferences -- that can help organizations make informed business decisions. On a broad scale, data analytics technologies and techniques give organizations a way to analyze data sets and gather new information. Business intelligence (BI) queries answer basic questions about business operations and performance. Big data analytics is a form of advanced analytics, which involve complex applications with elements such as predictive models, statistical algorithms and what-if analysis powered by analytics systems. An example of big data analytics can be found in the healthcare industry, where millions of patient records, medical claims, clinical results, care management records and other data must be collected, aggregated, processed and analyzed. Big data analytics is used for accounting, decision-making, predictive analytics and many other purposes. This data varies greatly in type, quality and accessibility, presenting significant challenges but also offering tremendous benefits. This article is part of The ultimate guide to big data for businesses Download this entire guide for FREE now! Why is big data analytics important? Organizations can use big data analytics systems and software to make data-driven decisions that can improve their business-related outcomes. The benefits can include more effective marketing, new revenue opportunities, customer personalization and improved operational efficiency. With an effective strategy, these benefits can provide competitive advantages over competitors. How does big data analytics work? Data analysts, data scientists, predictive modelers, statisticians and other analytics professionals collect, process, clean and analyze growing volumes of structured transaction data, as well as other forms of data not used by conventional BI and analytics programs. The following is an overview of the four steps of the big data analytics process: Types of big data analytics There are several different types of big data analytics, each with their own application within the enterprise. Key big data analytics technologies and tools Many different types of tools and technologies are used to support big data analytics processes, including the following: Big data analytics applications often include data from both internal systems and external sources, such as weather data or demographic data on consumers compiled by third-party information services providers. In addition, streaming analytics applications are becoming more common in big data environments as users look to perform real-time analytics on data fed into Hadoop systems through stream processing engines, such as Spark, Flink and Storm. Early big data systems were mostly deployed on premises, particularly in large organizations that collected, organized and analyzed massive amounts of data. But cloud platform vendors, such as Amazon Web Services (AWS), Google and Microsoft, have made it easier to set up and manage Hadoop clusters in the cloud. The same goes for Hadoop suppliers such as Cloudera, which support the distribution of the big data framework on AWS, Google and Microsoft Azure clouds. Users can spin up clusters in the cloud, run them for as long as they need and then take them offline with usage-based pricing that doesn't require ongoing software licenses. Big data has become increasingly beneficial in supply chain analytics. Big supply chain analytics uses big data and quantitative methods to enhance decision-making processes across the supply chain. Specifically, big supply chain analytics expands data sets for increased analysis that goes beyond the traditional internal data found on enterprise resource planning and supply chain management systems. Also, big supply chain analytics implements highly effective statistical methods on new and existing data sources. Big data analytics uses and examples The following are some examples of how big data analytics can be used to help organizations: Big data analytics benefits The benefits of using big data analytics include the following: Big data analytics challenges Despite the wide-reaching benefits that come with using big data analytics, its use also comes with the following challenges: History and growth of big data analytics The term big data was first used to refer to increasing data volumes in the mid-1990s. In 2001, Doug Laney, then an analyst at consultancy Meta Group Inc., expanded the definition of big data. This expansion described the increase of the following: Those three factors became known as the 3V's of big data. Gartner popularized this concept in 2005 after acquiring Meta Group and hiring Laney. Over time, the 3V's became the 5V's by adding value and veracity and sometimes a sixth V for variability. Another significant development in the history of big data was the launch of the Hadoop distributed processing framework. Hadoop was launched in 2006 as an Apache open source project. This planted the seeds for a clustered platform built on top of commodity hardware that could run big data applications. The Hadoop framework of software tools is widely used for managing big data. By 2011, big data analytics began to take a firm hold in organizations and the public eye, along with Hadoop and various related big data technologies. Initially, as the Hadoop ecosystem took shape and started to mature, big data applications were primarily used by large internet and e-commerce companies such as Yahoo, Google and Facebook, as well as analytics and marketing services providers. More recently, a broader variety of users have embraced big data analytics as a key technology driving digital transformation. Users include retailers, financial services firms, insurers, healthcare organizations, manufacturers, energy companies and other enterprises. High-quality decision-making using data analysis can help contribute to a high-performance organization. Learn which roles and responsibilities are important to a data management team.  \n\r\n\t\t\tContinue Reading About big data analytics\r\n\t\t \n\t\t\t\tRelated Terms \nDig Deeper on Data science and analytics Hadoop as a service (HaaS) Hadoop Cloudera embraces Apache Iceberg as cloud data lake evolves Compare Hadoop vs. Spark vs. Kafka for your big data strategy Factors including a combination of commitment to data quality, proper technology and pertinent processes are key to preparing ... PowerSchool is creating personalized educational experiences for teachers, students, parents, administrators and others by using ... The alliance aims to make it easier and faster for the data cloud vendor's customers to use the Claude line of large language ... Compare Datadog vs. New Relic capabilities including alerts, log management, incident management and more. Learn which tool is ... Many organizations struggle to manage their vast collection of AWS accounts, but Control Tower can help. The service automates ... There are several important variables within the Amazon EKS pricing model. Dig into the numbers to ensure you deploy the service ... Unlike other development platforms, Power Apps lets users build mobile apps through its web-based designer with zero lines of ... Drupal is a highly customizable and scalable CMS, which makes it suitable for large organizations. Yet, inexperienced admins ... OpenText users get a passel of AI in the latest Cloud Editions release -- but also analytics and CCM tools. With its Cerner acquisition, Oracle sets its sights on creating a national, anonymized patient database -- a road filled with ... Oracle plans to acquire Cerner in a deal valued at about $30B. The second-largest EHR vendor in the U.S. could inject new life ... The Supreme Court ruled 6-2 that Java APIs used in Android phones are not subject to American copyright law, ending a ... SAP is injecting more generative AI into its Intelligent Spend Group via its Joule copilot. It also introduced new services for ... At SAP TechEd, the vendor unveiled new functionality for its generative AI copilot, Joule, as well as ABAP capabilities in its ... Faced with an aging on-premises ERP system, U.S. Sugar took time to plan a move to S/4HANA Cloud and settled on Rise with SAP ... All Rights Reserved, \nCopyright 2010 - 2024, TechTarget\n\n\nPrivacy Policy\n\n\n\nCookie Preferences \n\n\n\nCookie Preferences \n\n\n\nDo Not Sell or Share My Personal Information\n"}
{"url": "https://www.jair.org/", "title": "\n\t\tJournal of Artificial Intelligence Research\n\t\t\t\t\t", "content": "\n\n\n\n The Journal of Artificial Intelligence Research (JAIR) is dedicated to the rapid dissemination of important research results to the global artificial intelligence (AI) community. The journal\u2019s scope encompasses all areas of AI, including agents and multi-agent systems, automated reasoning, constraint processing and search, knowledge representation, machine learning, natural language, planning and scheduling, robotics and vision, and uncertainty in AI. \n\t\t\t\t\tCurrent Issue\n\t\t\t\t \n\t\t\t\tVol. 81 (2024)\n\t\t\t \n\n\t\t\t\t\t\tPublished:\n\t\t\t\t\t\n\t\t\t\t\t2024-09-11\n\t\t\t\t \nArticles\n \n\n\t\t\t\tQCDCL vs QBF Resolution: Further Insights\n\t\t\t\t\t\t\t\n \n\t\t\t\t\t741-769\n\t\t\t\t \n\n\t\t\t\tA Scoping Study on AI Affordances in Early Childhood Education: Mapping the Global Landscape, Identifying Research Gaps, and  Charting Future Research Directions\n\t\t\t\t\t\t\t\n \n\t\t\t\t\t701-740\n\t\t\t\t \n\n\t\t\t\tDifferentially Private Neural Tangent Kernels (DP-NTK) for Privacy-Preserving Data Generation\n\t\t\t\t\t\t\t\n \n\t\t\t\t\t683-700\n\t\t\t\t \n\n\t\t\t\tApproximate Counting of Linear Extensions in Practice\n\t\t\t\t\t\t\t\n \n\t\t\t\t\t643-681\n\t\t\t\t \n\n\t\t\t\tTruth-tracking with Non-expert Information Sources\n\t\t\t\t\t\t\t\n \n\t\t\t\t\t619-641\n\t\t\t\t \n\n\t\t\t\tEfficiently Adapt to New Dynamic via Meta-Model\n\t\t\t\t\t\t\t\n \n\t\t\t\t\t579-617\n\t\t\t\t \n\n\t\t\t\tA Unified Perspective on Value Backup and Exploration in Monte-Carlo Tree Search\n\t\t\t\t\t\t\t\n \n\t\t\t\t\t511-577\n\t\t\t\t \n\n\t\t\t\tTowards Robust Offline-to-Online Reinforcement Learning via Uncertainty and Smoothness\n\t\t\t\t\t\t\t\n \n\t\t\t\t\t481-509\n\t\t\t\t \n\n\t\t\t\tExpected 1.x Makespan-Optimal Multi-Agent Path Finding on Grid Graphs in Low Polynomial Time\n\t\t\t\t\t\t\t\n \n\t\t\t\t\t443-479\n\t\t\t\t \n\n\t\t\t\tA Fortiori Case-Based Reasoning: From Theory to Data\n\t\t\t\t\t\t\t\n \n\t\t\t\t\t401-441\n\t\t\t\t \n\n\t\t\t\tInverting Cryptographic Hash Functions via Cube-and-Conquer\n\t\t\t\t\t\t\t\n \n\t\t\t\t\t359-399\n\t\t\t\t \n\n\t\t\t\tEfficient and Fair Healthcare Rationing\n\t\t\t\t\t\t\t\n \n\t\t\t\t\t337-358\n\t\t\t\t \n\n\t\t\t\tUncertainty as a Fairness Measure\n\t\t\t\t\t\t\t\n \n\t\t\t\t\t307-335\n\t\t\t\t \n\n\t\t\t\tThe Human in Interactive Machine Learning: Analysis and Perspectives for Ambient Intelligence\n\t\t\t\t\t\t\t\n \n\t\t\t\t\t263-305\n\t\t\t\t \n\n\t\t\t\tThe Effect of Preferences in Abstract Argumentation under a Claim-Centric View\n\t\t\t\t\t\t\t\n \n\t\t\t\t\t203-262\n\t\t\t\t \n\n\t\t\t\tDigraph k-Coloring Games: New Algorithms and Experiments\n\t\t\t\t\t\t\t\n \n\t\t\t\t\t163-202\n\t\t\t\t \n\n\t\t\t\tOpening the Analogical Portal to Explainability: Can Analogies Help Laypeople in AI-assisted Decision Making?\n\t\t\t\t\t\t\t\n \n\t\t\t\t\t117-162\n\t\t\t\t \n\n\t\t\t\tSeparating and Collapsing Electoral Control Types\n\t\t\t\t\t\t\t\n \n\t\t\t\t\t71-116\n\t\t\t\t \n\n\t\t\t\tThe State of Computer Vision Research in Africa\n\t\t\t\t\t\t\t\n \n\t\t\t\t\t43-69\n\t\t\t\t \n\n\t\t\t\tUnderstanding What Affects the Generalization Gap in Visual Reinforcement Learning: Theory and Empirical Evidence\n\t\t\t\t\t\t\t\n \n\t\t\t\t\t1-42\n\t\t\t\t news 2024 IJCAI-JAIR Prize Awarded Special Track on Multi-Agent Path Finding JAIR Now Available in ACM Library Help Support JAIR submission JAIR invites submissions in all areas of AI. Articles published in JAIR must meet the highest quality standards as measured by originality and significance of the contribution. Submit an Article afiliatedsites  JAIR is published by\u00a0AI Access Foundation,\u00a0a nonprofit public charity whose purpose is to facilitate the dissemination of scientific results in artificial intelligence. JAIR, established in 1993, was\u00a0one of the first open-access scientific journals on the Web, and has been a leading publication venue since its inception.\u00a0We invite you to check out our other initiatives. Learn more \n\t\tMake a Submission\n\t FAQ\u00a0 \u00a0|\u00a0\u00a0\u00a0Mailing List\u00a0 |\u00a0\u00a0\u00a0Donate to JAIR\u00a0 |\u00a0 \u00a0Follow us on Twitter\u00a9 Copyright 1993-2023 AI Access Foundation, Inc."}
{"url": "https://www.qualcomm.com/research/artificial-intelligence/ai-research/papers", "title": "AI Research Papers | Generative Modeling & More | Qualcomm", "content": ""}
{"url": "https://en.wikipedia.org/wiki/Supervised_learning", "title": "Supervised learning - Wikipedia", "content": "Contents Supervised learning Supervised learning (SL) is a paradigm in machine learning where input objects (for example, a vector of predictor variables) and a desired output value (also known as a human-labeled supervisory signal) train a model. The training data is processed, building a function that maps new data to expected output values.[1] An optimal scenario will allow for the algorithm to correctly determine output values for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a \"reasonable\" way (see inductive bias). This statistical quality of an algorithm is measured through the so-called generalization error.\n Steps to follow To solve a given problem of supervised learning, one has to perform the following steps:\n Algorithm choice A wide range of supervised learning algorithms are available, each with its strengths and weaknesses. There is no single learning algorithm that works best on all supervised learning problems (see the No free lunch theorem).\n There are four major issues to consider in supervised learning:\n Bias-variance tradeoff A first issue is the tradeoff between bias and variance.[2] Imagine that we have available several different, but equally good, training data sets. A learning algorithm is biased for a particular input \n\n\n\nx\n\n\n{\\displaystyle x}\n\n if, when trained on each of these data sets, it is systematically incorrect when predicting the correct output for \n\n\n\nx\n\n\n{\\displaystyle x}\n\n. A learning algorithm has high variance for a particular input \n\n\n\nx\n\n\n{\\displaystyle x}\n\n if it predicts different output values when trained on different training sets. The prediction error of a learned classifier is related to the sum of the bias and the variance of the learning algorithm.[3] Generally, there is a tradeoff between bias and variance. A learning algorithm with low bias must be \"flexible\" so that it can fit the data well. But if the learning algorithm is too flexible, it will fit each training data set differently, and hence have high variance. A key aspect of many supervised learning methods is that they are able to adjust this tradeoff between bias and variance (either automatically or by providing a bias/variance parameter that the user can adjust).\n Function complexity and amount of training data The second issue is of the amount of training data available relative to the complexity of the \"true\" function (classifier or regression function). If the true function is simple, then an \"inflexible\" learning algorithm with high bias and low variance will be able to learn it from a small amount of data. But if the true function is highly complex (e.g., because it involves complex interactions among many different input features and behaves differently in different parts of the input space), then the function will only be able to learn with a large amount of training data paired with a \"flexible\" learning algorithm with low bias and high variance.\n Dimensionality of the input space A third issue is the dimensionality of the input space. If the input feature vectors have large dimensions, learning the function can be difficult even if the true function only depends on a small number of those features. This is because the many \"extra\" dimensions can confuse the learning algorithm and cause it to have high variance. Hence, input data of large dimensions typically requires tuning the classifier to have low variance and high bias. In practice, if the engineer can manually remove irrelevant features from the input data, it will likely improve the accuracy of the learned function. In addition, there are many algorithms for feature selection that seek to identify the relevant features and discard the irrelevant ones. This is an instance of the more general strategy of dimensionality reduction, which seeks to map the input data into a lower-dimensional space prior to running the supervised learning algorithm.\n Noise in the output values A fourth issue is the degree of noise in the desired output values (the supervisory target variables). If the desired output values are often incorrect (because of human error or sensor errors), then the learning algorithm should not attempt to find a function that exactly matches the training examples. Attempting to fit the data too carefully leads to overfitting. You can overfit even when there are no measurement errors (stochastic noise) if the function you are trying to learn is too complex for your learning model. In such a situation, the part of the target function that cannot be modeled \"corrupts\" your training data - this phenomenon has been called deterministic noise. When either type of noise is present, it is better to go with a higher bias, lower variance estimator.\n In practice, there are several approaches to alleviate noise in the output values such as early stopping to prevent overfitting as well as detecting and removing the noisy training examples prior to training the supervised learning algorithm. There are several algorithms that identify noisy training examples and removing the suspected noisy training examples prior to training has decreased generalization error with statistical significance.[4][5]\n Other factors to consider Other factors to consider when choosing and applying a learning algorithm include the following:\n When considering a new application, the engineer can compare multiple learning algorithms and experimentally determine which one works best on the problem at hand (see  cross-validation). Tuning the performance of a learning algorithm can be very time-consuming. Given fixed resources, it is often better to spend more time collecting additional training data and more informative features than it is to spend extra time tuning the learning algorithms.\n Algorithms The most widely used learning algorithms are: \n How supervised learning algorithms work Given a set of \n\n\n\nN\n\n\n{\\displaystyle N}\n\n training examples of the form \n\n\n\n{\n(\n\nx\n\n1\n\n\n,\n\ny\n\n1\n\n\n)\n,\n.\n.\n.\n,\n(\n\nx\n\nN\n\n\n,\n\n\ny\n\nN\n\n\n)\n}\n\n\n{\\displaystyle \\{(x_{1},y_{1}),...,(x_{N},\\;y_{N})\\}}\n\n such that \n\n\n\n\nx\n\ni\n\n\n\n\n{\\displaystyle x_{i}}\n\n is the feature vector of the \n\n\n\ni\n\n\n{\\displaystyle i}\n\n-th example and \n\n\n\n\ny\n\ni\n\n\n\n\n{\\displaystyle y_{i}}\n\n is its label (i.e., class), a learning algorithm seeks a function \n\n\n\ng\n:\nX\n\u2192\nY\n\n\n{\\displaystyle g:X\\to Y}\n\n, where \n\n\n\nX\n\n\n{\\displaystyle X}\n\n is the input space and \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n is the output space. The function \n\n\n\ng\n\n\n{\\displaystyle g}\n\n is an element of some space of possible functions \n\n\n\nG\n\n\n{\\displaystyle G}\n\n, usually called the hypothesis space. It is sometimes convenient to represent \n\n\n\ng\n\n\n{\\displaystyle g}\n\n using a scoring function \n\n\n\nf\n:\nX\n\u00d7\nY\n\u2192\n\nR\n\n\n\n{\\displaystyle f:X\\times Y\\to \\mathbb {R} }\n\n such that \n\n\n\ng\n\n\n{\\displaystyle g}\n\n is defined as returning the \n\n\n\ny\n\n\n{\\displaystyle y}\n\n value that gives the highest score: \n\n\n\ng\n(\nx\n)\n=\n\n\n\narg\n\u2061\nmax\n\ny\n\n\n\nf\n(\nx\n,\ny\n)\n\n\n{\\displaystyle g(x)={\\underset {y}{\\arg \\max }}\\;f(x,y)}\n\n. Let \n\n\n\nF\n\n\n{\\displaystyle F}\n\n denote the space of scoring functions.\n Although \n\n\n\nG\n\n\n{\\displaystyle G}\n\n and \n\n\n\nF\n\n\n{\\displaystyle F}\n\n can be any space of functions, many learning algorithms are probabilistic models where \n\n\n\ng\n\n\n{\\displaystyle g}\n\n takes the form of a conditional probability model \n\n\n\ng\n(\nx\n)\n=\n\n\n\narg\n\u2061\nmax\n\ny\n\n\n\nP\n(\ny\n\n|\n\nx\n)\n\n\n{\\displaystyle g(x)={\\underset {y}{\\arg \\max }}\\;P(y|x)}\n\n, or \n\n\n\nf\n\n\n{\\displaystyle f}\n\n takes the form of a joint probability model \n\n\n\nf\n(\nx\n,\ny\n)\n=\nP\n(\nx\n,\ny\n)\n\n\n{\\displaystyle f(x,y)=P(x,y)}\n\n. For example, naive Bayes and linear discriminant analysis are joint probability models, whereas logistic regression is a conditional probability model.\n There are two basic approaches to choosing \n\n\n\nf\n\n\n{\\displaystyle f}\n\n or \n\n\n\ng\n\n\n{\\displaystyle g}\n\n: empirical risk minimization and structural risk minimization.[6] Empirical risk minimization seeks the function that best fits the training data. Structural risk minimization includes a penalty function that controls the bias/variance tradeoff.\n In both cases, it is assumed that the training set consists of a sample of independent and identically distributed pairs, \n\n\n\n(\n\nx\n\ni\n\n\n,\n\n\ny\n\ni\n\n\n)\n\n\n{\\displaystyle (x_{i},\\;y_{i})}\n\n. In order to measure how well a function fits the training data, a loss function \n\n\n\nL\n:\nY\n\u00d7\nY\n\u2192\n\n\nR\n\n\n\u2265\n0\n\n\n\n\n{\\displaystyle L:Y\\times Y\\to \\mathbb {R} ^{\\geq 0}}\n\n is defined. For training example \n\n\n\n(\n\nx\n\ni\n\n\n,\n\n\ny\n\ni\n\n\n)\n\n\n{\\displaystyle (x_{i},\\;y_{i})}\n\n, the loss of predicting the value \n\n\n\n\n\n\ny\n^\n\n\n\n\n\n{\\displaystyle {\\hat {y}}}\n\n is \n\n\n\nL\n(\n\ny\n\ni\n\n\n,\n\n\n\ny\n^\n\n\n\n)\n\n\n{\\displaystyle L(y_{i},{\\hat {y}})}\n\n.\n The risk \n\n\n\nR\n(\ng\n)\n\n\n{\\displaystyle R(g)}\n\n of function \n\n\n\ng\n\n\n{\\displaystyle g}\n\n is defined as the expected loss of \n\n\n\ng\n\n\n{\\displaystyle g}\n\n. This can be estimated from the training data as\n Empirical risk minimization In empirical risk minimization, the supervised learning algorithm seeks the function \n\n\n\ng\n\n\n{\\displaystyle g}\n\n that minimizes \n\n\n\nR\n(\ng\n)\n\n\n{\\displaystyle R(g)}\n\n. Hence, a supervised learning algorithm can be constructed by applying an optimization algorithm to find \n\n\n\ng\n\n\n{\\displaystyle g}\n\n.\n When \n\n\n\ng\n\n\n{\\displaystyle g}\n\n is a conditional probability distribution \n\n\n\nP\n(\ny\n\n|\n\nx\n)\n\n\n{\\displaystyle P(y|x)}\n\n and the loss function is the negative log likelihood: \n\n\n\nL\n(\ny\n,\n\n\n\ny\n^\n\n\n\n)\n=\n\u2212\nlog\n\u2061\nP\n(\ny\n\n|\n\nx\n)\n\n\n{\\displaystyle L(y,{\\hat {y}})=-\\log P(y|x)}\n\n, then empirical risk minimization is equivalent to maximum likelihood estimation.\n When \n\n\n\nG\n\n\n{\\displaystyle G}\n\n contains many candidate functions or the training set is not sufficiently large, empirical risk minimization leads to high variance and poor generalization. The learning algorithm is able to memorize the training examples without generalizing well. This is called overfitting.\n Structural risk minimization Structural risk minimization seeks to prevent overfitting by incorporating a regularization penalty into the optimization. The regularization penalty can be viewed as implementing a form of Occam's razor that prefers simpler functions over more complex ones.\n A wide variety of penalties have been employed that correspond to different definitions of complexity. For example, consider the case where the function \n\n\n\ng\n\n\n{\\displaystyle g}\n\n is a linear function of the form\n A popular regularization penalty is \n\n\n\n\n\u2211\n\nj\n\n\n\n\u03b2\n\nj\n\n\n2\n\n\n\n\n{\\displaystyle \\sum _{j}\\beta _{j}^{2}}\n\n, which is the squared Euclidean norm of the weights, also known as the \n\n\n\n\nL\n\n2\n\n\n\n\n{\\displaystyle L_{2}}\n\n norm. Other norms include the \n\n\n\n\nL\n\n1\n\n\n\n\n{\\displaystyle L_{1}}\n\n norm, \n\n\n\n\n\u2211\n\nj\n\n\n\n|\n\n\n\u03b2\n\nj\n\n\n\n|\n\n\n\n{\\displaystyle \\sum _{j}|\\beta _{j}|}\n\n, and the \n\n\n\n\nL\n\n0\n\n\n\n\n{\\displaystyle L_{0}}\n\n \"norm\", which is the number of non-zero \n\n\n\n\n\u03b2\n\nj\n\n\n\n\n{\\displaystyle \\beta _{j}}\n\ns. The penalty will be denoted by \n\n\n\nC\n(\ng\n)\n\n\n{\\displaystyle C(g)}\n\n.\n The supervised learning optimization problem is to find the function \n\n\n\ng\n\n\n{\\displaystyle g}\n\n that minimizes\n The parameter \n\n\n\n\u03bb\n\n\n{\\displaystyle \\lambda }\n\n controls the bias-variance tradeoff. When \n\n\n\n\u03bb\n=\n0\n\n\n{\\displaystyle \\lambda =0}\n\n, this gives empirical risk minimization with low bias and high variance. When \n\n\n\n\u03bb\n\n\n{\\displaystyle \\lambda }\n\n is large, the learning algorithm will have high bias and low variance. The value of \n\n\n\n\u03bb\n\n\n{\\displaystyle \\lambda }\n\n can be chosen empirically via  cross-validation.\n The complexity penalty has a Bayesian interpretation as the negative log prior probability of \n\n\n\ng\n\n\n{\\displaystyle g}\n\n, \n\n\n\n\u2212\nlog\n\u2061\nP\n(\ng\n)\n\n\n{\\displaystyle -\\log P(g)}\n\n, in which case \n\n\n\nJ\n(\ng\n)\n\n\n{\\displaystyle J(g)}\n\n is the posterior probability of \n\n\n\ng\n\n\n{\\displaystyle g}\n\n.\n Generative training The training methods described above are discriminative training methods, because they seek to find a function \n\n\n\ng\n\n\n{\\displaystyle g}\n\n that discriminates well between the different output values (see discriminative model). For the special case where \n\n\n\nf\n(\nx\n,\ny\n)\n=\nP\n(\nx\n,\ny\n)\n\n\n{\\displaystyle f(x,y)=P(x,y)}\n\n is a joint probability distribution and the loss function is the negative log likelihood \n\n\n\n\u2212\n\n\u2211\n\ni\n\n\nlog\n\u2061\nP\n(\n\nx\n\ni\n\n\n,\n\ny\n\ni\n\n\n)\n,\n\n\n{\\displaystyle -\\sum _{i}\\log P(x_{i},y_{i}),}\n\n a risk minimization algorithm is said to perform generative training, because \n\n\n\nf\n\n\n{\\displaystyle f}\n\n can be regarded as a generative model that explains how the data were generated. Generative training algorithms are often simpler and more computationally efficient than discriminative training algorithms. In some cases, the solution can be computed in closed form as in naive Bayes and linear discriminant analysis.\n Generalizations There are several ways in which the standard supervised learning problem can be generalized:\n Approaches and algorithms Applications General issues See also References External links"}
{"url": "https://www.ibm.com/topics/supervised-learning", "title": "What Is Supervised Learning? | IBM", "content": "Supervised learning, also known as supervised machine learning, is a subcategory of\u00a0machine learning\u00a0and\u00a0artificial intelligence. It is defined by its use of labeled data sets to train algorithms that to classify data or predict outcomes accurately. As input data is fed into the model, it adjusts its weights until the model has been fitted appropriately, which occurs as part of the cross validation process. Supervised learning helps organizations solve for a variety of real-world problems at scale, such as classifying spam in a separate folder from your inbox. It can be used to build highly accurate machine learning models. Learn about barriers to AI adoptions, particularly lack of AI governance and risk management solutions.  Register for the IDC report Supervised learning uses a training set to teach models to yield the desired output. This training dataset includes inputs and correct outputs, which allow the model to learn over time. The algorithm measures its accuracy through the loss function, adjusting until the error has been sufficiently minimized. Supervised learning can be separated into two types of problems when data mining\u2014classification and regression: Various algorithms and computations techniques are used in supervised machine learning processes. Below are brief explanations of some of the most commonly used learning methods, typically calculated through use of programs like R or Python: \u00a0 Unsupervised machine learning\u00a0and supervised machine learning are frequently discussed together. Unlike supervised learning, unsupervised learning uses unlabeled data. From that data, it discovers patterns that help solve for clustering or association problems. This is particularly useful when subject matter experts are unsure of common properties within a data set. Common clustering algorithms are hierarchical, k-means, and Gaussian mixture models. Semi-supervised learning occurs when only part of the given input data has been labeled. Unsupervised and semi-supervised learning can be more appealing alternatives as it can be time-consuming and costly to rely on domain expertise to label data appropriately for supervised learning. For a deep dive into the differences between these approaches, check out \"Supervised vs. Unsupervised Learning: What's the Difference?\" Supervised learning models can be used to build and advance a number of business applications, including the following: Although supervised learning can offer businesses advantages, such as deep data insights and improved automation, there are some challenges when building sustainable supervised learning models. The following are some of these challenges: Build and scale trusted AI on any cloud. Automate the AI lifecycle for ModelOps.  Connect the right data, at the right time, to the right people anywhere.  Hybrid. Open. Resilient. Your platform and partner for digital transformation.  Granite is IBM's flagship series of LLM foundation models based on decoder-only transformer architecture. Granite language models are trained on trusted enterprise data spanning internet, academic, code, legal and finance. Learn the fundamental concepts for AI and generative AI, including prompt engineering, large language models and the best open source projects. In this article, we\u2019ll explore the basics of two data science approaches: supervised and unsupervised.  Explore some supervised learning approaches such as support vector machines and probabilistic classifiers Train, validate, tune and deploy generative AI, foundation models and machine learning capabilities with\u00a0IBM watsonx.ai, a next-generation enterprise studio for AI builders. Build AI applications in a fraction of the time with a fraction of the data."}
{"url": "https://cloud.google.com/discover/what-is-unsupervised-learning", "title": "What is unsupervised learning? | Google Cloud", "content": "What is unsupervised learning? Unsupervised learning in artificial intelligence is a type of machine learning that learns from data without human supervision. Unlike supervised learning, unsupervised machine learning models are given unlabeled data and allowed to discover patterns and insights without any explicit guidance or instruction.\u00a0 Whether you realize it or not, artificial intelligence and machine learning are impacting every aspect of daily life, helping to turn data into insights that can improve efficiencies, reduce costs, and better inform decision-making. Today, businesses are using machine learning algorithms to help power personalized recommendations, real-time translations, or even automatically generate text, images, and other types of content. Here, we\u2019ll cover the basics of unsupervised machine learning, how it works, and some of its common real-life applications. New customers get up to $300 in free credits\u00a0to try Vertex AI and other Google Cloud products.\u00a0 How does unsupervised learning work? As the name suggests, unsupervised learning uses self-learning algorithms\u2014they learn without any labels or prior training. Instead, the model is given raw, unlabeled data and has to infer its own rules and structure the information based on similarities, differences, and patterns without explicit instructions on how to work with each piece of data. Unsupervised learning algorithms are better suited for more complex processing tasks, such as organizing large datasets into clusters. They are useful for identifying previously undetected patterns in data and can help identify features useful for categorizing data.\u00a0 Imagine that you have a large dataset about weather. An unsupervised learning algorithm will go through the data and identify patterns in the data points. For instance, it might group data by temperature or similar weather patterns.\u00a0 While the algorithm itself does not understand these patterns based on any previous information you provided, you can then go through the data groupings and attempt to classify them based on your understanding of the dataset. For instance, you might recognize that the different temperature groups represent all four seasons or that the weather patterns are separated into different types of weather, such as rain, sleet, or snow.\u00a0 Unsupervised machine learning methods In general, there are three types of unsupervised learning tasks: clustering, association rules, and dimensionality reduction.\u00a0 Below we\u2019ll delve a little deeper into each type of unsupervised learning technique. Clustering Clustering is a technique for exploring raw, unlabeled data and breaking it down into groups (or clusters) based on similarities or differences. It is used in a variety of applications, including customer segmentation, fraud detection, and image analysis. Clustering algorithms split data into natural groups by finding similar structures or patterns in uncategorized data.\u00a0 Clustering is one of the most popular unsupervised machine learning approaches. There are several types of unsupervised learning algorithms that are used for clustering, which include exclusive, overlapping, hierarchical, and probabilistic.\u00a0\u00a0 Association Association rule mining is a rule-based approach to reveal interesting relationships between data points in large datasets. Unsupervised learning algorithms search for frequent if-then associations\u2014also called rules\u2014to discover correlations and co-occurrences within the data and the different connections between data objects.\u00a0 It is most commonly used to analyze retail baskets or transactional datasets to represent how often certain items are purchased together. These algorithms uncover customer purchasing patterns and previously hidden relationships between products that help inform recommendation engines or other cross-selling opportunities. You might be most familiar with these rules from the \u201cFrequently bought together\u201d and \u201cPeople who bought this item also bought\u201d sections on your favorite online retail shop.\u00a0 Association rules are also often used to organize medical datasets for clinical diagnoses. Using unsupervised machine learning and association rules can help doctors identify the probability of a specific diagnosis by comparing relationships between symptoms from past patient cases.\u00a0 Typically, Apriori algorithms are the most widely used for association rule learning to identify related collections of items or sets of items. However, other types are used, such as Eclat and FP-growth algorithms. Dimensionality reduction Dimensionality reduction is an unsupervised learning technique that reduces the number of features, or dimensions, in a dataset. More data is generally better for machine learning, but it can also make it more challenging to visualize the data. Dimensionality reduction extracts important features from the dataset, reducing the number of irrelevant or random features present. This method uses principle component analysis (PCA) and singular value decomposition (SVD) algorithms to reduce the number of data inputs without compromising the integrity of the properties in the original data. Real-world unsupervised learning examples Now that you understand the basics of how unsupervised learning works, let\u2019s look at the most common use cases helping businesses explore large volumes of data quickly.\u00a0 \u00a0\u00a0 Here are some real-world unsupervised learning examples: Unsupervised learning is well suited for tasks that require exploring large amounts of unlabeled data. This approach makes it easier for businesses to gain insights from data when no labels are present, helping them to understand the underlying structure of a dataset and identify patterns and relationships between datasets without the need for a human to teach them. Supervised learning vs. unsupervised learning The main difference between supervised learning and unsupervised learning is the type of input data that you use. Unlike unsupervised machine learning algorithms, supervised learning relies on labeled training data to determine whether pattern recognition within a dataset is accurate.\u00a0 The goals of supervised learning models are also predetermined, meaning that the type of output of a model is already known before the algorithms are applied. In other words, the input is mapped to the output based on the training data. Solve your business challenges with Google Cloud Related products and services Google offers a number of innovative AI and machine learning products, solutions, and applications, enabling businesses to easily build and implement machine learning algorithms and models. Solution Take the next step Start building on Google Cloud with $300 in free credits and 20+ always free products. Need help getting started? Work with a trusted partner Continue browsing Why Google Products and pricing Solutions Resources Engage"}
{"url": "https://www.javatpoint.com/unsupervised-machine-learning", "title": "Unsupervised Machine learning - Javatpoint", "content": "Python AI, ML and Data Science Java B.Tech and MCA Web Technology Software Testing Technical Interview Java Interview Web Interview Database Interview Company Interviews Machine Learning Supervised Learning Classification Miscellaneous Related Tutorials Interview Questions Unsupervised Machine Learning In the previous topic, we learned supervised machine learning in which models are trained using labeled data under the supervision of training data. But there may be many cases in which we do not have labeled data and need to find the hidden patterns from the given dataset. So, to solve such types of cases in machine learning, we need unsupervised learning techniques. What is Unsupervised Learning? As the name suggests, unsupervised learning is a machine learning technique in which models are not supervised using training dataset. Instead, models itself find the hidden patterns and insights from the given data. It can be compared to learning which takes place in the human brain while learning new things. It can be defined as: Unsupervised learning cannot be directly applied to a regression or classification problem because unlike supervised learning, we have the input data but no corresponding output data. The goal of unsupervised learning is to find the underlying structure of dataset, group that data according to similarities, and represent that dataset in a compressed format. Example: Suppose the unsupervised learning algorithm is given an input dataset containing images of different types of cats and dogs. The algorithm is never trained upon the given dataset, which means it does not have any idea about the features of the dataset. The task of the unsupervised learning algorithm is to identify the image features on their own. Unsupervised learning algorithm will perform this task by clustering the image dataset into the groups according to similarities between images. Why use Unsupervised Learning? Below are some main reasons which describe the importance of Unsupervised Learning: Working of Unsupervised Learning Working of unsupervised learning can be understood by the below diagram: Here, we have taken an unlabeled input data, which means it is not categorized and corresponding outputs are also not given. Now, this unlabeled input data is fed to the machine learning model in order to train it. Firstly, it will interpret the raw data to find the hidden patterns from the data and then will apply suitable algorithms such as k-means clustering, Decision tree, etc. Once it applies the suitable algorithm, the algorithm divides the data objects into groups according to the similarities and difference between the objects. Types of Unsupervised Learning Algorithm: The unsupervised learning algorithm can be further categorized into two types of problems: Note: We will learn these algorithms in later chapters. Unsupervised Learning algorithms: Below is the list of some popular unsupervised learning algorithms: Advantages of Unsupervised Learning Disadvantages of Unsupervised Learning Learn Important Tutorial B.Tech / MCA Preparation We provides tutorials and interview questions of all technology like java tutorial, android, java frameworks  Contact info  G-13, 2nd Floor, Sec-3, Noida, UP, 201301, India [email\u00a0protected]. Follow us Latest Post PRIVACY POLICY Tutorials Interview Questions Online Compiler"}
{"url": "https://builtin.com/data-science/transfer-learning", "title": "What Is Transfer Learning? A Guide for Deep Learning | Built In", "content": "What Is Transfer Learning? Exploring the Popular Deep Learning Approach. \n                    Discover the value of transfer learning and how to use it.\n                 Transfer learning is the reuse of a pre-trained model on a new problem. It\u2019s popular in deep learning because it can train deep\u00a0neural networks with comparatively little data. This is very useful in the\u00a0data science field since most real-world problems typically do not have millions of labeled data points to train such complex models. We\u2019ll take a look at what transfer learning is, how it works and why and when it should be used. Additionally, we\u2019ll cover the different approaches of transfer learning and provide you with some resources on already pre-trained models. What Is Transfer Learning? \u00a0 What Is Transfer Learning? In transfer learning, the knowledge of an already trained machine learning model is applied to a different but related problem. For example, if you trained a simple classifier to predict whether an image contains a backpack, you could use the knowledge that the model gained during its training to recognize other objects like sunglasses. With transfer learning, we basically try to exploit what has been learned in one task to improve generalization in another. We transfer the weights that a network has learned at \u201ctask A\u201d to a new \u201ctask B.\u201d The general idea is to use the knowledge a model has learned from a task with a lot of available labeled training data in a new task that doesn\u2019t have much data. Instead of starting the learning process from scratch, we start with patterns learned from solving a related task. Transfer learning is mostly used in computer vision and\u00a0natural language processing tasks like sentiment analysis due to the huge amount of computational power required. Transfer learning isn\u2019t really a machine learning technique, but can be seen as a \u201cdesign methodology\u201d within the field. It is also not exclusive to machine learning. Nevertheless, it has become quite popular in combination with neural networks that require huge amounts of data and computational power. \u00a0 How Transfer Learning Works In computer vision, for example, neural networks usually try to detect edges in the earlier layers, shapes in the middle layer and some task-specific features in the later layers. In transfer learning, the early and middle layers are used and we only retrain the latter layers. It helps leverage the labeled data of the task it was initially trained on. This process of retraining models is known as fine-tuning. In the case of transfer learning, though, we need to isolate specific layers for retraining. There are then two types of layers to keep in mind when applying transfer learning:\u00a0 Let\u2019s go back to the example of a model trained for recognizing a backpack in an image, which will be used to identify sunglasses. In the earlier layers, the model has learned to recognize objects, so we will only retrain the latter layers to help it learn what separates sunglasses from other objects.  In transfer learning, we try to transfer as much knowledge as possible from the previous task the model was trained on to the new task at hand. This knowledge can be in various forms depending on the problem and the data. For example, it could be how models are composed, which allows us to more easily identify novel objects. \u00a0 Why Use Transfer Learning The main advantages of transfer learning are saving training time, improving the performance of neural networks (in most cases) and not needing a lot of data.\u00a0 Usually, a lot of data is needed to train a neural network from scratch, but access to that data isn\u2019t always available. With transfer learning, a solid machine learning model can be built with comparatively little training data because the model is already pre-trained. This is especially valuable in natural language processing because mostly expert knowledge is required to create large labeled data sets. Additionally, training time is reduced because it can sometimes take days or even weeks to train a deep neural network from scratch on a complex task. \u00a0 When to Use Transfer Learning As is always the case in machine learning, it is hard to form rules that are generally applicable, but here are some guidelines on when transfer learning might be used: If the original model was trained using an open-source library like TensorFlow, you can simply restore it and retrain some layers for your task. Keep in mind, however, that transfer learning only works if the features learned from the first task are general, meaning they can be useful for another related task as well. Also, the input of the model needs to have the same size as it was initially trained with. If you don\u2019t have that, add a pre-processing step to resize your input to the needed size. \u00a0 Approaches to Transfer Learning 1. Training a Model to Reuse it Imagine you want to solve task A but don\u2019t have enough data to train a deep neural network. One way around this is to find a related task B with an abundance of data. Train the deep neural network on task B and use the model as a starting point for solving task A. Whether you\u2019ll need to use the whole model or only a few layers depends heavily on the problem you\u2019re trying to solve. If you have the same input in both tasks, possibly reusing the model and making predictions for your new input is an option. Alternatively, changing and retraining different task-specific layers and the output layer is a method to explore. 2. Using a Pre-Trained Model The second approach is to use an already pre-trained model. There are a lot of these models out there, so make sure to do a little research. How many layers to reuse and how many to retrain depends on the problem.\u00a0 Keras, for example, provides numerous pre-trained models that can be used for transfer learning, prediction, feature extraction and fine-tuning. You can find these models, and also some brief tutorials on how to use them, here. There are also many research institutions that release trained models. This type of transfer learning is most commonly used throughout deep learning. 3. Feature Extraction Another approach is to use deep learning to discover the best representation of your problem, which means finding the most important features. This approach is also known as representation learning, and can often result in a much better performance than can be obtained with hand-designed representation.  In machine learning, features are usually manually hand-crafted by researchers and domain experts. Fortunately, deep learning can extract features automatically. Of course, you still have to decide which features you put into your network. That said, neural networks have the ability to learn which features are really important and which ones aren\u2019t. A representation learning algorithm can discover a good combination of features within a very short timeframe, even for complex tasks which would otherwise require a lot of human effort. The learned representation can then be used for other problems as well. Simply use the first layers to spot the right representation of features, but don\u2019t use the output of the network because it is too task-specific. Instead, feed data into your network and use one of the intermediate layers as the output layer. This layer can then be interpreted as a representation of the raw data. This approach is mostly used in computer vision because it can reduce the size of your dataset, which decreases computation time and makes it more suitable for traditional algorithms as well. \u00a0 Popular Pre-Trained Models There are some pre-trained machine learning models out there that are quite popular. One of them is the Inception-v3 model, which was trained for the ImageNet \u201cLarge Visual Recognition Challenge.\u201d In this challenge, participants had to classify images into\u00a01,000 classes like \u201czebra,\u201d \u201cDalmatian\u201d and \u201cdishwasher.\u201d Here\u2019s a very good tutorial from TensorFlow on how to retrain image classifiers. Microsoft also offers some pre-trained models, available for both R and Python development, through the\u00a0MicrosoftML R package and the microsoftml Python package. Other quite popular models are ResNet and AlexNet. Frequently Asked Questions What is transfer learning? Transfer learning is a machine learning technique where a model trained on one task is reused for another related task. This way, a model can build on its previous knowledge to master new tasks, and you can continue training a model despite having limited data. What is the difference between transfer learning and deep learning? Transfer learning is a specific technique in machine learning that involves reusing a model and its knowledge for learning another task. Meanwhile, deep learning is a type of machine learning that involves using artificial neural networks to mimic the way humans learn. Deep learning requires large amounts of data to train models from scratch, so transfer learning serves as an alternative when limited data is available. What is the difference between CNN and transfer learning? Convolutional neural networks (CNNs) are deep learning algorithms that imitate neural networks in the human brain and use three-dimensional data to excel at image-related tasks like recognizing objects and classifying images. Transfer learning is a technique used in deep learning and machine learning, where a pre-trained model is applied to another task. As a result, transfer learning can be used to train CNNs. Recent Data Science Articles"}
{"url": "https://aws.amazon.com/what-is/transfer-learning/", "title": "What is Transfer Learning? - Transfer Learning in Machine Learning Explained - AWS", "content": "What is Transfer Learning? What is transfer learning? Transfer learning (TL) is a machine learning (ML) technique where a model pre-trained on one task is fine-tuned for a new, related task. Training a new ML model is a time-consuming and intensive process that requires a large amount of data, computing power, and several iterations before it is ready for production. Instead, organizations use TL to retrain existing models on related tasks with new data. For example, if a machine learning model can identify images of dogs, it can be trained to identify cats using a smaller image set that highlights the feature differences between dogs and cats. What are the benefits of transfer learning? TL offers several of the following benefits to researchers creating ML applications. Enhanced efficiency Training ML models takes time as they build knowledge and identify patterns. It also requires a large data set and is computationally expensive. In TL, a pre-trained model retains fundamental knowledge of tasks, features, weights, and functions, allowing it to adapt to new tasks faster. You can use a much smaller dataset and fewer resources while achieving better results.\u00a0 Increased accessibility Building deep-learning neural networks requires large data volumes, resources, computing power, and time. TL overcomes these barriers to creation, allowing organizations to adopt ML for custom use cases. You can adapt existing models to your requirements at a fraction of the cost. For example, using a pre-trained image recognition model, you can create models for medical imaging analysis, environmental monitoring, or facial recognition with minimal adjustments. Improved performance Models developed through TL often demonstrate greater robustness in diverse and challenging environments. They better handle real-world variability and noise, having been exposed to a wide range of scenarios in their initial training. They give better results and adapt to unpredictable conditions more flexibly. What are the different transfer learning strategies? The strategy you use to facilitate TL will depend on the domain of the model you are building, the task it needs to complete, and the availability of training data. Transductive transfer learning Transductive transfer learning involves transferring knowledge from a specific source domain to a different but related target domain, with the primary focus being on the target domain. It is especially useful when there is little or no labeled data from the target domain. Transductive transfer learning asks the model to make predictions on target data by using previously-gained knowledge. As the target data is mathematically similar to the source data, the model finds patterns and performs faster.\u00a0 For example, consider adapting a sentiment analysis model trained on product reviews to analyze movie reviews. The source domain (product reviews) and the target domain (movie reviews) differ in context and specifics but share similarities in structure and language use. The model quickly learns to apply its understanding of sentiment from the product domain to the movie domain. Inductive transfer learning Inductive transfer learning is where the source and target domains are the same, but the tasks the model must complete differ. The pre-trained model is already familiar with the source data and trains faster for new functions. An example of inductive transfer learning is in natural language processing (NLP). Models are pre-trained on a large set of texts and then fine-tuned using inductive transfer learning to specific functions like sentiment analysis. Similarly, computer vision models like VGG are pre-trained on large image datasets and then fine-tuned to develop object detection. Unsupervised transfer learning Unsupervised transfer learning uses a strategy similar to inductive transfer learning to develop new abilities. However, you use this form of transfer learning when you only have unlabeled data in both the source and target domains.\u00a0 The model learns the common features of unlabeled data to generalize more accurately when asked to perform a target task. This method is helpful if it is challenging or expensive to obtain labeled source data. For example, consider the task of identifying different types of motorcycles in traffic images. Initially, the model is trained on a large set of unlabeled vehicle images. In this instance, the model independently determines the similarities and distinguishing features among different types of vehicles like cars, buses, and motorcycles. Next, the model is introduced to a small, specific set of motorcycle images. The model performance improves significantly compared to before. What are the steps in transfer learning? There are three main steps when fine-tuning a machine-learning model for a new task. Select a pre-trained model First, select a pre-trained model with prior knowledge or skills for a related task. A useful context for choosing a suitable model is to determine the source task of each model. If you understand the original tasks the model performed, you can find one that more effectively transitions to a new task. Configure your pre-trained models After selecting your source model, configure it to pass knowledge to a model to complete the related task. There are two main methods of doing this. Freeze pre-trained layers Layers are the building blocks of neural networks. Each layer consists of a set of neurons and performs specific transformations on the input data. Weights are the parameters the network uses for decision-making. Initially set to random values, weights are adjusted during the training process as the model learns from the data. By freezing the weights of the pre-trained layers, you keep them fixed, preserving the knowledge that the deep learning model obtained from the source task. Remove the last layer In some use cases, you can also remove the last layers of the pre-trained model. In most ML architectures, the last layers are task-specific. Removing these final layers helps you reconfigure the model for new task requirements. Introduce new layers Introducing new layers on top of your pre-trained model helps you adapt to the specialized nature of the new task. The new layers adapt the model to the nuances and functions of the new requirement. Train the model for the target domain You train the model on target task data to develop its standard output to align with the new task. The pre-trained model likely produces different outputs from those desired. After monitoring and evaluating the model\u2019s performance during training, you can adjust the hyperparameters or baseline neural network architecture to improve output further. Unlike weights, hyperparameters are not learned from the data. They are pre-set and play a crucial role in determining the efficiency and effectiveness of the training process. For example, you could adjust regularization parameters or the model\u2019s learning rates to improve its ability in relation to the target task. What are transfer learning strategies in generative AI? Transfer learning strategies are critical for generative AI adoption in various industries. Organizations can customize existing foundation models without having to train new ones on billions of data parameters at scale. The following are some transfer learning strategies used in generative AI. Domain adversarial training Domain adversarial training involves training a foundation model to produce data that is indistinguishable from real data in the target domain. This technique typically employs a discriminator network, as seen in generative adversarial networks, that attempts to distinguish between true data and generated data. The generator learns to create increasingly realistic data. For example, in image generation, a model trained on photographs might be adapted to generate artwork. The discriminator helps ensure the generated artwork is stylistically consistent with the target domain. Teacher-student learning Teacher-student learning involves a larger and more complex \u201cteacher\u201d model teaching a smaller and simpler \u201cstudent\u201d model. The student model learns to mimic the teacher model's behavior, effectively transferring knowledge. This is useful for deploying large generative models in resource-constrained environments. For example, a large language model (LLM) could serve as a teacher to a smaller model, transferring its language generation capabilities. This would allow the smaller model to generate high-quality text with less computational overhead. Feature disentanglement Feature disentanglement in generative models involves separating different aspects of data, such as content and style, into distinct representations. This enables the model to manipulate these aspects independently in the transfer learning process. For example, in a face generation task, a model might learn to disentangle facial features from artistic style. This would allow it to generate portraits in various artistic styles while maintaining the subject's likeness. Cross-modal transfer learning Cross-modal transfer learning involves transferring knowledge between different modalities, like text and images. Generative models can learn representations applicable across these modalities. A model trained on textual descriptions and corresponding images might learn to generate relevant images from new text descriptions, effectively transferring its understanding from text to image. Zero-shot and few-shot learning In zero-shot and few-shot learning, generative models are trained to perform tasks or generate data for which they have seen few or no examples of during training. This is achieved by learning rich representations that generalize well. For example, a generative model might be trained to create images of animals. Using few-shot learning, it could generate images of a rarely seen animal by understanding and combining features from other animals. How can AWS help with your transfer learning requirements? Amazon SageMaker JumpStart is an ML hub where you can access pre-trained models, including foundation models, to perform tasks like article summarization and image generation. You can use transfer learning to produce accurate models on your smaller datasets, with lower training costs than those involved in training the original model. For example, with SageMaker JumpStart, you can: When using the Cross-modal transfer learning approach, you can also use Amazon SageMaker Debugger to detect serious hidden problems. For example, you can examine model predictions to find mistakes, validate the robustness of your model, and consider how much of this robustness is from the inherited abilities. You can also validate input and preprocesses to the model for realistic expectations. Get started with transfer learning on AWS by creating a free account today.  Next Steps on AWS Instant get access to the AWS Free Tier. Get started building in the AWS management console.  Learn About AWS  Resources for AWS  Developers on AWS  Help  Ending Support for Internet Explorer"}
{"url": "https://medium.com/@davidfagb/guide-to-transfer-learning-in-deep-learning-1f685db1fc94", "title": "Guide To Transfer Learning in Deep Learning | by David Fagbuyiro | Medium", "content": "Sign up Sign in Sign up Sign in Guide To Transfer Learning in Deep Learning David Fagbuyiro Follow -- Listen Share In this guide, we will cover what transfer learning is, and the main approaches to transfer learning in deep learning. What is Transfer Learning Transfer learning is an approach to machine learning where a model trained on one task is used as the starting point for a model on a new task. This is done by transferring the knowledge that the first model has learned about the features of the data to the second model. In deep learning, transfer learning is often used to solve problems with limited data. This is because deep learning models typically require a large amount of data to train, which can be difficult or expensive to obtain. Why Use Transfer Learning? Here are some reasons why you might want to use transfer learning: Types of Transfer Learning Transfer learning can be classified into two types: Example: The Output: This approach is useful if you have limited data for the target task. For example, if you have a small number of labeled reviews, you can use a pre-trained model to extract features from the reviews. Below is a code example of fine-tuning a pre-trained language model (LLM) to classify text as either positive or negative: This code will fine-tune the pre-trained DistilBERT LLM on the train_data dataset. The train_data dataset should be a PyTorch DataLoader object containing batches of text sequences and their corresponding labels (either positive or negative). To use the fine-tuned model, you can simply load it and then call the model() method with the text sequence that you want to classify. For example: The code above will print the predicted label for the text sequence This is a positive review.. In this case, the predicted label is 1, which corresponds to the positive class. Fine-tuning can be used to improve the performance of pre-trained LLMs on a wide variety of tasks, such as sentiment analysis, question answering, and text summarization. What Is a Pre-Trained Model? A pre-trained model is a machine learning model that has been trained on a large dataset of data. This dataset is typically much larger than the dataset that will be used to train the final model. The pre-trained model learns to extract features from the data, and these features can be used to train the final model more quickly and efficiently. Popular Pre-Trained Architectures There are many popular pre-trained architectures, but some of the most common include: These are just a few of the many popular pre-trained architectures. The best architecture for a particular task will depend on the specific requirements of the task. What is Transferable Knowledge? Transferable knowledge in the context of transfer learning refers to the information, patterns, and representations learned by a machine learning model during training on one task or domain that can be effectively applied to improve performance on a different, often related, task or domain. Low-level Features vs. High-level Semantics In transfer learning, models can transfer both low-level features and high-level semantics from a source task/domain to a target task/domain. 1. Low-level Features: These are basic, primitive, and often raw data representations. In image processing, low-level features could include edges, colors, textures, or local patterns. In natural language processing, they might involve word embeddings or character-level features. Low-level features capture fundamental characteristics of the data that are often reusable in various contexts. 2. High-level Semantics: High-level semantics represent more abstract and complex concepts or meanings within the data. In image analysis, high-level semantics could involve recognizing objects, scenes, or context. In natural language understanding, it might entail understanding sentiment, intent, or named entities. Domain-specific vs. Generic Knowledge Transfer learning can involve transferring both domain-specific and generic knowledge. 1. Domain-specific Knowledge: This refers to knowledge and information that is specific to a particular task or domain. For example, medical image analysis might involve domain-specific knowledge related to anatomical structures or disease patterns. Transfer learning can adapt such knowledge to improve performance on similar medical imaging tasks. 2. Generic Knowledge: Generic knowledge, on the other hand, is more general and broadly applicable. It includes foundational concepts and representations that are not tied to a specific task or domain. For instance, knowledge about spatial relationships, language syntax, or image textures can be considered generic. This type of knowledge can be transferred across a wide range of tasks and domains to aid in learning. Task Similarity & Domains Fine-tuning vs. Feature Extraction In transfer learning, fine-tuning and feature extraction are two common techniques to adapt a pre-trained model to a new task or domain: Same-domain vs. Cross-domain Transfer Same-domain and cross-domain transfer refer to the relationship between the source and target domains in transfer learning: Steps to Implement Transfer Learning Here are the key steps to implement transfer learning in a nutshell: 1. Dataset Preparation: Begin by collecting and preprocessing your dataset. Ensure it is well-organized and contains labeled data for your target task. Typically, transfer learning works best when you have a smaller dataset, and it\u2019s essential to split it into training, validation, and testing sets. 2. Model Selection & Architecture: Choose a pre-trained deep learning model that suits your problem. Common choices include VGG, ResNet, Inception, and BERT, depending on whether it\u2019s an image classification, object detection, or natural language processing task. Next, adapt the architecture of the selected model to match the number of classes or labels in your dataset. 3. Transfer Strategy: Decide on the transfer learning strategy. There are two primary approaches which are: feature extraction and fine-tuning. 4. Hyperparameter Tuning: Optimize hyperparameters such as learning rate, batch size, and the number of epochs. You can use techniques like grid search or random search to find the best combination of hyperparameters. This step is crucial for achieving optimal model performance. 5. Training & Evaluation: Train the modified model on your dataset using the chosen hyperparameters. Monitor the training process by tracking metrics on the validation set. To prevent overfitting, consider using techniques like early stopping or dropout. Finally, evaluate the model\u2019s performance on the test set using appropriate evaluation metrics, depending on your task (accuracy, F1-score, mean squared error, etc.). Challenges and Considerations While it offers numerous advantages, it also comes with its own set of challenges and considerations that practitioners must be aware of. 1. Dataset Bias & Mismatch: Transfer learning relies heavily on the assumption that the source and target domains share some similarities. However, in real-world scenarios, datasets can exhibit bias or mismatch in terms of distribution, quality, or context. 2. Overfitting & Generalization: One of the key challenges in transfer learning is finding the right balance between overfitting and generalization. Transferring too much knowledge from the source domain may lead to overfitting on the target domain, while transferring too little may hinder generalization. 3. Catastrophic Forgetting: Transfer learning often involves fine-tuning a pre-trained model on a new task. However, this process can lead to the phenomenon known as catastrophic forgetting, where the model forgets important information from the source domain while adapting to the target domain. Strategies like progressive learning or elastic weight consolidation can help alleviate this issue. 4. Ethical & Privacy Concerns: Transfer learning can be problematic when dealing with sensitive data, such as medical records or personal information. Care must be taken to ensure that transferred knowledge doesn\u2019t violate privacy or ethical standards. 5. Computational Resources: Large pre-trained models, such as those in natural language processing, demand substantial computational resources for training and fine-tuning. Smaller organizations or researchers with limited access to high-performance computing may face challenges in implementing transfer learning effectively. Practical Applications of Transfer Learning Here, we explore some practical applications of transfer learning in different fields with code examples: Conclusion Through this guide, we\u2019ve explored all you need to know about transfer learning, including different transfer learning scenarios, popular pre-trained models, and the steps involved in adapting them for various tasks with code examples. -- -- Written by David Fagbuyiro Technical writer No responses yet Help Status About Careers Press Blog Privacy Terms Text to speech Teams"}
{"url": "https://www.oracle.com/artificial-intelligence/ai-model-training/", "title": "What Is AI Model Training & Why Is It Important?", "content": "What Is AI Model Training & Why Is It Important? Michael Chen | Content Strategist | December 6, 2023 In This Article In popular culture, AI sometimes gets a bad rap. Movies show it as the first step on the road to a robot apocalypse, while the news is filled with stories of how AI will take all our jobs. The truth is that AI has existed for a while, and neither of those worst-case scenarios are likely imminent. Fundamentally, AI uses data to make predictions. That capability may power \u201cyou may also like\u201d tips on streaming services, but it\u2019s also behind chatbots capable of understanding natural language queries and predicting the correct answer and applications that look at a photo and use facial recognition to suggest who\u2019s in the picture. Getting to those predictions, though, requires effective AI model training, and newer applications that depend on AI may demand slightly different approaches to learning. What Is AI Model Training?  At its core, an AI model is both a set of selected algorithms and the data used to train those algorithms so that they can make the most accurate predictions. In some cases, a simple model uses only a single algorithm, so the two terms may overlap, but the model itself is the output after training. In a mathematical sense, an algorithm can be considered an equation with undefined coefficients. The model comes together when the selected algorithms digest data sets to determine what coefficient values fit best, thus creating a model for predictions. The term \u201cAI model training\u201d refers to this process: feeding the algorithm data, examining the results, and tweaking the model output to increase accuracy and efficacy. To do this, algorithms need massive amounts of data that capture the full range of incoming data. Outliers, surprises, inconsistencies, patterns that don\u2019t make sense at first glance\u2026algorithms must deal with all of these and more, repeatedly, across all incoming data sets. This process is the foundation of learning\u2014the ability to recognize patterns, understand context, and make appropriate decisions. With enough AI model training, the set of algorithms within the model will represent a mathematical predictor for a given situation that builds in tolerances for the unexpected while maximizing predictability. Key Takeaways AI Model Training Explained AI model training is an iterative process whose success depends on the quality and depth of the input as well as the ability of trainers to identify and compensate for deficiencies. Data scientists usually handle the training process, although even business users can be involved in some low-code/no-code environments. In fact, the cycle of processing, observing, providing feedback, and improving is akin to teaching a child a new skill. With AI model training, the goal is to create a mathematical model that accurately creates an output while balancing the many different possible variables, outliers, and complications in data. When you think about it, parenting offers a similar\u2014but much messier\u2014journey. Consider how children learn a skill. For example, let\u2019s say you want to teach a toddler to identify the difference between dogs and cats. This starts out with basic pictures and encouragement. Then more variables are introduced, with details such as average sizes, barks versus meows, and behavior patterns. Based on what the child might be struggling with, you can put more emphasis on a certain area to help facilitate learning. At the end of this process, the toddler should be able to identify all manner of dogs and cats, from common household pets to wildlife species. Training an AI model is similar. AI: Select algorithms and initial training data set for the model. Child: Use basic photos to establish the general differences between a dog and a cat. AI: Evaluate output accuracy and tune the model to reduce or eliminate certain inaccuracies. Child: Give praise or corrections depending on the answers. AI: Provide additional data sets with specific diverse inputs to customize and fine-tune the model. Child: Highlight different traits, shapes, and sizes as part of the learning process. Like with children, initial AI model training can highly influence what happens down the road\u2014and if further lessons are needed to unlearn poor influences. This highlights the importance of quality data sources, both for initial training and continuous iterative learning even after the model launches. The Value of AI Models in Business Most organizations already benefit from AI within their workflows and processes, thanks to applications that generate analytics, highlight data outliers, or use text recognition and natural language processing. Think transcribing paper receipts and documents into data records, for example. However, many organizations are looking to develop AI models for the purpose of addressing a specific, pressing need. The development process itself may unlock deeper layers of benefits, from short-term value, such as accelerated processes, to long-term gain, such as uncovering previously hidden insights or perhaps even launching a new product or service. A core reason to invest in an infrastructure capable of supporting AI stems from the way businesses grow. Simply put, data is everywhere. With so much data coming in from all directions, new insights can be generated for nearly every part of an organization, including internal operations and the performance of sales and marketing teams. With that in mind, proper training and thoughtful application allows for AI to provide business value in nearly any circumstance. To consider how an organization might train AI for maximum benefit, the first step is to identify inputs and what goes into a solid decision. For example, consider a manufacturing supply chain. Once all relevant data is available to a properly trained AI system, it can calculate shipping costs, predict ship times and quality/defect rates, recommend price changes based on market conditions, and perform many more tasks. The combination of heavy incoming data volumes and a need for data-driven decisions make supply chains ripe for AI problem solving. In contrast, in cases where soft skills remain a top priority, AI can provide supporting information but is unlikely to offer a revolutionary change. An example is a manager\u2019s assessment of employee performance during annual reviews. In this case, AI might make it easier to gather metrics, but it can\u2019t replace the assessments made based on human-to-human interaction. To get the most out of an AI investment, organizations must consider the following: By establishing those parameters, organizations can identify the business areas most likely to benefit from AI, then begin taking steps to make those a reality. The Process of Training an AI Model While each project comes with its own challenges and requirements, the general process for training AI models remains the same. These five steps comprise an overview for training an AI model. Prepare the data: Successful AI model training starts with quality data that accurately and consistently represents real-world and authentic situations. Without it, ensuing results are meaningless. To succeed, project teams must curate the right data sources, build processes and infrastructure for manual and automated data collection, and institute appropriate cleaning/transformation processes. Select a training model: If curating data provides the groundwork for the project, model selection builds the mechanism. Variables for this decision include defining project parameters and goals, choosing the architecture, and selecting model algorithms. Because different training models require different amounts of resources, these factors must be weighed against practical elements such as compute requirements, deadlines, costs, and complexity. Perform initial training: Just as with the example above of teaching a child to tell a cat from a dog, AI model training starts with basics. Using too wide of a data set, too complex of an algorithm, or the wrong model type could lead to a system that simply processes data rather than learning and improving. During initial training, data scientists should focus on getting results within expected parameters while watching for algorithm-breaking mistakes. By training without overreaching, models can methodically improve in steady, assured steps. Validate the Training: Once the model passes the initial training phase, it reliably creates expected results across key criteria. Training validation represents the next phase. Here, experts set out to appropriately challenge the model in an effort to reveal problems, surprises, or gaps in the algorithm. This stage uses a separate group of data sets from the initial phase, generally with increased breadth and complexity versus the training data sets. As data scientists run passes with these data sets, they evaluate the model\u2019s performance. While output accuracy is important, the process itself is just as critical. Top priorities for the process include variables such as precision, the percentage of accurate predictions, and recall, the percentage of correct class identification. In some cases, the results can be judged with a metric value. For example, an F1 score is a metric assigned to classification models that incorporate the weights of different types of false positives/negatives, allowing a more holistic interpretation of the model's success. Test the Model: Once the model has been validated using curated and fit-for-purpose data sets, live data can be used to test performance and accuracy. The data sets for this stage should be pulled from real-world scenarios, a proverbial \u201ctaking the training wheels off\u201d step to let the model fly on its own. If the model delivers accurate\u2014and more importantly, expected\u2014results with test data, it\u2019s ready to go live. If the model shows deficiencies in any way, the training process repeats until the model meets or exceeds performance standards. While going live is a significant milestone, achieving that stage doesn\u2019t mean the end of the model\u2019s training. Depending on the model, every data set processed may be another \u201clesson\u201d for the AI, leading to further improvement and refinement of the algorithm. Data scientists must continue to monitor performance and results, particularly when the model deals with unexpected outlier data. Should inaccurate results arise, even only on rare occasions, the model may need further tweaking so as not to taint future output. Types of AI Model Training Methods AI training comes in many different forms that range in complexity, types of results, capabilities, and compute power. One method may use up more resources than necessary while in other cases a method may provide a binary response, as in a yes or no for a loan approval, when the situation requires a more qualitative outcome, such as a conditional \u201cno\u201d until more documentation is supplied. The choice of method used for an AI model must factor in both goals and resources; venturing forward without careful planning may require data science teams to restart from scratch, wasting time and money. Deep Neural Networks While some AI models use rules and inputs to make decisions, deep neural networks offer the ability to handle complex decisions based on diverse data relationships. Deep neural networks work with numerous layers that identify patterns and weighted relationships among data points to make predictive outputs or informed assessments. Examples of deep neural networks include voice-activated assistants such as Apple\u2019s Siri or Amazon\u2019s Alexa. Linear Regression In statistics, linear regression is used to determine the relationship between input and output. In its simplest form, this can be represented by the algebraic formula y = Ax + B. This model uses a data set to create that formula based on input, output, and possible variable coefficients. The final model used for prediction assumes a linear relationship between input and output. An example use case for linear regression is a sales forecast based on previous sales data. Logistic Regression Taken from the field of statistics, logistic regression is an effective model for binary situations. Logistic regression is based on the logistic function, which is an S-curve equation often used for calculating probability. In the case of AI modeling, logistic regression determines probability and delivers a binary outcome to ultimately make predictions or decide, for example, whether an applicant should be approved for a loan. An example use case for logistic regression is a finance application performing fraud detection. Decision Trees Most people have experience with decision trees, even outside of AI. Decision trees work similarly to nodes in flowcharts. In machine learning, the training processes feed the tree through iterative data to identify when to add nodes and where to send the different node paths. An example use case for decision trees is a financial loan approval. Random Forest Decision trees can become overfit for their training sets by establishing too much depth. The random forest technique compensates for that by combining a group of decision trees\u2014hence the term \u201cforest\u201d\u2014and finding the greatest consensus or a weighted average in results. An example use case for a random forest is predicting customer behavior based on a variety of decision trees across different elements of a customer\u2019s profile. Supervised Learning In child education terms, supervised learning is the equivalent of having your child go through a set curriculum with methodical lessons. For AI modeling, that means using established training data sets and defined parameters to train the model, with data scientists acting as the proverbial teachers in curating training data sets, running test data sets, and providing model feedback. An example use case for supervised learning is finding abnormal cells in lung X-rays. The training data set is X-rays with and without abnormalities and telling the model which is which. Unsupervised Learning Continuing the child education analogy, unsupervised learning is similar to the Montessori philosophy, where children are presented with a range of possibilities and the freedom to self-direct based on their curiosity. For AI modeling, that means ingesting an unlabeled data set without parameters or goals\u2014it\u2019s up to the AI to determine patterns in the data. An example use case for unsupervised learning is a retailer feeding an AI model quarterly sales data with the goal of finding correlations in customer behavior. Reinforcement Learning If you\u2019ve ever reinforced desired behavior with treats, you\u2019ve participated in reinforcement learning. On an AI level, reinforcement learning starts with experimental decisions that lead to positive or negative reinforcement. After time, the AI learns the best decisions, as in the most accurate or successful ones, to handle a situation and maximize positive reinforcement. An example use case for reinforcement learning is that list of \u201cyou might also like\u201d suggestions presented by YouTube based on viewing history. Transfer Learning An AI model may have success when applied to a different situation. Transfer learning refers to the method of using an existing AI model as a starting point for a new model. This repurposing works best when the existing model handles a general scenario; anything too specific may prove too difficult to retrain. An example use case for transfer learning is a new AI model for a specific type of image classification based on parameters from an existing image classification model. Semi-Supervised Learning Using principles of both supervised and unsupervised learning, semi-supervised learning starts with training the model on a small group of labeled data sets. From there, the model uses unlabeled and uncurated data sets to refine patterns and create unexpected insights. In general, semi-supervised learning uses only labeled data sets for the first few steps, like training wheels. After that, the process heavily leans on unlabeled data. An example use case for semi-supervised learning is a text-classifying model, which uses a curated set to establish basic parameters before being fed large volumes of unsupervised text documents. Generative Models Generative models are an unsupervised AI method that use very large example data sets to create a prompted output. Examples of this are AI-generated images based on the metadata of an image archive or predictive text based on a database of typed sentences. Rather than simply classifying data in its output, results from generative models can take thousands, possibly millions, of pieces of example data to learn and create an original output. An example use case of a generative model is a chatbot, such as ChatGPT. Role of Data in AI Model Training For an AI model to be properly trained, it needs data\u2014a lot of data. In fact, data is the most crucial element in AI model training. Without it, the model simply can\u2019t learn. And without quality data, the model will learn the wrong things. Thus, data scientists select data sets for their projects with intention and care. Data set curation must involve the following factors for optimal AI model training: Challenges in AI Model Training AI model training comes with its own unique challenges. Some of these are logistical\u2014infrastructure, compute power, and other practical considerations of getting from start to finish. Other challenges require introspection on the part of data scientists, such as developing an understanding of how to mitigate biases and keep the resultant system objective. The following challenges should be considerations for any AI model training initiative: Data bias: To get accurate results from an AI model, training requires quality data. To mitigate data bias, data scientists must vet data sources thoroughly before curating training data sets. The right data: Training data sets requires heavy volumes of data that represent appropriate diversity and granularity. Not only does this call on teams to curate large amounts of quality data, it brings in many practical considerations. Storage, cleaning/transformation, processing, and general quality control all grow increasingly difficult as a data set gets larger. Computing power and infrastructure requirements: The more complex the AI model, the more compute power and infrastructure support are required. The practicality of running the model, from training to going live, needs to be considered when selecting the model method. If a model type requires more resources than what\u2019s feasible to deliver, the whole project will collapse. Overfitting: When an AI model becomes too tuned into the training data sets, it can lock into those specifics rather than being capable of handling diversity and surprises. That phenomenon is known as \u201coverfitting,\u201d and it prevents accurate predictions in the future. An example of overfitting is when the training data set produces 99% accuracy but a real-world data set produces only 75% to 85% accuracy.  Note that perceived accuracy in AI refers to how well a system appears to perform in terms of accuracy based on its current capabilities. It\u2019s the accuracy that\u2019s observed or experienced by users or stakeholders. On the other hand, potential accuracy in AI refers to the maximum level of accuracy that a system could achieve in ideal conditions, with optimal resources. Understanding the difference between perceived accuracy and potential accuracy is important in evaluating the performance of an AI system and identifying areas for improvement or future development. The terms \u201coverfitting\u201d and \u201covertraining\u201d are often used interchangeably, but they have distinct meanings. Overfitting, as discussed, is when AI performs extremely well on its training data but fails to generalize well on new data. Overtraining is when a model has been trained excessively, leading to poor performance on both the training data and new data. Overtraining can occur when a model is trained for too long or with too much complexity, causing it to struggle to generalize. Both issues need to be avoided in the model training process. Explainability: One outstanding issue in AI modeling is the lack of explainability around how decisions are made. Users can make inferences based on outputs, but the model\u2019s reasons may remain nebulous. Some developers have created tools to bridge this gap, including models built to have more transparent explainability. However, implementation, usability, detail, and accessibility all vary, both for input and output. Future of AI Model Training While AI has been around in some form since the dawn of computing, advancements in algorithms, CPU power, graphics processing unit (GPU) power, and the cloud-based sharing of resources have significantly pushed AI forward over the last two decades. AI is embedded in so many applications that many users employ it without realizing it. When you stream music, customized playlists come from an AI analyzing your favorite songs and artists. When you type a text message, an AI offers predictive suggestions based on your commonly used words. If you found a new TV show you love thanks to an automated recommendation, thank AI. That\u2019s the present of AI, but what lies just over the horizon? The potential of AI depends on the evolving capabilities of model training. Let\u2019s take a look at future possibilities in AI model training. Advancements in AI Model Training Techniques If it feels like AI\u2019s innovations have grown exponentially, there\u2019s a good reason for that: The explosion of data and connectivity over the past decade has made it much easier to train AI systems and allowed for complex models to be realized, and new and improving algorithms are adding to success. Because of that, a number of lofty goals seem feasible within the next decade, including deep reasoning, where AI gains the ability to understand the how and why behind situations; increased training efficiency using smaller data sets; and more efficient and accurate models grown from unsupervised learning. The Potential of Transfer Learning For people, transferable skills increase employability and productivity by making it much easier to get started on a new task. The same applies to transfer learning in AI. However, effective transfer learning still faces a number of challenges. Currently, transfer learning works best in immediately similar domains for the original model, limiting its use. Widening the capabilities of transfer learning will require significantly more compute power and resources to support the greater complexity of retraining. Without innovations in efficiency and processing, it may be easier to simply build a model from scratch.  The Role of Human Oversight in AI Model Training Perhaps the most powerful trait of AI is its ability to perform tasks faster and more accurately than humans, relieving shipping clerks, accountants, and others from performing repetitive tasks. Of course, getting to that point requires time and effort curating data sets, observing outputs, and tweaking the model. How to Choose the Right AI Model Training Tool A variety of AI model training tools can accelerate the development and training process. These tools include prebuilt model libraries, open-source frameworks, coding and environment aides, and gradient boosting. Some rely on the type of model used while others require certain standards for compute resources. To determine which tool, or tools, work best for your project, compile the answers to the following questions: These answers can help build a short list of effective tools to help your AI model training process. OCI Supports Model Training and Parallel Applications Training complex AI models can be a resource-intensive initiative as hundreds, possibly thousands, of independent services coordinate and share information. Oracle Cloud Infrastructure (OCI) provides GPUs connected via a high-performance Ethernet network to save customers time and money while maximizing availability and stability. With OCI, customers get simple, fast interconnects to support training and deployment of highly complex models at scale. The machine learning precursors to AI were built on intensive rules and probability driven by high-powered calculations. The supercomputer Deep Blue competed in world-class chess tournaments that way. However, AI has evolved beyond using rules powered by outside data; instead, AI models now focus on generating internal insights by training through heavy volumes of data sets. While some AI models still use rule-based decision trees, others support complex processes and predictions thanks to neural networks. Advances in AI are exciting, but the future of this technology depends on high-quality training. Enterprises undertaking model training, at whatever level, will want to ensure relevant data sets and institutional knowledge are well documented. One great way to accomplish this is an AI center of excellence, which delivers myriad benefits beyond training support. AI Model Training FAQs What is AI model training? AI model training is the process of feeding an AI model curated data sets to evolve the accuracy of its output. The process may be lengthy, depending on the complexity of the AI model, the quality of the training data sets, and the volume of training data. Once the training process passes a benchmark for expected successes, data scientists continue to monitor results. If accuracy dips or the model has difficulty handling certain types of situations, the model may require further training. Where can I train an AI model? Anyone with access to the proper tools can train an AI model using any PC, assuming they have access to the needed data. The steps include identifying the problem, selecting the training model, finding training data sets, and running the training processes. This can be on a small, local scale or a large enterprise scale depending on the scope of the project and resources available. New or independent developers can take advantage of cloud services that provide CPU resources across a variety of programming languages and remove geography from the equation. How much does it cost to train AI models? The cost of training an AI model depends on the project\u2019s scope. Across the industry, costs continue to trend downward as CPU/GPU power and cloud access provide more resources. In fact, the average training cost for a small project, such as image classification, was $1,000 in 2017 but only $5 in 2022, according to Stanford\u2019s Institute for Human-Centered Artificial Intelligence AI Index. In comparison, the cost for large enterprise AI projects is actually increasing. For example, something like ChatGPT training can require an estimated budget of $3 million to $5 million. This disparity comes down to the complexity of projects and the fact that growing resources make increasingly complex and boundary-pushing projects available\u2014if you can afford them. How to learn AI modeling? To learn how to perform AI model training, formal education or on-the-job-training is required. Once you have the expertise, start with the four steps involved in creating an AI model. What are the four types of AI models? In general, the four types of AI models are the following: Some data scientists also use transfer learning, where an existing AI model is a starting point for a new model, and semi-supervised learning, which melds supervised and unsupervised learning."}
{"url": "https://www.eweek.com/artificial-intelligence/how-to-train-an-ai-model/", "title": "How to Train an AI Model: A Step-by-Step Guide for Beginners", "content": "How to Train an AI Model: A Step-by-Step Guide for Beginners AI models are trained on datasets to learn patterns, make predictions, and assist with decision-making, enabling task automation and personalized recommendations. Learn the key steps, challenges, and best practices for training reliable AI models. \neWEEK content and product recommendations are editorially independent. We may make money when you click on links to our partners. Learn More.\n Knowing how to train an artificial intelligence (AI) model\u2014essentially, making sure it learns the right patterns from the right data\u2014is important if you want it to make accurate and reliable predictions. Appropriately trained AI models can automate tasks, generate personalized recommendations, and reveal insights humans might not be able to find, but models trained incorrectly or trained on biased datasets introduce more problems than they solve. Understanding how to train an AI model, knowing what challenges to prepare for, and applying best practices in model training can help you develop effective AI systems that deliver reliable results. KEY TAKEAWAYS TABLE OF CONTENTS 6 Steps for Training an AI Model Training an AI model involves six chronological steps to ensure that it is well-designed, accurate, and ready for real-world deployment. 1. Prepare the Data The first step in training an AI model is preparing your data by collecting, cleaning, and preprocessing the information you will use to train the model. The quality and relevance of the data will have a significant impact on its performance, making this step critical. The most common methods of gathering data are web scraping, crowdsourcing, open-source data collection, in-house data collection, synthetic data generation, and sensor data collection. The following table gives a brief summary each technique: 2. Select the AI Model Selecting the appropriate AI model involves considering such factors as the size and structure of your dataset, the computational resources available, and the complexity of the problem you\u2019re looking to solve. Some of the most common AI training models include the following: 3. Choose the Training Technique Choosing the right learning technique involves weighing all the same factors as in the model-selection step to optimize the performance of your AI models. The following learning methods can be applied to AI training: 4. Train Your AI Model Feed your prepared data into the model to identify errors and make adjustments to increase accuracy. During this phase, it\u2019s important to be mindful of overfitting, which occurs when your AI model performs well on the training data because of memorization instead of learning, leading to failed interpretation of new, unseen data. 5. Validate Your AI Model Validate the performance of your AI model by evaluating how it performs on a separate and often more complex dataset not used during the training process. This step will aid in revealing overfitting problems and help you determine if the model needs additional training or modification. 6. Test Your AI Model for Readiness The final step is to test your AI model on an independent dataset to assess its real-world applications and make sure it is ready to be used effectively in production. If it performs as expected and delivers correct results based on unstructured data, then it is ready to go live. If not, fine-tune the model by gathering more data, retraining, and retesting it to enhance its precision. 5 Challenges in AI Model Training Training an AI model requires overcoming numerous challenges that might affect its reliability and effectiveness. From obtaining and maintaining data quality to addressing infrastructure demands and skills shortages, each aspect of the AI model training process presents unique hurdles. Acquiring Data and Managing Quality Obtaining and maintaining sufficient and high-quality data is difficult due to the need for large datasets that aptly represent the problem domain. In addition, data cleaning and preprocessing, which are necessary for consistency, can be time consuming. Ensuring Data Privacy and Security Ensuring data privacy and security is increasingly difficult with the rise of strict data protection laws that require stringent data protection measures to safeguard sensitive data throughout the AI training process. Understanding AI Model Functions As AI models become more advanced, it becomes more challenging to understand how they make decisions\u2014especially in sensitive domains like healthcare and finance. Developing methods to trace predictions and interpret outputs is complicated. Meeting Infrastructure Requirements Meeting the substantial computational resources for training AI models\u2014like powerful hardware and scalable cloud infrastructure, for example\u2014can be resource-intensive and expensive. Maintaining Regulatory and Ethical Compliance Compliance with regulatory laws, such as GDPR, requires strict data handling practices. In addition, ethical considerations that demand fair, transparent models that avoid bias and discrimination add to the complexity of the development process. Best Practices for AI Model Training The following best practices for training an AI model can help make sure that your AI systems perform at the highest levels of effectiveness: Frequently Asked Questions (FAQs) How Long Does It Take to Train an AI Model? AI model training times can vary widely, ranging from a few hours to several weeks. Factors such as model complexity, dataset size, computational resources, and the particular task being carried out directly affect the time it takes to train an AI model. Can You Earn Money by Training AI? Yes, you can earn money by training AI models. Common job titles for AI model training professionals include Machine Learning Engineer, Data Scientist, AI/ML Specialist, and AI Trainer. Bottom Line: Knowing How to Train an AI Model Leads to Powerful AI Systems Mastering how to train an AI model is an indispensable component of building an effective AI system. Following a structured process that includes data preparation, model and method selection, training, validation, and testing ensures that your models function accurately in real-world applications. Overcoming challenges such as data quality, privacy, and infrastructure requirements is needed to maintain the integrity of your AI model. Implementing thorough validation, continuous improvement, and other best practices elevates your model\u2019s capability. By following each step in AI model training meticulously, you can develop powerful AI systems that bring significant value across various domains, leading to impactful AI solutions. Discover the key players in the AI industry in our Top 150 AI Companies of 2024 article and gain insights into their noteworthy innovations. \n                    Get the Free Newsletter!                 \n                    Subscribe to Daily Tech Insider for top news, trends & analysis                 \n                    Get the Free Newsletter!                 \n                    Subscribe to Daily Tech Insider for top news, trends & analysis                 MOST POPULAR ARTICLES 9 Best Artificial Intelligence (AI) 3D Generators... RingCentral Expands Its Collaboration Platform 8 Best AI Data Analytics Software &... Zeus Kerravala on Networking: Multicloud, 5G, and... Datadog President Amit Agarwal on Trends in... eWeek has the latest technology news and analysis, buying guides, and product reviews for IT professionals and technology buyers. The site\u2019s focus is on innovative solutions and covering in-depth technical content. eWeek stays on the cutting edge of technology news and IT trends through interviews and expert analysis. Gain insight from top innovators and thought leaders in the fields of IT, business, enterprise software, startups, and more. Advertisers Advertise with TechnologyAdvice on eWeek and our other IT-focused platforms. Menu Our Brands Property of TechnologyAdvice.\n\u00a9 2024 TechnologyAdvice. All Rights Reserved\nAdvertiser Disclosure: Some of the products that appear on this site are from companies from which TechnologyAdvice receives compensation. This compensation may impact how and where products appear on this site including, for example, the order in which they appear. TechnologyAdvice does not include all companies or all types of products available in the marketplace."}
{"url": "https://cloud.google.com/vertex-ai/docs/training-overview", "title": "Train and use your own models \u00a0|\u00a0 Vertex AI \u00a0|\u00a0 Google Cloud", "content": "\n      Train and use your own models\n      \n     For help on deciding which of these methods to use, see\nChoose a training method. AutoML Machine learning (ML) models use training data to learn how to infer results\nfor data that the model was not trained on. AutoML on Vertex AI\nlets you build a code-free model based on the training data that you provide. Types of models you can build using AutoML The types of models you can build depend on the type of data that you have.\nVertex AI offers AutoML solutions for the following data types and\nmodel objectives: The workflow for training and using an AutoML model is the same, regardless of\nyour datatype or objective:  Image data AutoML uses machine learning to analyze the content of image data. You\ncan use AutoML to train an ML model to classify image data or find\nobjects in image data. Vertex AI lets you get online predictions and batch predictions from\nyour image-based models. Online predictions are synchronous\nrequests made to a model endpoint. Use online predictions when you are making\nrequests in response to application input or in situations that require timely\ninferences. Batch predictions are asynchronous requests. You request batch\npredictions directly from the model resource without needing to deploy the model\nto an endpoint. For image data, use batch predictions when you don't require an\nimmediate response and want to process accumulated data by using a single\nrequest. Classification for images A classification model analyzes image data and returns a list of content\ncategories that apply to the image. For example, you can train a model that\nclassifies images as containing a cat or not containing a cat, or you could\ntrain a model to classify images of dogs by breed. Documentation: Prepare data | Create dataset | Train model | Evaluate model | Get predictions | Interpret results To see an example of how to train an image classification model and get batch predictions as part of a more comprehensive workflow,\n      run the \"AutoML training image classification model for batch prediction\" Jupyter notebook in one of the following\n      environments:\n     Open\nin Colab\n\n        \n      \n         | \n        \nOpen\nin Colab Enterprise\n\n        \n      \n         | \n        \nOpen\nin Vertex AI Workbench user-managed notebooks\n\n        \n      \n         | \n        \nView on GitHub Object detection for images An object detection model analyzes your image data and returns annotations\nfor all objects found in an image, consisting of a label and bounding box\nlocation for each object. For example, you can train a model to find the\nlocation of the cats in image data. Documentation: Prepare data | Create dataset | Train model | Evaluate model | Get predictions | Interpret results To see an example of how to train an object detection model and get online predictions as part of a more comprehensive workflow,\n      run the \"AutoML training image object detection model for online prediction\" Jupyter notebook in one of the following\n      environments:\n     Open\nin Colab\n\n        \n      \n         | \n        \nOpen\nin Colab Enterprise\n\n        \n      \n         | \n        \nOpen\nin Vertex AI Workbench user-managed notebooks\n\n        \n      \n         | \n        \nView on GitHub To see an example of how to train an object detection model and get batch predictions as part of a more comprehensive workflow,\n      run the \"AutoML training image object detection model for batch prediction\" Jupyter notebook in one of the following\n      environments:\n     Open\nin Colab\n\n        \n      \n         | \n        \nOpen\nin Colab Enterprise\n\n        \n      \n         | \n        \nOpen\nin Vertex AI Workbench user-managed notebooks\n\n        \n      \n         | \n        \nView on GitHub Tabular data Vertex AI lets you perform machine learning with tabular data\nusing simple processes and interfaces. You can create the following model types\nfor your tabular data problems: To learn more, see Tabular data overview. If your tabular data is stored in BigQuery ML, you can\ntrain an AutoML tabular model directly in BigQuery ML.\nTo learn more, see\nAutoML Tabular reference documentation. Text data AutoML uses machine learning to analyze the structure and meaning of\ntext data. You can use AutoML to train an ML model to classify text\ndata, extract information, or understand the sentiment of the authors. Vertex AI lets you get online predictions and batch predictions from\nyour text-based models. Online predictions are synchronous\nrequests made to a model endpoint. Use online predictions when you are making\nrequests in response to application input or in situations that require timely\ninferences. Batch predictions are asynchronous requests. You request batch\npredictions directly from the model resource without needing to deploy the model\nto an endpoint. For text data, use batch predictions when you don't require an\nimmediate response and want to process accumulated data by using a single\nrequest. Classification for text A classification model analyzes text data and returns a list of categories\nthat apply to the text found in the data. Vertex AI offers both\nsingle-label and multi-label text classification models. Documentation: Prepare data | Create dataset | Train model | Evaluate model | Get predictions | Interpret results To see an example of how to create, train, and deploy a text classification model as part of a more comprehensive workflow,\n      run the \"Create, train, and deploy an AutoML text classification model\" Jupyter notebook in one of the following\n      environments:\n     Open\nin Colab\n\n        \n      \n         | \n        \nOpen\nin Colab Enterprise\n\n        \n      \n         | \n        \nOpen\nin Vertex AI Workbench user-managed notebooks\n\n        \n      \n         | \n        \nView on GitHub Entity extraction for text An entity extraction model inspects text data for known entities referenced\nin the data and labels those entities in the text. Documentation: Prepare data | Create dataset | Train model | Evaluate model | Get predictions | Interpret results To see an example of how to train an entity extraction model and get online predictions as part of a more comprehensive workflow,\n      run the \"AutoML training text entity extraction model for online prediction\" Jupyter notebook in one of the following\n      environments:\n     Open\nin Colab\n\n        \n      \n         | \n        \nOpen\nin Colab Enterprise\n\n        \n      \n         | \n        \nOpen\nin Vertex AI Workbench user-managed notebooks\n\n        \n      \n         | \n        \nView on GitHub To see an example of how to train an entity extraction model and get batch predictions as part of a more comprehensive workflow,\n      run the \"AutoML training text entity extraction model for batch prediction\" Jupyter notebook in one of the following\n      environments:\n     Open\nin Colab\n\n        \n      \n         | \n        \nOpen\nin Colab Enterprise\n\n        \n      \n         | \n        \nOpen\nin Vertex AI Workbench user-managed notebooks\n\n        \n      \n         | \n        \nView on GitHub Sentiment analysis for text A sentiment analysis model inspects text data and identifies the prevailing\nemotional state within it, especially to determine a writer's attitude as\npositive, negative, or neutral. Documentation: Prepare data | Create dataset | Train model | Evaluate model | Get predictions | Interpret results To see an example of how to train a sentiment analysis model and get online predictions as part of a more comprehensive workflow,\n      run the \"Training an AutoML text sentiment analysis model for online predictions\" Jupyter notebook in one of the following\n      environments:\n     Open\nin Colab\n\n        \n      \n         | \n        \nOpen\nin Colab Enterprise\n\n        \n      \n         | \n        \nOpen\nin Vertex AI Workbench user-managed notebooks\n\n        \n      \n         | \n        \nView on GitHub To see an example of how to train a sentiment analysis model and get batch predictions as part of a more comprehensive workflow,\n      run the \"AutoML training text sentiment analysis model for batch prediction\" Jupyter notebook in one of the following\n      environments:\n     Open\nin Colab\n\n        \n      \n         | \n        \nOpen\nin Colab Enterprise\n\n        \n      \n         | \n        \nOpen\nin Vertex AI Workbench user-managed notebooks\n\n        \n      \n         | \n        \nView on GitHub Video data AutoML uses machine learning to analyze video data to classify shots\nand segments, or to detect and track multiple objects in your video data. Action recognition for videos An action recognition model analyzes your video data and returns a list of\ncategorized actions with the moments that the actions happened. For example, you\ncan train a model that analyzes video data to identify the action moments\ninvolving a soccer goal, a golf swing, a touchdown, or a high five. Documentation: Prepare data | Create dataset | Train model | Evaluate model | Get predictions | Interpret results To see an example of how to train an action recognition model as part of a more comprehensive workflow,\n      run the \"AutoML training video action recognition model for batch prediction\" Jupyter notebook in one of the following\n      environments:\n     Open\nin Colab\n\n        \n      \n         | \n        \nOpen\nin Colab Enterprise\n\n        \n      \n         | \n        \nOpen\nin Vertex AI Workbench user-managed notebooks\n\n        \n      \n         | \n        \nView on GitHub Classification for videos A classification model analyzes your video data and returns a list of\ncategorized shots and segments. For example, you could train a model that\nanalyzes video data to identify if the video is of a baseball, soccer,\nbasketball, or football game. Documentation: Prepare data | Create dataset | Train model | Evaluate model | Get predictions | Interpret results To see an example of how to train a video classification model as part of a more comprehensive workflow,\n      run the \"AutoML training video classification model for batch prediction\" Jupyter notebook in one of the following\n      environments:\n     Open\nin Colab\n\n        \n      \n         | \n        \nOpen\nin Colab Enterprise\n\n        \n      \n         | \n        \nOpen\nin Vertex AI Workbench user-managed notebooks\n\n        \n      \n         | \n        \nView on GitHub Object tracking for videos An object tracking model analyzes your video data and returns a list of\nshots and segments where these objects were detected. For example, you could\ntrain a model that analyzes video data from soccer games to identify and track\nthe ball. Documentation: Prepare data | Create dataset | Train model | Evaluate model | Get predictions | Interpret results To see an example of how to train an object tracking model as part of a more comprehensive workflow,\n      run the \"AutoML training video object tracking model for batch prediction\" Jupyter notebook in one of the following\n      environments:\n     Open\nin Colab\n\n        \n      \n         | \n        \nOpen\nin Colab Enterprise\n\n        \n      \n         | \n        \nOpen\nin Vertex AI Workbench user-managed notebooks\n\n        \n      \n         | \n        \nView on GitHub Custom training If none of the AutoML solutions address your needs, you can also create\nyour own training application and use it to train custom models on\nVertex AI. You can use any ML framework that you want and configure the\ncompute resources to use for training, including the following: To learn more about custom training on Vertex AI, see\nCustom training overview. Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates. Last updated 2024-11-26 UTC. Why Google Products and pricing Solutions Resources Engage"}
{"url": "https://builtin.com/articles/feature-engineering", "title": "Feature Engineering Explained | Built In", "content": "Feature Engineering Explained \n                    Feature engineering is the process of selecting, manipulating and transforming raw data into features that can be used in supervised learning.\n                 Feature engineering is the process of selecting, manipulating and transforming raw data into features that can be used in supervised learning. It\u2019s also necessary to design and train new machine learning features so it can tackle new tasks. A \u201cfeature\u201d is any measurable input that can be used in a predictive model. It could be the color of an object or the sound of someone\u2019s voice. Feature engineering is the act of converting raw observations into desired features using statistical or machine learning approaches. Feature Engineering Definition Feature engineering is the process of selecting, manipulating and transforming raw data into features that can be used in supervised learning. It consists of five processes: feature creation, transformations, feature extraction, exploratory data analysis and benchmarking.\u00a0 In this article we\u2019ll cover: \u00a0 What Is Feature Engineering? Feature engineering is a machine learning technique that leverages data to create new variables that aren\u2019t in the training set. It can produce new features for both supervised and unsupervised learning, with the goal of simplifying and speeding up data transformations while also enhancing model accuracy. Feature engineering is required when working with machine learning models. Regardless of the data or architecture, a terrible feature will have a direct impact on your model. To better understand it, let\u2019s look at a simple example. Below are the prices of properties in x city. It shows the area of the house and total price. This data might have some errors or might be incorrect but not all sources on the internet are correct. To begin, we\u2019ll add a new column to display the cost per square foot. This new feature will help us understand a lot about our data. So, we have a new column which shows cost per square foot. There are three main ways you can find any error. You can use Domain Knowledge to contact a property advisor or real estate agent and show him the per square foot rate. If your counsel states that pricing per square foot can\u2019t be less than $3,400, you may have a problem. The data can be visualized this way: When you plot the data, you\u2019ll notice that one price is significantly different from the rest. In the visualization method, you can readily notice the problem. The third way is to use statistics to analyze your data and find any problem. More on Machine LearningMachine Learning Basics Every Beginner Should Know \u00a0 Feature Engineering Processes Feature engineering consists of various processes: Now, let\u2019s have a look at why we need feature engineering in machine learning. \u00a0 Importance of Feature Engineering Feature engineering is a very important step in machine learning. Feature engineering refers to the process of designing artificial features into an algorithm. These artificial features are then used by that algorithm in order to improve its performance, or in other words, reap better results. Data scientists spend most of their time with data, and it becomes important to make models accurate. When feature engineering activities are done correctly, the resulting data set is optimal and contains all of the important factors that affect the business problem. As a result of these data sets, the most accurate predictive models and the most useful insights are produced. \u00a0 Feature Engineering Techniques for Machine Learning Lets review a few feature engineering best techniques that you can use. Some of the techniques listed may work better with certain algorithms or data sets, while others may be useful in all situations.\n\u00a0 1. Imputation Missing values are one of the most common issues you\u2019ll come across when it comes to preparing your data for machine learning. Human error, data flow interruptions, privacy concerns and other factors could all contribute to missing values. Missing values have an impact on the performance of machine learning models. The main goal of imputation is to handle these missing values. There are two types of imputation: 1. Numerical Imputation\u00a0 To figure out what numbers should be assigned to people currently in the population, we usually use data from completed surveys or censuses. These data sets can include information about how many people eat different types of food, whether they live in a city or country with a cold climate and how much they earn every year. That is why numerical imputation is used to fill gaps in surveys or censuses when certain pieces of information are missing. 2. Categorical Imputation\u00a0 When dealing with categorical columns, replacing missing values with the highest value in the column is a smart solution. However, if you believe the values in the column are evenly distributed and there is no dominating value, imputing a category like \u201cOther\u201d would be a better choice, as your imputation is more likely to converge to a random selection in this scenario. \u00a0 2. Handling Outliers Outlier handling is a technique for removing outliers from a data set. This method can be used on a variety of scales to produce a more accurate data representation. This has an impact on the model\u2019s performance. Depending on the model, the effect could be large or minimal. For example, linear regression is particularly susceptible to outliers. This procedure should be completed prior to model training. The various methods of handling outliers include: \u00a0 3. Log Transform Log transform is the most used technique among data scientists. It\u2019s mostly used to turn a skewed distribution into a normal or less-skewed distribution. We take the log of the values in a column and utilize those values as the column in this transform. It\u2019s used to handle confusing data, and the data becomes more approximative to normal applications. \u00a0 4. One-Hot Encoding A one-hot encoding is a type of encoding in which an element of a finite set is represented by the index in that set, where only one element has its index set to \u201c1\u201d and all other elements are assigned indices within the range [0, n-1]. In contrast to binary encoding schemes, where each bit can represent two values (i.e. 0 and 1), this scheme assigns a unique value for each possible case. \u00a0 5. Scaling Feature scaling is one of the most pervasive and difficult problems in machine learning, yet it\u2019s one of the most important things to get right. In order to train a predictive model, we need data with a known set of features that needs to be scaled up or down as appropriate. After a scaling operation, the continuous features become similar in terms of range. Although this step isn\u2019t required for many algorithms, it\u2019s still a good idea to do so. Distance-based algorithms like k-nearest neighbor and k-means, on the other hand, require scaled continuous features as model input. There are two common ways for scaling: 1. Normalization\u00a0 All values are scaled in a specified range between 0 and 1 via normalization (or min-max normalization). This modification has no influence on the feature\u2019s distribution, however, it does exacerbate the effects of outliers due to lower standard deviations. As a result, it\u2019s advised that outliers be dealt with prior to normalization. 2. Standardization\u00a0 Standardization, or z-score normalization, is the process of scaling values while accounting for standard deviation. If the standard deviation of features differs, the range of those features will differ. The effect of outliers in the characteristics is reduced as a result. To arrive at a distribution with a 0 mean and 1 variance, all the data points are subtracted by their mean and the result divided by the distribution\u2019s variance. \u00a0 Best Feature Engineering Tools to Know There are many tools which will help you in automating the entire feature engineering process and producing a large pool of features in a short period of time for both classification and regression tasks. So, let\u2019s have a look at some of the best feature engineering tools.\n\u00a0 1. FeatureTools FeatureTools is a framework to perform automated feature engineering. It excels at transforming temporal and relational data sets into feature matrices for machine learning. FeatureTools integrates with the machine learning pipeline-building tools you already have. You can load in Pandas DataFrames and automatically construct significant features in a fraction of the time it would take to do it manually. FeatureTools Summary \u00a0 2. AutoFeat AutoFeat helps to perform linear prediction models with automated feature engineering and selection. AutoFeat allows you to select the units of the input variables in order to avoid the construction of physically nonsensical features. AutoFeat Summary \u00a0 3. TsFresh TsFresh is a Python package. It automatically calculates a huge number of time series characteristics or features. In addition, the package includes methods for assessing the explanatory power and significance of such traits in regression and classification tasks. TsFresh Summary \u00a0 4. OneBM OneBM interacts directly with a database\u2019s raw tables. It slowly joins the tables, taking different paths on the relational tree. It recognizes simple data types like numerical or categorical, and complicated data types like set of numbers, set of categories, sequences, time series and texts in the joint results and applies pre-defined feature engineering approaches to the supplied types. OneBM Summary \u00a0 5. ExploreKit Based on the idea that extremely informative features are typically the consequence of manipulating basic ones, ExploreKit identifies common operators to alter each feature independently or combine multiple of them. Instead of running feature selection on all developed features, which can be quite huge, meta learning is used to rank candidate features. More on Machine Learning5 Anomaly Detection Algorithms to Know \u00a0 Advantages of Feature Engineering Can Help Find New, Relevant Features Feature engineering is the development of new data features from raw data. With this technique, engineers analyze the raw data and potential information in order to extract a new or more valuable set of features. These new features can supplement or be used in lieu of original data features, and offer a bigger picture of population or behavior characteristics, for example. This makes machine learning model predictions more relevant to the problem it\u2019s intending to solve. \u00a0 Can Enhance Model Accuracy and Insights Feature engineering can be seen as a generalization of mathematical optimization. Creating or manipulating features with feature engineering can provide additional understanding to given data. As such, this can improve machine learning model accuracy, and uncover more useful insights when applying the model for data analytics. Frequently Asked Questions What are the 4 processes of feature engineering? The four main processes of feature engineering include:\u00a0 Why is feature engineering so difficult? Feature engineering requires technical knowledge about machine learning models, algorithms, coding and data engineering in order to use it effectively. When done manually, feature engineering can also be time-consuming and labor-intensive, as features often need to be explored and tested to determine which ones are most valuable. What is feature engineering vs. feature selection? Feature engineering involves creating new features or transforming features from raw data for machine learning model input. Feature selection involves selecting relevant features (from raw data or engineered features) for model input. Feature selection is one kind of process in feature engineering. What are some examples of feature engineering? One example of feature engineering includes having to use a machine learning model to predict housing prices, though you are only given data about the sizes of houses. Feature engineering can be used to add features such as home location, number of bedrooms and date of construction to provide a more accurate price prediction. Another example includes predicting how likely a presidential candidate will win in an upcoming election. If data given only includes candidates' political party and age, new features such as candidate gender, education level and number of delegates could be added through feature engineering to improve model accuracy. Recent Artificial Intelligence Articles"}
{"url": "https://www.analyticsvidhya.com/blog/2021/10/a-beginners-guide-to-feature-engineering-everything-you-need-to-know/", "title": "What is Feature Engineering in Machine Learning?", "content": "Mastering Python\u2019s Set Difference: A Game-Changer for Data Wrangling Reading list \n\n                Basics of Machine Learning            \n \n\n                Machine Learning Lifecycle            \n \n\n                Importance of Stats and EDA            \n \n\n                Understanding Data            \n \n\n                Probability            \n \n\n                Exploring Continuous Variable            \n \n\n                Exploring Categorical Variables            \n \n\n                Missing Values and Outliers            \n \n\n                Central Limit theorem            \n \n\n                Bivariate Analysis Introduction            \n \n\n                Continuous - Continuous Variables            \n \n\n                Continuous Categorical            \n \n\n                Categorical Categorical            \n \n\n                Multivariate Analysis            \n \n\n                Different tasks in Machine Learning            \n \n\n                Build Your First Predictive Model            \n \n\n                Evaluation Metrics            \n \n\n                Preprocessing Data            \n \n\n                Linear Models            \n \n\n                KNN            \n \n\n                Selecting the Right Model            \n \n\n                Feature Selection Techniques            \n \n\n                Decision Tree            \n \n\n                Feature Engineering            \n \n\n                Naive Bayes            \n \n\n                Multiclass and Multilabel            \n \n\n                Basics of Ensemble Techniques            \n \n\n                Advance Ensemble Techniques            \n \n\n                Hyperparameter Tuning            \n \n\n                Support Vector Machine            \n \n\n                Advance Dimensionality Reduction            \n \n\n                Unsupervised Machine Learning Methods            \n \n\n                Recommendation Engines            \n \n\n                Improving ML models            \n \n\n                Working with Large Datasets            \n \n\n                Interpretability of Machine Learning Models            \n \n\n                Interpretability of Machine Learning Models            \n \n\n                Automated Machine Learning            \n \n\n                Model Deployment            \n \n\n                Deploying ML Models            \n \n\n                Embedded Devices            \n Introduction to Feature Engineering \u2013 Everything You Need to Know! Overview Say, you were setting up a gift shop and your supplier dumps all the toys that you asked for in a room. It\u2019s going to look something like this. Total chaos! Now picture yourself standing in front of this huge pile of toys trying to find the right toy for a customer! You know it is in there, but you just don\u2019t know where to look for it! Frustrating right? In a second scenario, you first organize the toys before opening up the shop. You might want to group the toys into categories or you might even choose to replace some broken toys with newer ones. You might even realize some toys that you had asked for were missing and take the necessary actions. That sounds like a more organized and sensible approach, right? Well, when we build Machine Learning models, in most cases the data we deal with looks like this unorganized chaos of toys. This data needs to be cleaned up and pre-processed before it can be put to use and that is where Feature Engineering comes into play. It is a process that aims to bring order to chaos. In this article, you will explore machine learning feature engineering, understand what is feature engineering, and discover the benefits of feature engineering in machine learning to enhance model performance. Key topics covered include: So let\u2019s dive right in and have a look at the topics we are going to cover! This article was published as a part of the\u00a0Data Science Blogathon Table of contents What is Feature Engineering? Feature engineering is a machine learning method that uses data to create new variables that are not included in the training set. It can generate new features for both supervised and unsupervised learning. The main aim is to make data transformations easier and faster while improving the accuracy of the model. Feature Engineering encapsulates various data engineering techniques such as selecting relevant features, handling missing data, encoding the data, and normalizing it. It is one of the most crucial tasks and plays a major role in determining the outcome of a model. In order to ensure that the chosen algorithm can perform to its optimum capability, it is important to engineer the features of the input data effectively. Why is Feature Engineering so important? Do you know what takes the maximum amount of time and effort in a Machine Learning workflow? Well to analyze that, let us have a look at this diagram. (Source: https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/) This pie-chart shows the results of a survey conducted by Forbes. It is abundantly clear from the numbers that one of the main jobs of a Data Scientist is to clean and process the raw data. This can take up to 80% of the time of a data scientist. This is where Feature Engineering comes into play. After the data is cleaned and processed it is then ready to be fed into the machine learning models to train and generate outputs. So far we have established that Data Engineering is an extremely important part of a Machine Learning Pipeline, but why is it needed in the first place? To understand that, let us understand how we collect the data in the first place. In most cases, Data Scientists deal with data extracted from massive open data sources such as the internet, surveys, or reviews. This data is crude and is known as raw data. It may contain missing values, unstructured data, incorrect inputs, and outliers. If we directly use this raw, un-processed data to train our models, we will land up with a model having a very poor efficiency. Thus Feature Engineering plays an extremely pivotal role in determining the performance of any machine learning model Benefits of Feature Engineering An effective Feature Engineering implies: Well, cleaning up bulks of raw, unstructured, and dirty data may seem like a daunting task, but that is exactly what this guide is all about. So let\u2019s get started and demystify Feature Engineering! Analyzing The Dataset Features Whenever you get a dataset, I would strongly advise you to first spend some time analyzing the dataset. This will help you get an understanding of the type of features and data you are dealing with. Analyzing the dataset will also help you create a mind map of the feature engineering techniques that you will need to process your data. So let us import the libraries and have a look at our dataset. This is how our dataset looks. Once you have identified the input features and the\u00a0values to be predicted (in our case \u2018Purchased\u2019 is the column to be predicted and the rest are the input features) let us analyze the data we have. We also see that we have a column \u201cName\u201d which plays no role in determining the output of our model. So we can safely exclude it from the training set. This can be done as follows. Output: Output: The variable \u2018x\u2019 shall contain the inputs and the variable \u2018y\u2019 shall contain the outputs. Handling Missing Data \u2013 An important Feature Engineering Step Now let us check if we have any missing data. A neat way to do that would be to display the sum of all the null values in each column of our dataset. The following line of code helps us do just that. Output: This gives us a very clear representation of the total number of missing values present in each column. Now let us see how we can handle these missing values. Deleting the Columns Sometimes there may be certain features in our dataset which contain multiple empty entries or null values. These columns which have a very high number of null values often do not contribute much to the predicted output. In such cases, we may choose to completely delete the column. We can fix a certain threshold value, say 70% or 80%, and if the number of null values exceeds the threshold we may want to delete that particular column from our training dataset. OUTPUT: What this piece of code is doing is basically selecting only those columns which have null values less than the given threshold value. In our example, we see that the \u2018Cars\u2019 column has been removed. The number of null values is 14 and the total number of entries per column is 20. As the number of null values is not less than our desired threshold, we delete the column. BENEFITS DRAWBACKS For easier understanding, we are dealing with a small dataset, however in reality this method is preferred only when the dataset is large and deleting a few columns will not affect it much, or when the column to be deleted is a relatively less important feature. Impute Missing Values for Continuous Variable Imputing Missing Values refers to the process of filling up the missing values with some values computed from the corresponding feature columns. We can use a number of strategies for Imputing the values of Continuous variables. Some such strategies are imputing with Mean, Median or Mode. Let us first display our original variable x. Output: IMPUTING WITH MEAN Now, to do this, we will import\u00a0SimpleImputer\u00a0from\u00a0sklearn.impute and pass our strategy as the parameter. We shall also specify the columns in which this strategy is to be applied using the slicing. Output We see that the nan values have been replaced with the mean values of their corresponding columns. IMPUTING WITH MEDIAN Now, instead of mean if we wish to impute the missing values with median instead of mean, we simply have to change the parameter to \u2018median\u2019. Output IMPUTING WITH MODE One of the most commonly used imputation methods to handle missing values is to substitute the missing values with the most frequent value in the column. In such cases, we impute the missing values with mode. To do this, we simply have to pass \u201cmost_frequent\u201d as our strategy parameter. Output Impute Missing Values for Categorical Variable In our case, our dataset does not have any Categorical Variable with missing values. However, there may be cases when you come across a dataset where you might have to impute the missing values for some categorical variable. To understand how to deal with such a scenario, let us modify our dataset a little and another new categorical \u2018Gender\u2019 which has a few missing entries. This will help us understand how to handle such cases. Our dataset now looks something like this: Now, look carefully at the \u2018Gender\u2019 column. It has \u2018M\u2019, \u2018F\u2019, and missing values (nan) as the entries. There are three main ways to deal with missing Categorical values. We shall discuss each one of them. DROPPING THE ROWS CONTAINING MISSING CATEGORICAL VALUES Observe that all the rows in which the \u2018Gender\u2019 was NAN have been removed from the dataset. Here axis=0 specifies that the rows containing missing values must be removed and the \u2018subset\u2019 parameter contains the list of columns that should be checked for missing values. ASSIGNING A NEW CATEGORY TO THE MISSING CATEGORICAL VALUES Simply deleting the values which are missing, causes loss of information. To avoid that we can also replace the missing values with a new category. For example, we may assign \u2018U\u2019 to the missing genders where \u2018U\u2019 stands for Unknown. Here all the missing values in the \u2018Gender\u2019 column have been replaced with \u2018U\u2019. This method adds information to the dataset instead of causing information loss. IMPUTING CATEGORICAL VARIABLE WITH MOST FREQUENT VALUE Finally, we may also impute the missing value with the most frequent value for that particular column. Yes, you guessed it right! We are going to substitute the mode value in the missing fields. Since in our dataset the category with the highest frequency is \u2018M\u2019, the missing values should be substituted with \u2018M\u2019. Predict the Missing Values We are almost done with the various techniques to handle missing values. We are now down to the last method and that is Prediction Imputation. The intuition behind this method is very simple yet effective. We are going to think of the column having missing values as the dependent variable ( or the y column). The rest of the columns can be the independent variable ( or the x column). Now, we take the completely filled rows as our training set and the missing value containing rows as our test set. Then we simply use a simple Linear regression model or a classification model to predict the missing values. Since this method takes into account the correlation between the missing value column and other columns to predict the missing values, it yields much better results than the previous methods. This is a great strategy to handle missing values. What are Feature Engineering Techniques in ML? Encoding Categorical Data Congratulations! You\u2019re done with all the missing data handling techniques. Now comes one of the most important Feature Engineering steps \u2013\u00a0 Encoding the categorical variables. Let us first understand why this is needed. Our dataset contains fields like \u2018Country\u2019 which have country names such as India, Spain and Belgium. The \u2018Purchased\u2019 column contains Yes or No. We cannot work with these Categorical variables as they are literals. All these non-numeric values must be encoded into a convenient numeric value that can be used to train our model. This is why we need Encoding of Categorical variables. Encoding Independent Variables Let us get back to our original dataset and have a look at our Independent variable x. Our independent variable x contains a categorical variable \u201cCountry\u201d. This field has 3 different values \u2013 India, Spain, and Belgium. So should we encode India, Spain, and Belgium as 0, 1, and 2? This apparently seems to be okay, right? But hold on. There is a catch! The correct answer is NO. We cannot directly encode the 3 countries as 0,1 and 2. This is because, if we encode the countries in this manner then the machine learning model will wrongly assume that there is some sort of sequential relationship between the countries. This will make the model believe that India, Spain, and Belgium have a sequential order like the numbers 0, 1, and 2. This is not true. Hence, we must not feed in the model with such incorrect information. So what is the solution? The solution is to create separate columns for each category of the Categorical variable. Then we assign 1 to the column which is true and 0 to the others. The entire set of columns that represent the Categorical variable shall give us the result without creating any ordinal relationship For our example, we may encode the countries as follows This can be done with the help of One Hot Encoding. The separate columns which are created to represent the categorical variables are known as the Dummy Variables. The fit_transform() method is called from the OneHotEncoder class which creates the dummy variables and assigns them with binary values. Let us have a look at the code. Voila! There we have our Categorical variables beautifully encoded into dummy variables without any ordinal relationship among the various categories. Encoding Dependent Variables Let us now have a look at our dependent variable y. Our dependent variable y is also a categorical variable. However in this case we can simply assign 0 and 1 to the two categories \u2018No\u2019 and \u2018Yes\u2019. In this case, we do not require dummy variables to encode the \u2018Predicted\u2019 variable as it is a dependent variable that will not be used to train the model. To code this, we are going to need the LabelEncoder class. Feature Scaling \u2013 The last step of Feature Engineering Finally, we come to the last step of Feature Engineering \u2013 Feature Scaling. Feature Scaling is the process of scaling or converting all the values in our dataset to a given scale. Some machine learning algorithms like linear regression, logistic regression, etc use gradient descent optimization. Such algorithms require the data to be scaled in order to perform optimally. K Nearest Neighbours, Support Vector Machine, and K-Means clustering also show a drastic rise in performance on scaling the data. There are two main techniques of feature scaling: NORMALIZATION Normalization is the process of scaling the data values in such a way that that the value of all the features lies between 0 and 1. This method works well when the data is normally distributed. STANDARDIZATION Standardization is the process of scaling the data values in such a way that that they gain the properties of standard normal distribution. This means that the data is rescaled in such a way that the mean becomes zero and the data has unit standard deviation. Standardized values do not have a fixed bounded range like Normalised values. Let us have a look at the code. If you do not have separate training and test sets then you can split your dataset into two parts \u2013 one for training and the other for testing. Now we shall import the StandardScaler class to scale all the variables. We observe that all our values have been scaled. This is how Feature Scaling is performed. NOTE: Keep in mind, that while scaling the features, we must only use the independent variables of the training set to compute mean(x) and standard deviation(x). Then these same values of\u00a0 mean(x) and standard deviation(x) of the training set must be used to apply feature scaling to the test set. Conclusion Now our dataset is feature engineered and all ready to be fed into a Machine Learning model. This dataset can now be used to train the model to make the desired predictions. We have effectively engineered all our features. The missing values have been handled, the categorical variables have been effectively encoded and the features have been scaled to a uniform scale. Rest assured, now we can safely sit back and wait for our data to generate some amazing results! Once you have effectively feature engineered all the variables in your dataset, you can be sure to generate models having the best possible efficiency as all the algorithms can now perform to their optimum capabilities. Hope you find this information on machine learning feature engineering helpful! Understanding what is feature engineering and its benefits can significantly enhance your model\u2019s performance. Happy learning! Feature Engineering is the process of extracting, selecting, and transforming raw data into meaningful features that enhance the performance of machine learning models. It involves techniques like handling missing data, encoding categorical variables, and scaling features. Feature Engineering is vital because it significantly impacts a model\u2019s accuracy and efficiency. By cleaning and organizing data, it ensures that the machine learning algorithms can learn and make predictions effectively. Without proper feature engineering, models may perform poorly due to unprocessed or irrelevant data. Missing data can be handled through various methods such as deleting columns or rows with high missing values, imputing missing values using mean, median, or mode, and predicting missing values using other related features. Each method has its pros and cons, depending on the dataset. Common techniques in Feature Engineering include data cleaning and imputation, feature scaling (normalization and standardization), encoding categorical data (one-hot encoding and label encoding), feature creation (deriving new features), and feature selection (choosing the most relevant features). Hey, I'm Tithi Sreemany. I have great interest in technology and am passionate about coding. I also have keen interest in AI and Machine Learning and wish to learn and gain experience in these fields.\nI strongly believe in pairing technology with creativity to create efficient and powerful tools to shape a smarter future. It's so much fun to explore these fields and the possibilities are endless! Free Courses Generative AI - A Way of Life Explore Generative AI for beginners: create text and images, use top AI tools, learn practical skills, and ethics. Getting Started with Large Language Models Master Large Language Models (LLMs) with this course, offering clear guidance in NLP and model training made simple. Building LLM Applications using Prompt Engineering  This free course guides you on building LLM apps, mastering prompt engineering, and developing chatbots with enterprise data. Improving Real World RAG Systems: Key Challenges & Practical Solutions Explore practical solutions, advanced retrieval strategies, and agentic RAG systems to improve context, relevance, and accuracy in AI-driven applications. Microsoft Excel: Formulas & Functions Master MS Excel for data analysis with key formulas, functions, and LookUp tools in this comprehensive course. Recommended Articles \n          Top 100 Data Science Interview Questions &...           \n          Feature Engineering Techniques to follow in Mac...           \n          Step by Step process of Feature Engineering for...           \n          Feature Engineering: How to transform variables...           \n          Complete Guide to Feature Engineering: Zero to ...           \n          A Comprehensive Guide on Feature Engineering           \n          Feature Engineering  (Feature Improvements R...           \n          How to Handle Missing Values of Categorical Var...           \n          A Complete Guide to Dealing with Missing values...           \n          Getting Started with Feature Engineering           Responses From Readers ClearSubmit reply \n\n  \u0394 \n                  Write for us\n                  \n\n\n\n Write, captivate, and earn accolades and rewards for your work \n                    We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our  Privacy Policy & Cookies Policy.\n                 Show details Powered By  \n                            \n Cookies This site uses cookies to ensure that you get the best experience possible. To learn more about how we use cookies, please refer to our Privacy Policy & Cookies Policy. \n\nNecessary (2)\nNecessary cookies help make a website usable by enabling basic functions like page navigation and access to secure areas of the website. The website cannot function properly without these cookies.\n\n Necessary (2) Necessary cookies help make a website usable by enabling basic functions like page navigation and access to secure areas of the website. The website cannot function properly without these cookies. \n\nAnalytics Vidhya (4)\n\n                                                        learn more about analytics vidhya privacy\n                                                    \n\n Analytics Vidhya (4) brahmaid It is needed for personalizing the website. Expiry: Session Type: HTTP csrftoken This cookie is used to prevent Cross-site request forgery (often abbreviated as CSRF) attacks of the website Expiry: Session Type: HTTPS Identityid Preserves the login/logout state of users across the whole site. Expiry: Session Type: HTTPS sessionid Preserves users' states across page requests. Expiry: Session Type: HTTPS \n\nGoogle (1)\n\n                                                        learn more about google privacy\n                                                    \n\n Google (1) g_state Google One-Tap login adds this g_state cookie to set the user status on how they interact with the One-Tap modal. Expiry: 365 days Type: HTTP \n\nStatistics (4)\nStatistic cookies help website owners to understand how visitors interact with websites by collecting and reporting information anonymously.\n\n Statistics (4) Statistic cookies help website owners to understand how visitors interact with websites by collecting and reporting information anonymously. \n\nMicrosoft (7)\n\n                                                        learn more about microsoft policy\n                                                    \n\n Microsoft (7) MUID Used by Microsoft Clarity, to store and track visits across websites. Expiry: 1 Year Type: HTTP _clck Used by Microsoft Clarity, Persists the Clarity User ID and preferences, unique to that site, on the browser. This ensures that behavior in subsequent visits to the same site will be attributed to the same user ID. Expiry: 1 Year Type: HTTP _clsk Used by Microsoft Clarity, Connects multiple page views by a user into a single Clarity session recording. Expiry: 1 Day Type: HTTP SRM_I Collects user data is specifically adapted to the user or device. The user can also be followed outside of the loaded website, creating a picture of the visitor's behavior. Expiry: 2 Years Type: HTTP SM Use to measure the use of the website for internal analytics Expiry: 1 Years Type: HTTP CLID The cookie is set by embedded Microsoft Clarity scripts. The purpose of this cookie is for heatmap and session recording. Expiry: 1 Year Type: HTTP SRM_B Collected user data is specifically adapted to the user or device. The user can also be followed outside of the loaded website, creating a picture of the visitor's behavior. Expiry: 2 Months Type: HTTP \n\nGoogle (7)\n\n                                                        learn more about google privacy\n                                                    \n\n Google (7) _gid This cookie is installed by Google Analytics. The cookie is used to store information of how visitors use a website and helps in creating an analytics report of how the website is doing. The data collected includes the number of visitors, the source where they have come from, and the pages visited in an anonymous form. Expiry: 399 Days Type: HTTP _ga_# Used by Google Analytics, to store and count pageviews. Expiry: 399 Days Type: HTTP _gat_# Used by Google Analytics to collect data on the number of times a user has visited the website as well as dates for the first and most recent visit. Expiry: 1 Day Type: HTTP collect Used to send data to Google Analytics about the visitor's device and behavior. Tracks the visitor across devices and marketing channels. Expiry: Session Type: PIXEL AEC cookies ensure that requests within a browsing session are made by the user, and not by other sites. Expiry: 6 Months Type: HTTP G_ENABLED_IDPS use the cookie when customers want to make a referral from their gmail contacts; it helps auth the gmail account. Expiry: 2 Years Type: HTTP test_cookie This cookie is set by DoubleClick (which is owned by Google) to determine if the website visitor's browser supports cookies. Expiry: 1 Year Type: HTTP \n\nWebengage (2)\n\n                                                        Learn more about webengage privacy\n                                                    \n\n Webengage (2) _we_us this is used to send push notification using webengage. Expiry: 1 Year Type: HTTP WebKlipperAuth used by webenage to track auth of webenagage. Expiry: Session Type: HTTP \n\nLinkedIn (16)\n\n                                                        learn more about linkedin privacy\n                                                    \n\n LinkedIn (16) ln_or Linkedin sets this cookie to registers statistical data on users' behavior on the website for internal analytics. Expiry: 1 Day Type: HTTP JSESSIONID Use to maintain an anonymous user session by the server. Expiry: 1 Year Type: HTTP li_rm Used as part of the LinkedIn Remember Me feature and is set when a user clicks Remember Me on the device to make it easier for him or her to sign in to that device. Expiry: 1 Year Type: HTTP AnalyticsSyncHistory Used to store information about the time a sync with the lms_analytics cookie took place for users in the Designated Countries. Expiry: 6 Months Type: HTTP lms_analytics Used to store information about the time a sync with the AnalyticsSyncHistory cookie took place for users in the Designated Countries. Expiry: 6 Months Type: HTTP liap Cookie used for Sign-in with Linkedin and/or to allow for the Linkedin follow feature. Expiry: 6 Months Type: HTTP visit allow for the Linkedin follow feature. Expiry: 1 Year Type: HTTP li_at often used to identify you, including your name, interests, and previous activity. Expiry: 2 Months Type: HTTP s_plt Tracks the time that the previous page took to load Expiry: Session Type: HTTP lang Used to remember a user's language setting to ensure LinkedIn.com displays in the language selected by the user in their settings Expiry: Session Type: HTTP s_tp Tracks percent of page viewed Expiry: Session Type: HTTP AMCV_14215E3D5995C57C0A495C55%40AdobeOrg Indicates the start of a session for Adobe Experience Cloud Expiry: Session Type: HTTP s_pltp Provides page name value (URL) for use by Adobe Analytics Expiry: Session Type: HTTP s_tslv Used to retain and fetch time since last visit in Adobe Analytics Expiry: 6 Months Type: HTTP li_theme Remembers a user's display preference/theme setting Expiry: 6 Months Type: HTTP li_theme_set Remembers which users have updated their display / theme preferences Expiry: 6 Months Type: HTTP \n\nPreferences (0)\nPreference cookies enable a website to remember information that changes the way the website behaves or looks, like your preferred language or the region that you are in.\n\n Preferences (0) Preference cookies enable a website to remember information that changes the way the website behaves or looks, like your preferred language or the region that you are in. \n\nMarketing (4)\nMarketing cookies are used to track visitors across websites. The intention is to display ads that are relevant and engaging for the individual user and thereby more valuable for publishers and third party advertisers.\n\n Marketing (4) Marketing cookies are used to track visitors across websites. The intention is to display ads that are relevant and engaging for the individual user and thereby more valuable for publishers and third party advertisers. \n\nGoogle (11)\n\n                                                        learn more about google privacy\n                                                    \n\n Google (11) _gcl_au Used by Google Adsense, to store and track conversions. Expiry: 3 Months Type: HTTP SID Save certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search. Expiry: 2 Years Type: HTTP SAPISID Save certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search. Expiry: 2 Years Type: HTTP __Secure-# Save certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search. Expiry: 2 Years Type: HTTP APISID Save certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search. Expiry: 2 Years Type: HTTP SSID Save certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search. Expiry: 2 Years Type: HTTP HSID Save certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search. Expiry: 2 Years Type: HTTP DV These cookies are used for the purpose of targeted advertising. Expiry: 6 Hours Type: HTTP NID These cookies are used for the purpose of targeted advertising. Expiry: 1 Month Type: HTTP 1P_JAR These cookies are used to gather website statistics, and track conversion rates. Expiry: 1 Month Type: HTTP OTZ Aggregate analysis of website visitors Expiry: 6 Months Type: HTTP \n\nFacebook (2)\n\n                                                        learn more about facebook privacy\n                                                    \n\n Facebook (2) _fbp This cookie is set by Facebook to deliver advertisements when they are on Facebook or a digital platform powered by Facebook advertising after visiting this website. Expiry: 4 Months Type: HTTP fr Contains a unique browser and user ID, used for targeted advertising. Expiry: 2 Months Type: HTTP \n\nLinkedIn (6)\n\n                                                        Learn about linkedin policy\n                                                    \n\n LinkedIn (6) bscookie Used by LinkedIn to track the use of embedded services. Expiry: 1 Year Type: HTTP lidc Used by LinkedIn for tracking the use of embedded services. Expiry: 1 Day Type: HTTP bcookie Used by LinkedIn to track the use of embedded services. Expiry: 6 Months Type: HTTP aam_uuid Use these cookies to assign a unique ID when users visit a website. Expiry: 6 Months Type: HTTP UserMatchHistory These cookies are set by LinkedIn for advertising purposes, including: tracking visitors so that more relevant ads can be presented, allowing users to use the 'Apply with LinkedIn' or the 'Sign-in with LinkedIn' functions, collecting information about how visitors use the site, etc. Expiry: 6 Months Type: HTTP li_sugr Used to make a probabilistic match of a user's identity outside the Designated Countries Expiry: 90 Days Type: HTTP \n\nMicrosoft (2)\n\n                                                        Learn more about microsoft privacy.\n                                                    \n\n Microsoft (2) MR Used to collect information for analytics purposes. Expiry: 1 year Type: HTTP ANONCHK Used to store session ID for a users session to ensure that clicks from adverts on the Bing search engine are verified for reporting purposes and for personalisation Expiry: 1 Day Type: HTTP \n\nUnclassNameified (0)\nUnclassNameified cookies are cookies that we are in the process of classNameifying, together with the providers of individual cookies.\n\n UnclassNameified (0) UnclassNameified cookies are cookies that we are in the process of classNameifying, together with the providers of individual cookies. Cookie declaration last updated on 24/03/2023 by Analytics Vidhya. Cookies are small text files that can be used by websites to make a user's experience more efficient. The law states that we can store cookies on your device if they are strictly necessary for the operation of this site. For all other types of cookies, we need your permission. This site uses different types of cookies. Some cookies are placed by third-party services that appear on our pages. Learn more about who we are, how you can contact us, and how we process personal data in our Privacy Policy. Flagship Courses  Free Courses Popular Categories Generative AI Tools and Techniques Popular GenAI Models Data Science Tools and Techniques Company Discover Learn Engage Contribute Enterprise \nTerms & conditions\n\n\n\nRefund Policy\n\n\n\nPrivacy Policy\n\n\n\nCookies Policy\n\u00a9 Analytics Vidhya 2024.All rights reserved.\n \nGenAI \nPinnacle \nProgram\n Revolutionizing AI Learning & Development Enroll with us today! Continue your learning for FREE Enter email address to continue  Enter OTP sent to \n\nEdit Resend OTP Resend OTP in 45s"}
{"url": "https://learn.microsoft.com/en-us/ai/playbook/capabilities/deployment/", "title": "AI model deployment | Microsoft Learn", "content": "This browser is no longer supported. Upgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support. Share via AI model deployment In this article Deploying a model means taking a trained ML model, packaging it (like as a container image or a pipeline), and setting it up for inference. Deployment also involves packaging the data prep code with the model to ensure incoming data matches what the model expects. \ud83d\udca1Key Outcomes of Deployment: Deployment Infrastructure There are several ways to deploy custom ML solutions in the cloud or on the edge. The following options are common Azure-Compatible deployment options: While there are other ways to deploy Docker containers to edge devices, we will focus on the mechanism mentioned above. Model Flighting Details on Model Flighting can be found in the Azure ML official docs Model Release Model Release involves the process of packaging the latest (and ideally best performing) model from the training pipeline,\nand promoting it through to the Production environment. Model packaging options Model processing pipeline for generating a docker image After a model is determined through experimentation by a data scientist, it's time to deploy to an environment. Although it is possible to deploy a model along with its artifacts directly to an environment, a better practice is to configure a docker image with model artifacts. Then, run containers based on the docker image. Docker helps in providing more flexibility to test the model including security scanning, smoke test and publishing it to a container registry. Model promotion using shared registries The Machine Learning Registries for MLOps in Azure ML allows you to register a model once, and easily retrieve it across multiple workspaces (including in different subscriptions). Instead of copying a model to workspaces in each deployment environment, each of those workspaces could refer to the same registry. Then, as a model is tested, its tags are updated to reflect its status. Models ready for production can be deployed directly from the shared registry to the production environment. Automating ML model deployment to production environments In general, there are 2 paths to deploying ML code into Production environments: Any of the deployment options can fit the proposed deployment workflow. CI/CD on the development branch This approach is the most common, where we prepare and store the required artifacts (models and libraries) in advance. To indicate their readiness for production, we assign a special tag to the approved artifacts. For example, if we execute training from our development branch on the full dataset and we are generating a model, the model should be evaluated and approved. Once the model is ready, we can label it with a special attribute, like a \"production tag.\" Then, during the continuous deployment (CD) phase, we use the latest model version with the production tag for deployment. We don't care much about our code in the development branch because we are not planning to use it in production. We only use it to do training in our development environment. Rollback strategy can be implemented with no challenges: we just need to remove tag from the latest model and execute the CI/CD again to pick up the previous version that was ready for production.  In above implementation, it's OK to use just one branch (main, for example) as the primary source for development and executing CI/CD. The following image demonstrates the final version of the CI/CD process:  An example implementation of a CI pipeline for training in Azure ML can be found here in this basic template. This example pipeline executes several steps when a PR is merged into the development branch: The full template repo is available in the following repo. There are more templates based on a similar pattern available at: Multi-branch strategy You can use several branches to move your code from development to production. This strategy works fine if you deploy the code itself, and don\u2019t update the production environment often. Also you have several versions of the production environment for different partners or departments. We cannot treat the development branch as a production ready branch. The branch is stable but moving it to production \"as is\" might cause some issues. For example, we could have updated training pipelines and no model exists based on it yet. Another example, somebody could activate a wrong model to be used in scoring service that is not critical for development branch, but critical for production. Hence, we propose having a flow moving development branch to production using two stages: We use branches rather than tags because it allows us to execute some more DevOps pipelines prior to commit code from one branch to another. In these pipelines, we can implement an approval process and move ML pipelines between environments. The PR Build on the movement from the development to the QA branch should include the following tasks:  Implementing this process, we are working across several branches: Once we finish testing for our QA environment, we can create another PR and start a process to move everything to main. The PR should be approved, and it will trigger the deployment Build. The Build has to update scoring infrastructure in the production environment and clone our model again.  Deployment Artifacts Deployment artifacts in CI/CD process depend on the model usage in the production environment. The most common usage scenarios are batch scoring and runtime scoring: Each scenario defines what components you need in the QA and Production environments. Batch Scoring For batch scoring, model registry and compute pipeline resources must be deployed to infrastructure, such as an Azure ML Workspace, or a Databricks instance. These resources will be used to trigger scoring pipelines (which need to be managed as outlined in ML Pipelines). Implementing a CD pipeline for the production environment generally consists of two core steps: copy the latest ready for production model to QA/Prod environment, and publish the scoring service to the appropriate scoring pipeline (for example, Azure ML Pipeline, or Databricks pipeline).  Alternatively, using the Azure ML registries preview, multiple Workspaces are able to access shared models, components, and environments from a single registry - removing the need to copy the model between each Workspace. These shared registries work even across multiple Azure Subscriptions. Runtime Scoring For a runtime service, a replicated model registry service is an optional component in the QA/Prod environment, and it depends on how you are planning to use the model. Depending on the scenario, several methods have been successful:  Technologies-related matrix Different solutions are often used for training and hosting AI models. For example, models trained on the Azure ML may not be run on Azure ML for inferencing. The following table summarizes options that can work for an Azure ML-managed deployment approach and for a custom deployment approach. The table below summarizes this comparison:  (Click title link to learn more about each type of scoring.) All our inferencing workloads can be divided into three separate scoring groups: Production vs. development subscriptions It is a common situation that development and training take place in one subscription, but deployment of the inferencing service into production is taking place in another subscription. NOTE: Batch Online Endpoint and Online Endpoints cannot be deployed outside the subscription where Azure ML is located. Thus, if you want to use Online Endpoints, you need to deploy a separate instance of Azure ML Workspace, copy your model into this workspace during the deployment and execute the deployment from there. A separate Azure ML Workspace is required for Azure ML Pipelines as well. For more information More generalized content outlining different Azure compute options available can be found here: Feedback Was this page helpful? Additional resources Additional resources In this article"}
{"url": "https://www.projectpro.io/article/machine-learning-model-deployment/872", "title": "Machine Learning Model Deployment- A Beginner\u2019s Guide", "content": "Project Library   Courses  Custom Project Path  Resources  Machine Learning Model Deployment-A Beginner\u2019s Guide From prototyping to production, learn the ins and outs of machine learning model deployment with our comprehensive tutorial. | ProjectPro \n Master the art of ML model deployment with our detailed tutorial. Learn best practices, tools, and a step-by-step approach to deploy your ML models effectively for real-world applications. MLOps Project to Deploy Resume Parser Model on Paperspace   Downloadable solution code | Explanatory videos | Tech Support  Imagine you are a renowned chef known for creating mouthwatering recipes at a famous restaurant. You have spent countless hours perfecting a new dish, but keeping it confined to your kitchen won't satisfy hungry diners or earn you the deserved recognition. In the world of machine learning, it's no different! Developing a powerful model is just the first step in machine learning projects; the real magic lies in its deployment. This comprehensive blog dives into the art of machine learning model deployment and the different tools and best practices you need to deploy ML models in your data science projects. So, let\u2019s get started!  Table of Contents What is Model Deployment in Machine Learning? Model deployment in machine learning means integrating a trained machine-learning model into a real-world system or application to automatically generate predictions or perform specific tasks. For example, imagine a healthcare company developing a model to predict the chances of readmission for patients with chronic diseases. Model deployment would involve taking the trained model and implementing it within the company's existing electronic health record system. Once deployed, the model can analyze patient data in real-time, offering insights to healthcare professionals to help them identify high-risk patients and take proactive measures to avoid patient readmissions. Let us now move on and further explore how to deploy ML models in production using various frameworks.  Machine Learning Model Deployment Tutorial This section will explore the step-by-step process of various approaches to deploying machine learning models using popular frameworks like Python, Flask, Django, and Streamlit. Whether you want to build RESTful APIs with Flask, create scalable web applications using Django, or develop interactive interfaces using Streamlit, this section will help you understand how to successfully deploy your machine learning models and make them accessible to users. Let's dive in and learn how to bring your models to life in production environments.  Learn ML Model Deployment in Python There are several programming languages used for ML model deployment, but this section mainly focuses on deploying a machine-learning model in Python. We will walk you through each step of deploying a machine learning model in detail, from preprocessing the data and training the model to serializing it and deploying it as an API. Let's consider an example of deploying a sentiment analysis model using FastAPI in Python. 1. Data Preprocessing The first step is handling missing values using techniques like mean imputation or more advanced methods like regression imputation. Then, you must perform feature scaling, such as standardization or normalization, to ensure all features are on a similar scale. You must encode categorical variables using methods like one-hot encoding or label encoding. In this example, you can start by preprocessing the text data for sentiment analysis. Also, remove any unwanted characters, perform tokenization, and apply techniques like stemming or lemmatization.  Begin Your Big Data Journey with ProjectPro's Project-Based PySpark Online Course! \n    Here's what valued users are saying about ProjectPro\n \n                    Gautam Vermani \n                 \n                    Data Consultant at Confidential                 \n                    Jingwei Li \n                 \n                    Graduate Research assistance at Stony Brook University                 \n        Not sure what you are looking for?\n     2. Model Optimization and Training The second step involves selecting a suitable machine learning algorithm based on your problem, such as a Random Forest Classifier. You must split the data into training and testing sets using train_test_split from scikit-learn. Using techniques like grid or random search, you can quickly train the model using the training data and optimize it by tuning hyperparameters, such as the number of trees in a Random Forest. In this sentiment analysis example, you will train the sentiment analysis model using a suitable algorithm like Naive Bayes or Support Vector Machine. You will also optimize the model's hyperparameters using techniques like cross-validation.  3. Model Serialization Once you are satisfied with the performance of your trained sentiment analysis model, it's crucial to serialize and save it for any future use. Serialization will convert the model into a binary format that can be stored on disk. The joblib library is commonly used for model serialization in Python. You can save the SVM Classifier model using the dump function, providing the model object and the file path where you want to save it, e.g., joblib.dump(model, 'model.pkl').   New Projects  4. Prepare the Deployment Environment The next step is installing the required libraries, such as FastAPI, to create a web API. To deploy your model, you need to set up the deployment environment. You must create a virtual environment to manage the dependencies and install the necessary packages using pip. This ensures that your deployment environment is isolated and has the required packages installed, avoiding conflicts with other Python projects. In this example, you will create a new FastAPI application and install the required dependencies.\u00a0  5. Build The Deployment API For building the deployment API for the Sentiment Analysis example, you must create a Python script to define the API endpoint. This script will handle incoming requests, preprocess the data, and make predictions using your trained SVM Classifier model. You will then load the serialized model into memory using joblib.load and preprocess incoming data to match the model's input requirements.\u00a0  Finally, you will use the loaded model to generate predictions and return the outcomes as shown in the following code-  6. Test And Validate The Deployment To ensure your deployment is functioning correctly, testing and validating it thoroughly is crucial. This step involves sending sample requests to the API using tools like requests or curl and verifying the output against expected results during model monitoring. You must further compare the predictions made by the deployment with those made during model development to ensure consistency and accuracy across multiple models.  7. Deploy The ML Model You must choose a server or cloud platform to deploy your FastAPI application, such as Heroku, AWS, or Azure, as they provide the necessary infrastructure to host your deployment. You must also configure the server/cloud environment to handle incoming requests and route them to the API endpoint you defined in the previous step. When deploying your ML model in a production environment, you must always follow best practices for security, scalability, and availability.  8. Monitor And Maintain The Deployment After deployment, monitoring and maintaining the model's performance continuously is crucial. You must implement logging and monitoring mechanisms to track API usage, performance metrics, and potential errors in existing or new models. You should regularly evaluate the model's performance and retrain it if necessary. Furthermore, you must update the deployment as needed, such as incorporating new model versions or enhancing the API to handle increasing traffic. Monitoring and maintaining the deployment through continuous delivery ensures that your model provides accurate and reliable predictions in real-world scenarios. You can better understand how to deploy ML models in Python by working on the \u2018AWS MLOps Project to Deploy Multiple Linear Regression Model\u2019 project. Now that you have understood how to deploy an ML model in Python using FastAPI, let us further explore this process using Flask, Django, and Streamlit. Kickstart your journey in the exciting domain of Data Science with these solved data science mini projects today! Machine Learning Model Deployment Using Flask Here are the detailed steps for Flask Machine Learning model deployment- 1. Data Preprocessing And Preparation- First, you must preprocess your data as needed, including handling missing values, feature scaling, and encoding categorical variables. 2. Model Training And Optimization- You will select a suitable machine learning algorithm, split your data into training and testing datasets, and train the model using the training data. You need to optimize the model's hyperparameters to achieve better performance. 3. Model Serialization- You can easily serialize and save the trained model using libraries like joblib or pickle. This allows you to load the model later without retraining. 4. Set Up a Flask Application- The next step is to create a new Flask application by importing the Flask module and creating an instance of the Flask class. 5. Define an API Endpoint- You will then define a Flask application route corresponding to the API endpoint. This endpoint will receive input data and return predictions. 6. Model Loading- In the API endpoint function, you will load the serialized model into memory using joblib or pickle. 7. Incoming Data Preprocessing- Once you have successfully loaded the model, it\u2019s time to preprocess the incoming data received through the API endpoint to match the format expected by the model. This may include scaling, encoding, or any other necessary transformations. 8. Generate Predictions- You will then pass the preprocessed data to the loaded model and make predictions. You will also retrieve the predicted results from the model and return the predictions as the response from the API endpoint. You can format the response as JSON, allowing easy integration with other applications. 9. Run the Flask Application- It\u2019s time for you to start the Flask application by running the app's run method. This will launch the Flask development server, allowing you to test the deployment locally. 10. Deploy the Flask Application- To deploy the Flask application, you can choose a suitable hosting platform like Heroku or AWS. Follow the platform's instructions to deploy your Flask application to a production server. 11. Test and Monitor the Deployment- The final step involves testing the deployed API using sample requests and verifying the returned predictions. You must leverage monitoring mechanisms to track the API's usage, performance, and potential errors. You can understand these steps better by working on the \u2018Deploying Machine Learning Models with Flask for Beginners\u2019 project.  Explore Categories Machine Learning Model Deployment Using Django Here are the detailed steps for deploying a machine learning model using Django- 1. Create a Django Project- You will start by creating a new Django project using the django-admin startproject command. This will create the basic structure for your deployment. 2. Model Designing- The next step is to define the structure of your machine learning model by creating a Django model. This model will represent the inputs and outputs of your ML model. 3. Model Training And Serialization- You will train your machine learning model using the desired algorithm and data. Once trained, it\u2019s time to serialize and save the model using libraries like joblib or pickle. 4. Create Django Views- You will also design Django views to handle incoming requests and preprocess the input data. 5. Model Integration- Then, you will load the serialized model into memory within the Django view. You will preprocess the incoming data to match the model's input requirements and make predictions using the loaded model. 6. Define URL Patterns- Next, you will configure URL patterns in the Django project to map the desired API endpoints to the corresponding views. 7. Deploy The Django Application- Now, it\u2019s time to deploy your Django application using a suitable web server like Apache or Nginx. You must configure the server to handle incoming requests and route them to your Django application. 8. Test and Monitor the Deployment- Lastly, you should thoroughly test the deployed application by sending sample requests and validating the returned predictions. You should implement monitoring mechanisms to track usage, performance, and potential errors. 9. Scale And Maintain The Deployment- As your application gains more traffic, you must always ensure proper scalability using load balancing or containerization techniques. You must continuously monitor the performance of your deployment and make necessary updates or improvements to ensure optimal functioning. Unlock the ProjectPro Learning Experience for FREE  Machine Learning Model Deployment Using Streamlit Below are the steps involved in deploying a machine learning model using Streamlit- 1. Install Streamlit- The first step is to install Streamlit using the command pip install streamlit. Streamlit is a Python library that allows you to create interactive web applications for machine learning. 2. Import Required Libraries- The next step is to import the necessary libraries, including Streamlit and any other libraries needed for your machine-learning model. 3. Data Loading And Preprocessing- You will now load the required data for your model and preprocess it as necessary. This may include handling missing values, feature scaling, or encoding categorical variables. 4. Model Training And Optimization- Once you have prepared the data, you must train your machine-learning model using the preprocessed data. \\You will also optimize the model's hyperparameters to improve its performance. 5. Create a Streamlit App- Now, it\u2019s time to create a new Python file and also, import the Streamlit library. You will define a Streamlit app using the st object, which serves as the interface for building the web application. 6. Design The User Interface- You will use Streamlit's built-in functions to design the user interface of your web application. This may include adding input fields, sliders, dropdowns, or other interactive components. 7. Model Integration- Within the Streamlit app, you must load the trained model into memory and define the necessary preprocessing steps to transform the user input data. 8. Generate Predictions- You will use the loaded model to predict user input data. You can also display the predictions and other relevant information. 9. Run And Deploy The Streamlit App- You can run the Streamlit app using the streamlit run command followed by the filename of your app. This will launch a local server and open a browser window with your web application. To deploy your Streamlit app, choose a suitable hosting platform like Heroku, AWS, or Azure, and follow the platform's instructions to deploy your app and make it accessible to users. 10. Test, Monitor, And Maintain The Deployment- The last step is to test your deployed Streamlit app to ensure its functionality, accuracy, and responsiveness. You must implement monitoring mechanisms to track usage, performance metrics, and potential errors. You must also regularly maintain and update your deployed app, including model retraining, bug fixes, and incorporating user feedback to ensure optimal performance and user satisfaction. You can better understand these steps by implementing them in the \u2018Model Deployment on GCP using Streamlit for Resume Parsing\u2019 project.\u00a0 Now that you have a better understanding of how to deploy ML models, it\u2019s time to explore some of the popular tools you can use to deploy your ML models successfully. Get confident to build end-to-end projects Access to a curated library of 250+ end-to-end industry projects with solution code, videos and tech support. Machine Learning Model Deployment Tools Machine learning model deployment tools offer powerful features and capabilities to streamline the deployment process, allowing organizations to efficiently integrate trained models into their production systems and leverage the benefits of machine learning in real-world applications. Let us explore some of the top ML model deployment tools- 1. TensorFlow Serving- TensorFlow Serving is a widely used open-source tool for deploying machine learning models in production. It provides a flexible architecture for serving TensorFlow models, enabling high-performance and efficient inference. It also supports dynamic model updates, allowing seamless versioning and updates without disrupting the serving process. Tensorflow Serving is commonly used in industries such as e-commerce for real-time product recommendations and fraud detection machine learning systems. 2. AWS SageMaker- Amazon SageMaker is a popular AWS platform for building, training, and deploying ML models. It offers extensive tools and services, including pre-built algorithms, auto-scaling capabilities, and managed infrastructure. A unique feature of SageMaker is its ability to automatically create model endpoints, allowing you to deploy models as API endpoints for real-time predictions easily. It is popularly used for ML deployment processes in various domains, such as healthcare for diagnosing diseases from medical images and finance for fraud detection and risk analysis. Work on this \u2018AWS Project to Build and Deploy LSTM Model with Sagemaker\u2019 that will help you gain a deeper understanding of how AWS Sagemaker supports ML model deployment. 3. Kubeflow- Kubeflow is an open-source platform that simplifies and streamlines the deployment of machine learning models on Kubernetes. It provides an extensive ecosystem of tools for managing and automating the end-to-end machine-learning workflow. One unique aspect of Kubeflow is its focus on reproducibility, enabling users to track experiments and version models and reproduce results easily. It leverages Kubernetes' scalability and resilience, efficiently deploying models across distributed clusters. Kubeflow is particularly well-suited for large-scale machine learning deployments in industries like finance for high-frequency trading and manufacturing for quality control and anomaly detection. You can work on the \u2018MLOps Project on GCP using Kubeflow for Model Deployment\u2019 to understand how Kubeflow simplifies model deployment. 4. MLFlow- MLFlow is an open-source platform that simplifies the management and deployment of machine learning models. It offers tools for experiment tracking, reproducibility, model packaging, and deployment. MLFlow's unique feature is its model registry, which allows users to organize, store, and manage models in a centralized repository. This enables easy collaboration among data scientists and promotes model versioning and governance. MLFlow supports various deployment options, including serving models as RESTful APIs or deploying them in cloud platforms like AWS or Azure. It finds applications in diverse domains, such as in healthcare for predicting patient outcomes and in marketing for personalized recommendation systems. Learn about the significance of R programming language wirh these data science projects in R with source code.  Machine Learning Model Deployment Best Practices This section discusses some of the best practices you must follow while deploying your machine learning models to ensure they continue to perform at their best in real-world scenarios. 1. Continuous Integration and Deployment (CI/CD)- Implement a CI/CD pipeline for seamless model deployment updates. For instance, regular model updates in autonomous driving can ensure the vehicle adapts to changing road conditions and improves safety. 2. Versioning and Tracking- Maintain a robust versioning system and track changes to models and dependencies. This helps in regulatory compliance, such as in healthcare, where tracking model versions and data sources ensures transparency and accountability. Think of it like keeping a journal for your models! 3. Containerization- Use containerization technologies like Docker to package models and their dependencies. This allows easy deployment across different environments and platforms. This applies to deploying predictive maintenance models in manufacturing, where containers enable consistent deployment across multiple machines. (It's like a smooth assembly line for your models.) 4. Scalability And Load Balancing- Ensure the deployment architecture can handle high traffic and scale horizontally. This is useful in e-commerce, where load balancing allows models to handle many simultaneous product recommendations during peak shopping seasons. You can think of it as having a checkout line with multiple cashiers so customers don't get stuck waiting! 5. Monitoring And Alerting- Implement comprehensive monitoring and alerting mechanisms to detect and respond to model performance degradation or anomalies. In finance, real-time monitoring alerts can help identify potential fraud patterns and trigger immediate actions for fraud prevention- like a superhero fighting off fraudsters! Struggling with solved data science projects? Check out these data science projects with source code in Python today! Machine Learning(ML) Model Deployment Projects For Practice This section will explore some exciting real-world projects for data scientists and ML experts that will showcase the practical applications of deploying ML models. Working on these ML projects will also help you understand the challenges faced by data engineers and scientists during deployment and the strategies employed to integrate ML models into production environments successfully. Here are some exciting ML projects that will help you better understand how you can easily deploy ML models in production- 1. MLOps Using Azure DevOps To Deploy A Classification Model  In this MLOps Azure project, you will learn how to deploy a classification machine learning model to predict the customer's license status on Azure through scalable CI/CD (Continuous Integration/Continuous Delivery) ML pipelines. Working on this ML project will help you understand Azure DevOps and also how to deploy the license status classification model through Azure DevOps. Source Code- MLOps Using Azure DevOps to Deploy a Classification Model 2. PyCaret Project To Build And Deploy An ML App Using Streamlit In this PyCaret Project, you will build a customer segmentation model with PyCaret and deploy the machine learning application using Streamlit. You will work with an unsupervised learning ML model that can categorize the customers into different segments using K means clustering to segment the data. You will then build a Streamlit application and deploy it on the Streamlit cloud by linking it to the Github repository containing the project. Source Code- PyCaret Project to Build and Deploy an ML App using Streamlit 3. Azure Deep Learning-Deploy RNN CNN models For TimeSeries  In this deep learning project, you will learn to perform docker-based deployment of RNN and CNN Models for Time Series Forecasting on Azure Cloud. You will learn how to deploy a time-series deep learning model in a multi-part format on the Azure cloud platform. Working on this ML project will teach you how to perform a Flask app deployment and a docker-based deployment. Source Code- Azure Deep Learning-Deploy RNN CNN models for TimeSeries From Theory To Practice- Machine Learning Model Deployment With ProjectPro Mastering the art of ML model deployment is a crucial skill for data science and ML professionals. Gaining expertise in model deployment helps you bridge the gap between theory and real-world implementation, ensuring the successful integration of ML models into production environments. To gain hands-on practice and further enhance your machine learning skills, ProjectPro offers more than 270 end-to-end solved Data Science and ML projects curated by industry experts that focus on model deployment. By working on these industry-level projects from the ProjectPro repository, you can gain practical experience, understand deployment challenges, and learn best practices. This will boost your knowledge and expertise, enhance your career prospects, and open doors to exciting career opportunities. Along with these project solutions, you will also get access to reusable project templates and recipes that come in handy in your day-to-day work.\u00a0 Get FREE Access to Data Analytics Example Codes for Data Cleaning, Data Munging, and Data Visualization FAQs on Machine Learning Model Deployment 1. Where do you deploy machine learning models? You can deploy machine learning models in various environments, including cloud platforms like AWS, Microsoft Azure, and GCP. You can also deploy models on-premises within an organization's infrastructure or even on edge devices such as mobile devices or IoT devices for real-time inference at the edge. 2. How are ML models deployed in production? You can deploy ML models in production by following these steps- Preprocessing and Feature Engineering- Prepare data for model input, transforming and normalizing it as needed. Training and Evaluation- Train the model using labeled data and evaluate its performance using metrics like accuracy or loss. Model Serialization- Save the trained model as a file for later use. Model Deployment- Integrate the serialized model into a production environment, such as an API or a server, to make predictions on new data. Monitoring and Maintenance- Continuously monitor the model's performance, retrain periodically, and update the deployment to ensure accurate and reliable predictions. \n \u00a0 PREVIOUS NEXT  About the Author \n\t\t\t\t\t\t\t\t\tDaivi\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\tDaivi is a highly skilled Technical Content Analyst with over a year of experience at ProjectPro. She is passionate about exploring various technology domains and enjoys staying up-to-date with industry trends and developments. Daivi is known for her excellent research skills and ability to distill\t\t\t\t\t\t\t\t Meet The Author  Start Your First Project Learn By Doing Related Blogs on Machine Learning Trending Blog Categories Project Categories Projects Blogs Certification Courses Tutorials \nProjectPro\n\n\n\n \u00a9 2024  \u00a9 2024 Iconiq Inc. About us Contact us Privacy policy User policy Write for ProjectPro"}
{"url": "https://dig8italx.com/what-is-ai-architecture-important-things-you-need-to-know/", "title": "What is AI Architecture? Important Things You Need to Know", "content": "What is AI Architecture? Important Things You Need to Know As the world becomes increasingly interconnected and reliant on technology, Artificial Intelligence (AI) is transforming industries, enhancing human capabilities, and changing how we interact with machines. This innovation makes it vital for organizations to understand AI architecture, the backbone that enables AI systems to function effectively and efficiently. So, what is AI architecture, and what should your organization know about it? Table of Contents What is AI Architecture? Much like traditional software or security architecture, AI architecture serves as a blueprint for AI system development. It comprises various components, including data pipelines, machine learning algorithms, frameworks, and hardware that are organized in a manner to serve specific organizational goals or to solve particular problems. The architecture takes into account the current state of the organization\u2019s data, technology stack, and specific AI needs, providing a strategic and technical plan for AI implementation. Similarity to Traditional Architecture Drawing a parallel with traditional architecture\u2014where architects consider factors like soil type, climate, and client preferences\u2014AI architects examine data availability, computational resources, and business objectives. They produce a plan (the architecture) that guides the developers and data scientists in constructing the AI system, ensuring alignment with business goals. Key Elements of AI Architecture AI architecture is adaptable to the needs of an organization. Nevertheless, there are core elements commonly included in most architectures. Purpose The primary objective of AI architecture is to align the technical components of AI\u2014such as machine learning models, data pipelines, and analytics tools\u2014with the business goals of the organization. Frameworks Frameworks play a crucial role in AI architecture. They are the guiding principles that help in the design, construction, and deployment of AI systems. Popular frameworks for AI include TensorFlow, PyTorch, and Scikit-learn, each offering different capabilities and advantages. Components Benefits of AI Architecture Streamlined Development Optimized Performance Scalability Cost Efficiency AI Architecture\u2019s Key Deliverables The deliverables depend on your goals and the complexity of your AI needs. However, commonly expected outcomes may include: Architecture Diagrams The architecture diagrams stand as the visual cornerstone of an AI project, offering stakeholders a comprehensive and intuitive representation of the system\u2019s entire structure. These meticulously crafted diagrams detail the intricate interactions between various components, ensuring that every stakeholder, from developers to decision-makers, comprehends the system\u2019s flow and intricacies. By providing a clear, layered visualization of the AI system, architecture diagrams streamline communication, set clear expectations, and facilitate smoother project progression. Data Strategy In the realm of AI, data is the lifeblood. The data strategy meticulously outlines the protocols for sourcing, collecting, and processing this invaluable asset. It doesn\u2019t just focus on gathering vast amounts of data but emphasizes the relevance, quality, and ethical handling of this data. By addressing aspects like data governance, security, and lifecycle management, the strategy ensures that data practices align with both the organization\u2019s goals and regulatory standards. A robust data strategy not only fuels the AI system with the right information but also fortifies the organization\u2019s reputation and compliance stance. Model Specifications Diving deep into the heart of AI, the model specifications provide a detailed roadmap for the development and tuning of machine learning algorithms. This document elucidates the rationale behind selecting specific algorithms, tailored to address the unique challenges and aspirations of the project. By detailing parameters, training methods, and expected outcomes, the model specifications act as a guiding beacon for data scientists. They ensure that the AI models developed are not just technically sound but also strategically aligned with the project\u2019s overarching goals. Reference Models: Today\u2019s AI realm boasts a plethora of advanced models and frameworks, here are a few of them: Deployment Plan Transitioning from development to real-world application, the deployment plan outlines the steps for rolling out the AI system into a live environment. This plan is not just a technical checklist; it encompasses considerations like user training, system integration, and potential rollback strategies. By providing a structured approach to launch, the deployment plan ensures that the AI system seamlessly integrates with existing infrastructures, aligning with both user expectations and organizational objectives. Monitoring Plan Post-deployment, the journey of an AI system is far from over. The monitoring plan comes into play, outlining the protocols to track the system\u2019s performance, accuracy, and overall health. By setting clear metrics, benchmarks, and review intervals, this plan ensures that any deviations or anomalies are swiftly detected and addressed. Beyond just technical monitoring, it also emphasizes feedback loops, ensuring that user insights and experiences are continually factored into system enhancements. In essence, the monitoring plan acts as the organization\u2019s eyes and ears, ensuring that the AI system consistently delivers value and stays in optimal health. How Long Does It Take? Establishing an AI architecture is not a one-size-fits-all endeavor, and the time it takes to design and implement can vary dramatically based on a multitude of factors.  At the forefront is the scope of the project. A simple AI solution designed for a specific task, like a chatbot for customer support, might take only a few weeks from conceptualization to deployment. However, complex, enterprise-wide solutions that integrate multiple data sources, cater to varied user needs, and span across different departments can extend over several months or even years.  The organizational size also plays a pivotal role. Larger organizations, with intricate legacy systems and vast data lakes, might face longer integration and testing phases.  Additionally, the specific AI applications being developed, the quality and readiness of the data, and the expertise of the team involved are all crucial determinants. It\u2019s also worth noting that the initial deployment is often just the beginning. Continuous monitoring, refining, and scaling based on real-world performance and feedback can lead to ongoing iterations. Thus, while initial timelines can be estimated, the true evolution of an AI system is an ongoing journey, adapting and growing with the organization\u2019s ever-changing needs. What Comes Next? Embarking on an AI journey is a significant commitment. An AI architect can guide you through the process from conceptualization to deployment and ongoing optimization. Whether you are a small business or a large enterprise, understanding and investing in AI architecture is a step toward future-proofing your organization. To consult an expert on how AI architecture can benefit your organization, consider scheduling a meeting with one of our professionals to discuss your specific needs. Share : Latest Post \n\n\t\t\t\tAI Will Not Replace You: Navigating the Future with Collaborative Intelligence\t\t\t\n \n\n\t\t\t\tRevolutionize Your NLP with Top AI Platforms: A Comprehensive Guide!\t\t\t\n \n\n\t\t\t\tRevolutionize Your Virtual Assistance with Top AI Platforms\t\t\t\n \n\n\t\t\t\tUnleashing the Power of AI Platforms for Predictive Analytics\t\t\t\n \n\n\t\t\t\tRevolutionizing Healthcare: The Benefits of Robotics Automation\t\t\t\n \n\n\t\t\t\tRevolutionizing Your Home with Robotics Automation\t\t\t\n Subscribe our Newsletter Sign up for our newsletter and get exclusive access to our best deals, tips, and resources \u2013 delivered straight to your inbox. AI EXPERTS: CUSTOMIZED SOLUTIONS FOR BUSINESSES Company Support Get in Touch"}
{"url": "https://learn.microsoft.com/en-us/azure/architecture/ai-ml/", "title": "Artificial intelligence (AI) architecture - Azure Architecture Center | Microsoft Learn", "content": "This browser is no longer supported. Upgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support. Share via Artificial intelligence (AI) architecture design In this article Artificial intelligence (AI) is a technology that allows machines to imitate intelligent human behavior. With AI, machines can: Architects design workloads that use artificial intelligence to perform functions or make decisions where traditional logic or processing would be prohibitive, or even nearly impossible, to implement. As an architect designing a solution, it's important to understand the AI and machine learning landscape and how Azure offers solutions to integration into your workload design. AI concepts Algorithms Algorithms or machine learning algorithms are pieces of code that help humans explore, analyze, and find meaning in complex data sets. Each algorithm is a finite set of unambiguous step-by-step instructions that a machine can follow to achieve a certain goal. In a machine learning model, the goal is to establish or discover patterns that humans can use to make predictions or categorize information. An algorithm may describe how to determine whether a pet is a cat, dog, fish, bird, or lizard. Another far more complicated algorithm may describe how to identify a written or spoken language, analyze its words, translate them into a different language, and then check the translation for accuracy. When you design a workload, you'll need to select an algorithm family that is suited for your task and evaluate the various available algorithms to find the appropriate fit. Machine learning Machine learning is an AI technique that uses algorithms to create predictive models. The algorithm is used to parse data fields and to \"learn\" from that data by using patterns found within it to generate models. Those models are then used to make informed predictions or decisions about new data. The predictive models are validated against known data, measured by performance metrics selected for specific business scenarios, and then adjusted as needed. This process of learning and validation is called training. Through periodic retraining, ML models are improved over time. When it comes to workload design, you consider using machine learning when you have a situation where past observations can reliably be used to predict future situations. These observations can be universal truths such as computer vision that detects one form of animal from another, or these observations can be specific to your situation such as computer vision that detects a potential assembly mistake on your assembly lines based on past warranty claim data. Deep learning Deep learning is a type of ML that can learn through its own data processing. Like machine learning, it also uses algorithms to analyze data, but it does by using artificial neural networks that contains many inputs, outputs, and layers of processing. Each layer can process the data in a different way, and the output of one layer becomes the input for the next. This allows deep learning to create more complex models than traditional machine learning. As a workload designer, this option requires a large investment in generating highly customized or exploratory models. Generally speaking, you'll consider other solutions presented in this article before adding deep learning into your workload. What is Deep Learning? Deep learning versus machine learning Generative AI Generative AI is a form of artificial intelligence in which models are trained to generate new original content based on many forms of content such as natural language, computer vision, audio, or image input. With generative AI, you can describe a desired output in normal everyday language, and the model can respond by creating appropriate text, image, code, and more. Some examples of generative AI applications are: Microsoft Copilot is primarily a user interface that can assist users in writing code, documents, and other text-based content. It's based on popular OpenAI models and is integrated into a wide range of Microsoft applications and user experiences. Azure OpenAI is a development platform as a service that provides access to OpenAI's powerful language models such as o1-preview, o1-mini, GPT-4o, GPT-4o mini, GPT-4 Turbo with Vision, GPT-4, GPT-3.5-Turbo, and Embeddings model series. These models can be adapted to your specific task such as: Language models Language models are a subset of Generative AI that focuses on natural language processing (NLP) tasks, such as text generation and sentiment analysis. These models represent natural language based on the probability of words or sequences of words occurring in a given context. Conventional language models have used in supervised settings for research purposes where the models are trained on well-labeled text datasets for specific tasks. Pretrained language models offer an accessible way to get started with AI and have become more widely used in recent years. These models are trained on large-scale text corpora from the internet using deep learning neural networks and can be fine-tuned on smaller datasets for specific tasks. The size of a language model is determined by its number of parameters, or weights, that determine how the model processes input data and generates output. Parameters are learned during the training process by adjusting the weights within layers of the model to minimize the difference between the model's predictions and the actual data. The more parameters a model has, the more complex and expressive it is, but also the more computationally expensive it is to train and use. In general, small language models have fewer than 10 billion parameters, and large language models have more than 10 billion parameters. For example, the Microsoft Phi-3 model family has three versions with different sizes: mini (3.8 billion parameters), small (7 billion parameters), and medium (14 billion parameters). Copilots The availability of language models led to the emergence of new ways to interact with applications and systems through digital copilots and connected, domain specific, agents. Copilots are generative AI assistants that are integrated into applications often as chat interfaces. They provide contextualized support for common tasks in those applications. Microsoft Copilot is integrated into a wide range of Microsoft applications and user experiences. It's based on an open architecture that enables third-party developers to create their own plug-ins to extend or customize the user experience with Microsoft Copilot. Additionally, third-party developers can create their own copilots using the same open architecture. Adopt, extend, and build Copilot experiences across the Microsoft Cloud. Microsoft Copilot Studio Azure AI Studio Retrieval Augmented Generation (RAG) Retrieval Augmented Generation (RAG) is an architecture pattern that augments the capabilities of a large language model (LLM) like ChatGPT, that was trained only on public data. This pattern allows you to add a retrieval system that provides relevant grounding data in the context with the user request. Adding an information retrieval system gives you control over grounding data used by a language model when it formulates a response. RAG architecture helps you scope generative AI to content that's sourced from vectorized documents, images, and other data formats. RAG is not limited to vector search storage however, the pattern is applicable in conjunction with any data store technology. Automated machine learning (AutoML) Automated machine learning, also referred to as automated ML or AutoML, is the process of automating the time-consuming, iterative tasks of machine learning model development. It allows data scientists, analysts, and developers to build ML models with high scale, efficiency, and productivity all while sustaining model quality. AI services With Azure AI services developers and organizations can create intelligent, market-ready, and responsible applications with out-of-the-box, prebuilt and customizable APIs and models. Usages include natural language processing for conversations, search, monitoring, translation, speech, vision, and decision-making. Choose an Azure AI services technology Azure AI services documentation Choosing a natural language processing technology in Azure MLflow is an open-source framework designed to manage the complete machine learning lifecycle. AI Language models Large Language Models (LLMs), such as OpenAI's GPT models, are powerful tools that can generate natural language across various domains and tasks. When considering using these models, consider factors such as data privacy, ethical use, accuracy, and bias. Phi open models are small, less compute-intensive models for generative AI solutions. A small language model (SLM) may be more efficient, interpretable, and explainable than a large language model. When designing a workload, you can use language models both as a hosted solution, behind a metered API or for many small language models you can host those in process or at least on the same compute as the consumer. When using language models in your solution, consider your choice of language model and its available hosting options to ensure your using an optimized solution for your use case. AI development platforms and tools Azure Machine Learning service Azure Machine Learning is a machine learning service to build and deploy models. Azure Machine Learning offers web interfaces and SDKs so you can train and deploy your machine learning models and pipelines at scale. Use these capabilities with open-source Python frameworks, such as PyTorch, TensorFlow, and scikit-learn. What are the machine learning products at Microsoft? Azure Machine Learning documentation overview What is Azure Machine Learning? General orientation with links to many learning resources, SDKs, documentation, and more Machine learning reference architectures for Azure Baseline OpenAI end-to-end chat reference architecture is a reference architecture that shows how to build an end-to-end chat architecture with OpenAI's GPT models. \n\n\n \nThe diagram shows the App Service baseline architecture with a private endpoint that connects to a managed online endpoint in a Machine Learning managed virtual network. The managed online endpoint sits in front of a Machine Learning compute cluster. The diagram shows the Machine Learning workspace with a dotted line that points to the compute cluster. This arrow represents that the executable flow is deployed to the compute cluster. The managed virtual network uses managed private endpoints that provide private connectivity to resources that are required by the executable flow, such as Container Registry and Storage. The diagram further shows user-defined private endpoints that provide private connectivity to the Azure OpenAI Service and Azure AI Search.  Azure OpenAI chat baseline architecture in an Azure landing zone shows you how to build on the Azure OpenAI baseline architecture to address changes and expectations when you deploy it in an Azure landing zone. Machine learning operationalization (MLOps) for Python models using Azure Machine Learning Batch scoring of Spark machine learning models on Azure Databricks Automated machine learning (AutoML) Build ML models at scale using the AutoML capability in Azure Machine Learning to automate tasks. Azure automated machine learning product home page Azure automated ML infographic (PDF) Tutorial: Create a classification model with automated ML in Azure Machine Learning Configure automated ML experiments in Python Use the CLI extension for Azure Machine Learning Automate machine learning activities with the Azure Machine Learning CLI MLflow Azure Machine Learning workspaces are MLflow-compatible, which means that you can use an Azure Machine Learning workspace the same way you use an MLflow server. This compatibility has the following advantages: For more information, see MLflow and Azure Machine Learning Generative AI tools Prompt flow is a suite of development tools designed to streamline the end-to-end development cycle of generative AI applications, from ideation, prototyping, testing, evaluation to production deployment and monitoring. It supports prompt engineering through expressing actions in a modular orchestration and flow engine. Azure AI Studio helps you experiment, develop, and deploy generative AI apps and APIs responsibly with a comprehensive platform. With Azure AI Studio, you have access to Azure AI services, foundation models, playground, and resources to help you build, train, fine-tune, and deploy AI models.\nAlso, you can evaluate model responses and orchestrate prompt application components with prompt flow for better performance. Azure Copilot Studio is used to extend Microsoft Copilot in Microsoft 365 and build custom copilots for internal and external scenarios. With Copilot Studio, users can design, test, and publish copilots using the comprehensive authoring canvas. Users can easily create generative AI-enabled conversations, provide greater control to responses for existing copilots, and accelerate productivity with specific automated workflows. Data platforms for AI Microsoft Fabric Microsoft Fabric is an end-to-end analytics and data platform designed for enterprises that require a unified solution. Workload teams can be granted access to data in these systems. It encompasses data movement, processing, ingestion, transformation, real-time event routing, and report building. It offers a comprehensive suite of services including Data Engineering, Data Factory, Data Science, Real-Time Analytics, Data Warehouse, and Databases. Microsoft Fabric integrates separate components into a cohesive stack. Instead of relying on different databases or data warehouses, you can centralize data storage with OneLake. AI capabilities are embedded within Fabric, eliminating the need for manual integration. What is Microsoft Fabric Learning Path - Get started with Microsoft Fabric AI services in Fabric Use Azure OpenAI in Fabric with REST API Using Microsoft Fabric for Generative AI: A Guide to Building and Improving RAG Systems Building Custom AI Applications with Microsoft Fabric: Implementing Retrieval Augmented Generation for Enhanced Language Models Copilots in Fabric Copilot and other generative AI features let you transform and analyze data, generate insights, and create visualizations and reports in Microsoft Fabric and Power BI. You can either build your own copilot, or choose one of the following prebuilt copilots: Overview of Copilot in Fabric Copilot for Data Science and Data Engineering Copilot for Data Factory Copilot for Data Warehouse Copilot for Power BI Copilot for Real-Time Intelligence AI skills in Fabric With a Microsoft Fabric AI skill, you can configure a generative AI system to generate queries that answer questions about your data. After you configure the AI skill, you can share it with your colleagues, who can then ask their questions in plain English. Based on their questions, the AI generates queries over your data that answer those questions. Apache Spark-based data platforms for AI Apache Spark is a parallel processing framework that supports in-memory processing to boost the performance of big data analytic applications. Spark provides primitives for in-memory cluster computing. A Spark job can load and cache data into memory and query it repeatedly, which is faster than disk-based applications, such as Hadoop. Apache Spark in Azure Fabric Microsoft Fabric Runtime is an Azure-integrated platform based on Apache Spark that enables the execution and management of data engineering and data science experiences. It combines key components from both internal and open-source sources, providing customers with a comprehensive solution. Major components of Fabric Runtime: Apache Spark - a powerful open-source distributed computing library that enables large-scale data processing and analytics tasks. Apache Spark provides a versatile and high-performance platform for data engineering and data science experiences. Delta Lake - an open-source storage layer that brings ACID transactions and other data reliability features to Apache Spark. Integrated within Fabric Runtime, Delta Lake enhances data processing capabilities and ensures data consistency across multiple concurrent operations. Default-level packages for Java/Scala, Python, and R - packages that support diverse programming languages and environments. These packages are automatically installed and configured, allowing developers to apply their preferred programming languages for data processing tasks. The Microsoft Fabric Runtime is built upon a robust open-source operating system, ensuring compatibility with various hardware configurations and system requirements. Azure Databricks Runtime for Machine Learning Azure Databricks is an Apache Spark\u2013based analytics platform with one-click setup, streamlined workflows, and an interactive workspace for collaboration between data scientists, engineers, and business analysts. Databricks Runtime for Machine Learning (Databricks Runtime ML) lets you start a Databricks cluster with all of the libraries required for distributed training. It provides an environment for machine learning and data science. Plus, it contains multiple popular libraries, including TensorFlow, PyTorch, Keras, and XGBoost. It also supports distributed training using Horovod. Azure Databricks documentation Machine learning capabilities in Azure Databricks How-to guide: Databricks Runtime for Machine Learning Batch scoring of Spark machine learning models on Azure Databricks Deep learning overview for Azure Databricks Apache Spark in Azure HDInsight Apache Spark in Azure HDInsight is the Microsoft implementation of Apache Spark in the cloud. Spark clusters in HDInsight are compatible with Azure Storage and Azure Data Lake Storage, so you can use HDInsight Spark clusters to process your data stored in Azure. The Microsoft machine learning library for Apache Spark is SynapseML (formerly known as MMLSpark). This open-source library adds many deep learning and data science tools, networking capabilities, and production-grade performance to the Spark ecosystem. Learn more about SynapseML features and capabilities. Azure HDInsight overview. Basic information about features, cluster architecture, and use cases, with pointers to quickstarts and tutorials. Tutorial: Build an Apache Spark machine learning application in Azure HDInsight Apache Spark best practices on HDInsight Configure HDInsight Apache Spark Cluster settings Machine learning on HDInsight GitHub repo for SynapseML: Microsoft machine learning library for Apache Spark Create an Apache Spark machine learning pipeline on HDInsight Data storage for AI Microsoft Fabric OneLake OneLake in Fabric is a unified and logical data lake that's tailored for the entire organization. It serves as the central hub for all analytics data and is included with every Microsoft Fabric tenant. OneLake in Fabric is built on the foundation of Data Lake Storage Gen2. OneLake in Fabric: For more information, see OneLake, the OneDrive for data. Azure Data Lake Storage Gen2 Azure Data Lake Storage is a single, centralized repository where you can store all your data, both structured and unstructured. A data lake enables your organization to quickly and more easily store, access, and analyze a wide variety of data in a single location. With a data lake, you don't need to conform your data to fit an existing structure. Instead, you can store your data in its raw or native format, usually as files or as binary large objects (blobs). Data Lake Storage Gen2 provides file system semantics, file-level security, and scale. Because these capabilities are built on Blob storage, you also get low-cost, tiered storage, with high availability/disaster recovery capabilities. Data Lake Storage Gen2 makes Azure Storage the foundation for building enterprise data lakes on Azure. Designed from the start to service multiple petabytes of information while sustaining hundreds of gigabits of throughput, Data Lake Storage Gen2 allows you to easily manage massive amounts of data. Data processing for AI Microsoft Fabric Data Factory With Data Factory, you can ingest, prepare, and transform data from multiple data sources (for example, databases, data warehouse, Lakehouse, real-time data, and more). When you design workloads, this is a tool that can be instrumental in meeting your DataOps requirements. Data Factory supports both code and no/low code solutions: Data pipelines let you create workflow capabilities at cloud-scale. With data pipelines, you can use the drag-and-drop interface to build workflows that can refresh your dataflow, move petabyte-size data, and define control flow pipelines. Dataflows provide a low-code interface for ingesting data from hundreds of data sources, transforming your data using 300+ data transformations. Also see: Azure Databricks With Databricks Data Intelligence Platform, you can write code to create a machine learning workflow using feature engineering: You can also use Mosaic AI Vector Search, which is optimized for storing and retrieving embeddings. Embeddings are crucial for applications that require similarity searches, such as RAG (Retrieval Augmented Generation), recommendation systems, and image recognition. Data connectors for AI Azure Data Factory and Azure Synapse Analytics pipelines support many data stores and formats via Copy, Data Flow, Look up, Get Metadata, and Delete activities. To see the available data store connectors, the supported capabilities and the corresponding configurations, and generic ODBC connection options, see Azure Data Factory and Azure Synapse Analytics connector overview. Custom AI Azure Machine Learning Azure Machine Learning is a cloud service for accelerating and managing the machine learning (ML) project lifecycle. ML professionals, data scientists, and engineers can use it in their day-to-day workflows to train and deploy models and manage machine learning operations (MLOps). Azure Machine Learning offer the following capabilities: Algorithm selection Some algorithms make particular assumptions about the structure of the data or the desired results. If you can find one that fits your needs, it can give you more useful results, more accurate predictions, or faster training times. How to select algorithms for Azure Machine Learning Hyperparameter tuning or optimization is the process of finding the configuration of hyperparameters that results in the best performance. The process is computationally expensive and manual. Hyperparameters are adjustable parameters that let you control the model training process. For example, with neural networks, you decide the number of hidden layers and the number of nodes in each layer. Model performance depends heavily on hyperparameters. Azure Machine Learning lets you automate hyperparameter tuning and run experiments in parallel to efficiently optimize hyperparameters. Model training. With Azure Machine Learning, you can iteratively use an algorithm to create or \"teach\" models. Once trained, these models can then be used to analyze data from which predictions can be made. During the training phase, a quality set of known data is tagged so that individual fields are identifiable. The tagged data is fed to an algorithm configured to make a particular prediction. When finished, the algorithm outputs a model that describes the patterns it found as a set of parameters. During validation, fresh data is tagged and used to test the model. The algorithm is adjusted as needed and possibly put through more training. Finally, the testing phase uses real-world data without any tags or preselected targets. Assuming the model's results are accurate, It's considered ready for use and can be deployed. Automated machine learning (AutoML) is the process of automating the time-consuming, iterative tasks of machine learning model development. It can significantly reduce the time it takes to get production-ready ML models. Automated ML can assist with model selection, hyperparameter tuning, model training, and other tasks, without requiring extensive programming or domain knowledge. You can use automated ML when you want Azure Machine Learning to train and tune a model for you using a specified target metric. Automated ML can be used regardless of data science expertise to identify an end-to-end machine learning pipeline for any problem. ML professionals and developers across industries can use automated ML to: Implement ML solutions without extensive programming or machine learning knowledge Save time and resources Apply data science best practices Provide agile problem-solving What is automated machine learning? Scoring is also called prediction and is the process of generating values based on a trained machine learning model, given some new input data. The values, or scores, that are created can represent predictions of future values,  but they might also represent a likely category or outcome. Feature engineering and featurization. Training data consists of rows and columns. Each row is an observation or record, and the columns of each row are the features that describe each record. Typically, the features that best characterize the patterns in the data are selected to create predictive models. Although many of the raw data fields can be used directly to train a model, it's often necessary to create other (engineered) features that provide information that better differentiates patterns in the data. This process is called feature engineering, where the use of domain knowledge of the data is used to create features that, in turn, help machine learning algorithms to learn better. In Azure Machine Learning, data-scaling and normalization techniques are applied to make feature engineering easier. Collectively, these techniques and this feature engineering are called featurization in automated machine learning (ML) experiments. Azure OpenAI Azure OpenAI Service lets you tailor OpenAI models to your personal datasets by using a process known as fine-tuning. This customization step lets you get more out of the service by providing: For more information, see: Azure AI services for custom AI Azure AI services offers features that let you build custom AI models and applications. This section provides an overview some of these key features. Custom Speech Custom speech is a feature of the Azure AI Speech service. With custom speech, you can evaluate and improve the accuracy of speech recognition for your applications and products. A custom speech model can be used for real-time speech to text, speech translation, and batch transcription. Out of the box, speech recognition utilizes a Universal Language Model as a base model that is trained with Microsoft-owned data and reflects commonly used spoken language. The base model is pretrained with dialects and phonetics representing various common domains. When you make a speech recognition request, the most recent base model for each supported language is used by default. The base model works well in most speech recognition scenarios. A custom model can be used to augment the base model to improve recognition of domain-specific vocabulary specific to the application by providing text data to train the model. It can also be used to improve recognition based for the specific audio conditions of the application by providing audio data with reference transcriptions. You can also train a model with structured text when the data follows a pattern, to specify custom pronunciations, and to customize display text formatting with custom inverse text normalization, custom rewrite, and custom profanity filtering. Custom Translator Custom Translator is a feature of the Azure AI Translator service. With Custom Translator, enterprises, app developers, and language service providers can build customized neural machine translation (NMT) systems. The customized translation systems seamlessly integrate into existing applications, workflows, and websites. The platform enables users to build and publish custom translation systems to and from English. Custom Translator supports more than three dozen languages that map directly to the languages available for NMT. For a complete list, see Translator language support. Custom Translator offers the following features: Document Intelligence custom models Azure AI Document Intelligence uses advanced machine learning technology to identify documents, detect and extract information from forms and documents, and return the extracted data in a structured JSON output. With Document Intelligence, you can use document analysis models, prebuilt/pretrained, or your trained standalone custom models. Document Intelligence custom models now include custom classification models for scenarios where you need to identify the document type before invoking the extraction model. A classification model can be paired with a custom extraction model to analyze and extract fields from forms and documents specific to your business. Standalone custom extraction models can be combined to create composed models. Custom AI tools Although prebuilt AI models are useful and increasingly flexible, the best way to get what you need from AI is to build a model that's tailored to your specific needs. There are two primary tools for creating custom AI models: Generative AI and traditional machine learning: Azure Machine Learning studio Azure Machine Learning studio is a cloud service for accelerating and managing the machine learning (ML) project lifecycle. ML professionals, data scientists, and engineers can use it in their day-to-day workflows to train and deploy models and manage machine learning operations (MLOps).: Azure AI Studio Azure AI Studio is designed to help you efficiently build and deploy custom generative AI applications with the power of the Azure broad AI offerings: For a detailed comparison between Azure Machine Learning and Azure AI Studio, see Azure Machine Learning vs. Azure AI Studio. Prompt flow in Azure AI Studio Prompt flow in Azure AI Studio is a development tool designed to streamline the entire development cycle of AI applications powered by Large Language Models (LLMs). Prompt flow provides a comprehensive solution that simplifies the process of prototyping, experimenting, iterating, and deploying your AI applications. Custom AI code languages The core concept of AI is the use of algorithms to analyze data and generate models to describe (or score) it in ways that are useful. Algorithms are written by developers and data scientists (and sometimes by other algorithms) using programming code. Two of the most popular programming languages for AI development are currently Python and R. Python is a general-purpose, high-level programming language. It has a simple, easy-to-learn syntax that emphasizes readability. There is no compiling step. Python has a large standard library, but it also supports the ability to add modules and packages. This encourages modularity and lets you expand capabilities when needed. There is a large and growing ecosystem of AI and ML libraries for Python, including many that are readily available in Azure. Python on Azure product home page Azure for Python developers Azure Machine Learning SDK for Python Introduction to machine learning with Python and Azure Notebooks scikit-learn. An open-source ML library for Python PyTorch. An open-source Python library with a rich ecosystem that can be used for deep learning, computer vision, natural language processing, and more TensorFlow. An open-source symbolic math library also used for ML applications and neural networks Tutorial: Apply machine learning models in Azure Functions with Python and TensorFlow R is a language and environment for statistical computing and graphics. It can be used for everything from mapping broad social and marketing trends online to developing financial and climate models. Microsoft has fully embraced the R programming language and provides many different options for R developers to run their code in Azure. Use R interactively on Azure Machine Learning. Tutorial: Create a logistic regression model in R with Azure Machine Learning General info on custom AI on Azure Microsoft AI on GitHub: Samples, reference architectures, and best practices Azure Machine Learning SDK for Python Azure Machine Learning Python SDK notebooks. A GitHub repo of example notebooks demonstrating the Azure Machine Learning Python SDK. Train R models using the Azure Machine Learning CLI (v2) Customer stories Different industries are applying AI in innovative and inspiring ways. Following are a few customer case studies and success stories: Browse more AI customer stories General info on Microsoft AI Learn more about Microsoft AI, and keep up-to-date with related news: Microsoft AI AI learning hub. Azure AI Microsoft AI News Microsoft AI on GitHub: Samples, reference architectures, and best practices Azure Architecture Center Next steps To learn about the artificial intelligence development products available from Microsoft, go to Microsoft AI. For training in how to develop AI solutions, go to AI learning hub. Microsoft AI on GitHub: Samples, reference architectures, and best practices organizes the Microsoft open source AI-based repositories, providing tutorials and learning materials. Find architecture diagrams and technology descriptions for AI solutions reference architectures. Feedback Was this page helpful? Additional resources Additional resources In this article"}
{"url": "https://www.coursera.org/articles/ai-programming-languages", "title": "AI Programming Languages: What to Know in 2025 | Coursera", "content": "AI Programming Languages: What to Know in 2024 AI is an essential part of the modern development process, and knowing suitable AI programming languages can help you succeed in the job market. Explore popular coding languages and other details that will be helpful in 2024. The programming world is undergoing a significant shift, and learning artificial intelligence (AI) programming languages appears more important than ever. In 2023, technological research firm Gartner revealed that up to 80 percent of organizations will use AI in some way by 2026, up from just 5 percent in 2023 [1].\u00a0 \n AI is rapidly evolving. Likewise, AI jobs are steadily increasing, with in-demand roles like machine learning engineers, data scientists, and software engineers often requiring familiarity with the technology. If you\u2019re considering working in AI or want to experiment with it, knowing the right language and remaining up-to-date on the state of programming can help you weather the changes AI brings across various industries.\u00a0 \n Below, we explore some popular AI programming languages. It will also examine the differences between traditional coding and coding for AI and how AI is changing programming.\u00a0 Read more: Python vs. C#: Which Language Is Best for AI?\n What AI programming languages should you learn? \n Different programming languages will be ideal, depending on the use cases. Here are four popular coding languages that are suitable for AI-related applications and technologies: Python, Java, C++, and Julia.\u00a0 \n 1. Python \n Python is a general-purpose, object-oriented programming language that has been a favorite among programmers. It's favored because of its simple learning curve, extensive community of support, and variety of uses. That same ease of use and Python's ability to simplify code make it a go-to option for AI programming. It features adaptable source code and works on various operating systems. Developers often use it for AI projects that require handling large volumes of data or developing models in machine learning. \n       2. Java \n Java is typically longer, requiring more lines of code than Python. It has a smaller community than Python, but AI developers often turn to Java for its automatic deletion of useless data, security, and maintainability. This powerful object-oriented language also offers simple debugging and use on multiple platforms. Java\u2019s libraries include essential machine learning tools and frameworks that make creating machine learning models easier, executing deep learning functions, and handling large data sets.       Read more: 10 Machine Learning Algorithms to Know\n 3. C++ \n Like Java, C++ typically requires code at least five times longer than you need for Python. It can be challenging to master but offers fast execution and efficient programming. Because of those elements, C++ excels when used in complex AI applications, particularly those that require extensive resources. It's a compiled, general-purpose language that's excellent for building AI infrastructure and working in autonomous vehicles. \n        Read more: 5 Benefits of AI to Know (+ 3 Risks to Watch Out For) 4. Julia \n Julia excels in performing calculations and data science, with benefits that include general use, fast and dynamic performance, and the ability to execute quickly. It's excellent for use in machine learning, and it offers the speed of C with the simplicity of Python. Julia remains a relatively new programming language, with its first iteration released in 2018. It supports distributed computing, an integrated package manager, and the ability to execute multiple processes.        AI vs. traditional coding: What\u2019s the difference?\u00a0 \n Coding is an essential skill for anyone who wants to work in AI. The programming languages may be the same or similar for both environments; however, the purpose of programming for AI differs from traditional coding. With AI, programmers code to create tools and programs that can use data to \u201clearn\u201d and make helpful decisions or develop practical solutions to challenges. In traditional coding, programmers use programming languages to instruct computers and other devices to perform actions. \n Additionally, AI programming requires more than just using a language. You also need frameworks and code editors to design algorithms and create computer models. Read more: How to Learn Artificial Intelligence: A Beginner\u2019s Guide\n How does AI-assisted programming change the industry? \n As AI continues expanding, the programming landscape is shifting. However, University of California San Diego professor Leo Porter believes AI will increase inclusion and diversity in programming. \n In 2022, AI-assisted coding tools like GitHub CoPilot emerged. Although the execution isn't flawless, AI-assisted coding eliminates human-generated syntax errors like missed commas and brackets. Porter believes that the future of coding will be a combination of AI and human interaction, as AI will allow humans to focus on the high-level coding skills needed for successful AI programming. \n Educators are updating teaching strategies to include AI-assisted learning and large language models (LLMs) capable of producing cod on demand. As Porter notes, \"We believe LLMs lower the barrier for understanding how to program [2].\"\u00a0 \n It\u2019s clear that AI will change the way programmers work. Developers could experience a boost in productivity and job satisfaction thanks to AI's assistance.\u00a0 Read more: Will AI Replace Programmers and Software Engineers?\n Will AI replace coding jobs? \n ChatGPT was released in 2022 and quickly surprised the coding community when it successfully created simple HTML websites using written instructions. The concern that coding could soon be obsolete seemed like a natural progression. However, like many jobs, AI will likely transform programming, working with humans to speed development. As Amazon\u2019s Vice President for AI Services told IEEE Spectrum, \u201cI don\u2019t believe AI is anywhere near replacing human developers\u201d [3] \n AI will likely assume repetitive, routine tasks in the future. Doing so will free human developers and programmers to focus on the high-level tasks and the creative side of their work.\u00a0 \n How to choose an AI programming language\u00a0 \n To choose which AI programming language to learn, consider your current abilities, skills, and career aspirations. For example, if you\u2019re new to coding, Python can offer an excellent starting point. This flexible, versatile programming language is relatively simple to learn, allowing you to create complex applications, which is why many developers start with this language. It also has an extensive community, including a substantial one devoted to using Python for AI.\u00a0 \n From there, it\u2019s helpful to consider your career goals. For example, if you want to create AI-powered mobile applications, you might consider learning Java, which offers a combination of easy use and simple debugging. Java is also an excellent option for anyone interested in careers that involve implementing machine learning programs or building AI infrastructure.\u00a0 \n If your professional interests are more focused on data analysis, you might consider learning Julia. This relatively new programming language allows you to conduct multiple processes at once, making it valuable for various uses in AI, including data analysis and building AI apps. However, if you want to work in areas such as autonomous cars or robotics, learning C++ would be more beneficial since the efficiency and speed of this language make it well-suited for these uses.\u00a0 \n Next steps with Coursera \n Coding will remain an in-demand skill\u2014both in AI and traditional settings\u2014for years to come. Build your coding skills with online courses like Python for Data Science, AI, & Development from IBM or Princeton University\u2019s Algorithms, Part 1, which will help you gain experience with Java.\u00a0 You can also gain a more robust foundation in AI with courses like AI for Everyone from DeepLearning.AI. This beginner-level class will teach you about AI terminology, strategies, and workflows.\u00a0                   \n Article sources Gartner. \u201cGartner Says More Than 80% of Enterprises Will Have Used Generative AI APIs or Deployed Generative AI-Enabled Applications by 2026, https://www.gartner.com/en/newsroom/press-releases/2023-10-11-gartner-says-more-than-80-percent-of-enterprises-will-have-used-generative-ai-apis-or-deployed-generative-ai-enabled-applications-by-2026.\u201d Accessed March 21, 2024.\u00a0 UC San Diego Today. \u201cIn This Era of AI, Will Everyone Be a Programmer?, https://today.ucsd.edu/story/in-this-era-of-ai-will-everyone-be-a-programmer.\u201d Accessed March 21, 2024.\u00a0 IEEE Spectrum. \u201cCoding Made AI\u2014Now, How Will AI Unmake Coding?, https://spectrum.ieee.org/ai-code-generation-language-models.\u201d Accessed March 21, 2024.\u00a0\n  Keep reading             Coursera Staff Editorial Team Coursera\u2019s editorial team is comprised of highly experienced professional editors, writers, and fact... This content has been made available for informational purposes only. Learners are advised to conduct additional research to ensure that courses and other credentials pursued meet their personal, professional, and financial goals.   Coursera Footer Get Started with AI Popular Career Certificates Popular Subjects Popular Resources Coursera Community More"}
{"url": "https://www.analyticsvidhya.com/blog/2024/01/best-libraries-for-machine-learning-explained/", "title": "10 Best Libraries for Machine Learning with Examples", "content": "Mastering Python\u2019s Set Difference: A Game-Changer for Data Wrangling Reading list \n\n                Intoduction to Python            \n \n\n                Variables and data types            \n \n\n                OOPs Concepts            \n \n\n                Conditional statement            \n \n\n                Looping Constructs            \n \n\n                Data Structures            \n \n\n                String Manipulation            \n \n\n                Functions            \n \n\n                Modules, Packages and Standard Libraries            \n \n\n                Python Libraries for Data Science            \n \n\n                Reading Data Files in Python            \n \n\n                Preprocessing, Subsetting and Modifying Pandas Dataframes            \n \n\n                Sorting and Aggregating Data in Pandas            \n \n\n                Visualizing Patterns and Trends in Data            \n \n\n                Programming            \n 10 Libraries for Machine Learning with Examples Machine learning has revolutionized the field of data analysis and predictive modelling. With the help of machine learning libraries, developers and data scientists can easily implement complex algorithms and models without writing extensive code from scratch. In this article, we will explore the top ten libraries for machine learning and understand their features, use cases, pros, and cons. Whether you are a beginner or an experienced professional, these libraries will undoubtedly enhance your machine-learning capabilities. In this article, you will discover the\u00a0best libraries for machine learning in Python, including essential\u00a0machine learning libraries\u00a0and specialized\u00a0deep learning libraries in Python\u00a0that enhance your data analysis capabilities. Table of contents What is Machine Learning? Machine learning is a subset of artificial intelligence that focuses on developing algorithms and models that enable computers to learn from data and make predictions or decisions without being explicitly programmed. It involves using statistical techniques and algorithms to analyze and interpret patterns in data, allowing machines to improve their performance over time. Importance of Libraries in Machine Learning Machine learning libraries play a crucial role in simplifying the implementation of complex machine learning algorithms and models. They provide pre-built functions and classes that can be easily integrated into your code, saving you time and effort. These libraries also offer various tools and utilities for data preprocessing, feature selection, model evaluation, and data visualization. By leveraging these libraries, developers can focus more on the core logic of their machine-learning projects rather than getting caught up in the nitty-gritty details. Top 10 ML Libraries Here are top 10 libraries for machine learning (ML): Library 1: NumPy Overview and Features NumPy is a fundamental library for scientific computing in Python. It supports large, multidimensional arrays and matrices and a collection of mathematical functions to operate on these arrays efficiently. NumPy is widely used in machine learning for data manipulation, numerical operations, and linear algebra computations. Use Cases and Applications NumPy is extensively used in various machine learning applications, including image processing, natural language processing, and data analysis. In image processing, NumPy arrays represent images, and the library\u2019s functions enable operations such as cropping, resizing, and filtering. Pros and Cons of NumPy Pros Cons Getting Started Guide To get started with NumPy, you can install it using the following command: Here\u2019s an example code snippet that demonstrates the creation of a NumPy array and performing basic operations: Also read: The Ultimate NumPy Tutorial for Data Science Beginners Library 2: Pandas Overview and Features Pandas is a powerful library for data manipulation and analysis. It provides data structures like DataFrames and Series for efficient, structured data handling. Pandas offers a wide range of data cleaning, transformation, and exploration functions, making it an essential tool for machine learning tasks. Use Cases and Applications Pandas are extensively used in data preprocessing, feature engineering, and exploratory data analysis. It enables tasks such as data cleaning, missing value imputation, and data aggregation. Pandas also integrates well with other libraries like NumPy and Matplotlib, facilitating seamless data analysis and visualization. Pros and Cons of Pandas Pros Cons Getting Started Guide To get started with Pandas, you can install it using the following command: Here\u2019s an example code snippet that demonstrates the creation of a DataFrame and performing basic operations: Also read: The Ultimate Guide to Pandas For Data Science! Library 3: Matplotlib Overview and Features Matplotlib is a popular library for data visualization in Python. It provides a wide range of functions and classes for creating various types of plots, including line plots, scatter plots, bar plots, and histograms. Matplotlib is highly customizable and allows for detailed control over plot aesthetics. Use Cases and Applications Matplotlib is extensively used in machine learning for visualizing data distributions, model performance, and feature importance. It enables the creation of informative and visually appealing plots that aid in data exploration and model interpretation. Matplotlib integrates well with other libraries like NumPy and Pandas, making it a versatile tool for data visualization. Pros and Cons of Matplotlib Pros Cons Getting Started Guide To get started with Matplotlib, you can install it using the following command: Here\u2019s an example code snippet that demonstrates the creation of a line plot using Matplotlib: Also read: Introduction to Matplotlib using Python for Beginners Library 4: Scikit-learn Overview and Features Scikit-learn is a comprehensive machine-learning library that provides various algorithms and tools for various tasks, including classification, regression, clustering, and dimensionality reduction. It offers a consistent API and supports integration with other libraries like NumPy and Pandas. Use Cases and Applications Scikit-learn is extensively used in machine learning projects for classification, regression, and model evaluation tasks. It provides a rich set of algorithms and functions for feature selection, model training, and performance evaluation. Scikit-learn also offers utilities for data preprocessing, cross-validation, and hyperparameter tuning. Pros and Cons of Scikit-learn Pros Cons Getting Started Guide To get started with Scikit-learn, you can install it using the following command: Here\u2019s an example code snippet that demonstrates the training of a classification model using Scikit-learn: Also read:15 Most Important Features of Scikit-Learn! Library 5: SciPy Overview and Features SciPy is a library for scientific computing in Python. It provides various functions and algorithms for numerical integration, optimization, signal processing, and linear algebra. SciPy builds on top of NumPy and provides additional functionality for scientific computing tasks. Use Cases and Applications SciPy is extensively used in machine learning for optimization, signal processing, and statistical analysis tasks. It offers functions for numerical integration, interpolation, and solving differential equations. SciPy also provides statistical distributions and hypothesis-testing functions, making it a valuable tool for data analysis and modelling. Pros and Cons of SciPy Pros Cons Getting Started Guide To get started with SciPy, you can install it using the following command: Here\u2019s an example code snippet that demonstrates the calculation of the definite integral using SciPy: Library 6: PyTorch Overview and Features PyTorch is a popular deep-learning library that provides a flexible and efficient framework for building and training neural networks. It offers dynamic computational graphs, automatic differentiation, and GPU acceleration, making it a preferred choice for deep learning research and development. Use Cases and Applications PyTorch is extensively used in deep learning projects for tasks such as image classification, object detection, and natural language processing. It provides many pre-built neural network architectures, modules, optimization algorithms, and loss functions. PyTorch also supports transfer learning and model deployment on various platforms. Pros and Cons of PyTorch Pros Cons Getting Started Guide To get started with PyTorch, you can install it using the following command: Here\u2019s an example code snippet that demonstrates the training of a simple neural network using PyTorch: Also read: An Introduction to PyTorch \u2013 A Simple yet Powerful Deep Learning Library Library 7: Keras Overview and Features Keras is a high-level deep-learning library that provides a user-friendly interface for building and training neural networks. It offers a wide range of pre-built layers, activation, and loss functions, making it easy to create complex neural network architectures. Keras supports CPU and GPU acceleration and can seamlessly integrate with other deep learning libraries like TensorFlow. Use Cases and Applications Keras is extensively used in deep learning projects for tasks such as image recognition, text classification, and generative modeling. It provides a simple and intuitive API for defining and training neural networks, allowing rapid prototyping and experimentation. Keras also supports transfer learning and model deployment on various platforms. Pros and Cons Keras Pros Cons Getting Started Guide To get started with Keras, you can install it using the following command: Here\u2019s an example code snippet that demonstrates the training of a simple convolutional neural network using Keras: Also read: Tutorial: Optimizing Neural Networks using Keras (with Image recognition case study) Library 8: TensorFlow Overview and Features TensorFlow is an open-source deep learning framework developed by Google, designed for building and deploying machine learning models at scale. It offers a flexible platform that supports both low-level customizations with TensorFlow Core and high-level APIs like Keras for ease of use. TensorFlow excels in handling complex computations, supports distributed computing, and integrates seamlessly with tools like TensorBoard for model visualization. Its extensive ecosystem includes TensorFlow Lite for mobile deployment and TensorFlow.js for web applications, making it a versatile choice for diverse ML tasks. Use Cases and Applications TensorFlow is widely used in applications such as natural language processing (NLP), computer vision, reinforcement learning, and speech recognition. It powers real-world solutions like recommendation systems, fraud detection, healthcare diagnostics, and self-driving car systems. Companies leverage TensorFlow for prototyping and deploying AI at scale in areas like predictive analytics and robotics. Pros and Cons Pros Cons Getting Started Guide Here is an Example Code for TensorFlow Libraries used in Machine Learning: Library 9: LightGBM Overview and Features LightGBM is a gradient boosting framework developed by Microsoft, optimized for speed and efficiency. It excels at handling large datasets and offers features like histogram-based algorithms, leaf-wise tree growth, and GPU support. These innovations make LightGBM faster and more memory-efficient than traditional boosting frameworks. With built-in support for categorical features, it reduces preprocessing overhead, making it a go-to solution for high-performance machine learning tasks. Use Cases and Applications LightGBM is ideal for tasks requiring high accuracy, such as financial modeling, ranking, and anomaly detection. It is widely used in Kaggle competitions and real-world projects like credit scoring, sales prediction, and click-through rate (CTR) estimation, where fast training and inference are critical. Pros and Cons Pros Cons Getting Started Guide Here is an Example Code for LightGBM Libraries for Machine Learning: Library 10: XGBoost Overview and Features XGBoost (eXtreme Gradient Boosting) is a powerful, open-source gradient boosting framework designed for speed and performance. It is known for its regularization capabilities, robust handling of missing data, and parallelized computation. XGBoost\u2019s scalability makes it suitable for large-scale datasets, and its advanced tree-pruning algorithms enhance accuracy and reduce overfitting. The framework\u2019s flexibility and cross-platform compatibility have made it a favorite in data science competitions and real-world applications. Use Cases and Applications XGBoost is extensively used in predictive modeling tasks, including classification, regression, and ranking problems. Common applications include customer segmentation, fraud detection, demand forecasting, and medical diagnosis. Its versatility also makes it popular in competitive data science platforms like Kaggle. Pros and Cons Pros Cons Getting Started Guide Here is an Example Code for XGBoost Libraries for Machine Learning: You can also check the Machine Learning course here:  Factors to Consider When Choosing a Machine Learning Library When choosing a machine learning library, there are several factors to consider: Conclusion In this article, we explored the 10 best libraries for machine learning and discussed their features, use cases, pros, and cons. NumPy, Pandas, Matplotlib, Scikit-learn, SciPy, PyTorch, and Keras are powerful tools that can significantly enhance your machine-learning capabilities. By leveraging these libraries, you can simplify the implementation of complex algorithms, perform efficient data manipulation and analysis, visualize data distributions, and build and train deep neural networks. Whether you are a beginner or an experienced professional, these deep-learning libraries are essential for your machine-learning journey. Unlock the future of technology with our Certified AI & ML BlackBelt Plus Program! Elevate your skills, gain industry-recognized certification, and become a master in Artificial Intelligence and Machine Learning. Don\u2019t miss out on this transformative opportunity. Enroll now and step into a world of limitless possibilities! Your journey to AI excellence begins here. Act fast; seats are limited! I hope you like the article! In data science, Python libraries for machine learning are very important. Some of the best libraries for machine learning in Python include popular machine learning libraries and specific deep learning libraries. Free Courses Generative AI - A Way of Life Explore Generative AI for beginners: create text and images, use top AI tools, learn practical skills, and ethics. Getting Started with Large Language Models Master Large Language Models (LLMs) with this course, offering clear guidance in NLP and model training made simple. Building LLM Applications using Prompt Engineering  This free course guides you on building LLM apps, mastering prompt engineering, and developing chatbots with enterprise data. Improving Real World RAG Systems: Key Challenges & Practical Solutions Explore practical solutions, advanced retrieval strategies, and agentic RAG systems to improve context, relevance, and accuracy in AI-driven applications. Microsoft Excel: Formulas & Functions Master MS Excel for data analysis with key formulas, functions, and LookUp tools in this comprehensive course. Recommended Articles \n          15+ Github Machine Learning Repositories for Da...           \n          Top 10 Machine Learning Libraries You Should Kn...           \n          Top 15 Libraries in Python You Must Know In 2024           \n          Top 20 Python libraries for data science Aspira...           \n          Top 40 Python Libraries for AI, ML and Data Sci...           \n          The Hidden Gems of Python \u2013 Libraries tha...           \n          Don\u2019t Miss out on these 24 Amazing Python...           \n          Top 10 Python Libraries for Data Analysis           \n          TOP 10 GitHub Repositories for Data Science           \n          10 Must Read Machine Learning Research Papers           Responses From Readers ClearSubmit reply \n\n  \u0394 \n                  Write for us\n                  \n\n\n\n Write, captivate, and earn accolades and rewards for your work \n                    We use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our  Privacy Policy & Cookies Policy.\n                 Show details Powered By  \n                            \n Cookies This site uses cookies to ensure that you get the best experience possible. To learn more about how we use cookies, please refer to our Privacy Policy & Cookies Policy. \n\nNecessary (2)\nNecessary cookies help make a website usable by enabling basic functions like page navigation and access to secure areas of the website. The website cannot function properly without these cookies.\n\n Necessary (2) Necessary cookies help make a website usable by enabling basic functions like page navigation and access to secure areas of the website. The website cannot function properly without these cookies. \n\nAnalytics Vidhya (4)\n\n                                                        learn more about analytics vidhya privacy\n                                                    \n\n Analytics Vidhya (4) brahmaid It is needed for personalizing the website. Expiry: Session Type: HTTP csrftoken This cookie is used to prevent Cross-site request forgery (often abbreviated as CSRF) attacks of the website Expiry: Session Type: HTTPS Identityid Preserves the login/logout state of users across the whole site. Expiry: Session Type: HTTPS sessionid Preserves users' states across page requests. Expiry: Session Type: HTTPS \n\nGoogle (1)\n\n                                                        learn more about google privacy\n                                                    \n\n Google (1) g_state Google One-Tap login adds this g_state cookie to set the user status on how they interact with the One-Tap modal. Expiry: 365 days Type: HTTP \n\nStatistics (4)\nStatistic cookies help website owners to understand how visitors interact with websites by collecting and reporting information anonymously.\n\n Statistics (4) Statistic cookies help website owners to understand how visitors interact with websites by collecting and reporting information anonymously. \n\nMicrosoft (7)\n\n                                                        learn more about microsoft policy\n                                                    \n\n Microsoft (7) MUID Used by Microsoft Clarity, to store and track visits across websites. Expiry: 1 Year Type: HTTP _clck Used by Microsoft Clarity, Persists the Clarity User ID and preferences, unique to that site, on the browser. This ensures that behavior in subsequent visits to the same site will be attributed to the same user ID. Expiry: 1 Year Type: HTTP _clsk Used by Microsoft Clarity, Connects multiple page views by a user into a single Clarity session recording. Expiry: 1 Day Type: HTTP SRM_I Collects user data is specifically adapted to the user or device. The user can also be followed outside of the loaded website, creating a picture of the visitor's behavior. Expiry: 2 Years Type: HTTP SM Use to measure the use of the website for internal analytics Expiry: 1 Years Type: HTTP CLID The cookie is set by embedded Microsoft Clarity scripts. The purpose of this cookie is for heatmap and session recording. Expiry: 1 Year Type: HTTP SRM_B Collected user data is specifically adapted to the user or device. The user can also be followed outside of the loaded website, creating a picture of the visitor's behavior. Expiry: 2 Months Type: HTTP \n\nGoogle (7)\n\n                                                        learn more about google privacy\n                                                    \n\n Google (7) _gid This cookie is installed by Google Analytics. The cookie is used to store information of how visitors use a website and helps in creating an analytics report of how the website is doing. The data collected includes the number of visitors, the source where they have come from, and the pages visited in an anonymous form. Expiry: 399 Days Type: HTTP _ga_# Used by Google Analytics, to store and count pageviews. Expiry: 399 Days Type: HTTP _gat_# Used by Google Analytics to collect data on the number of times a user has visited the website as well as dates for the first and most recent visit. Expiry: 1 Day Type: HTTP collect Used to send data to Google Analytics about the visitor's device and behavior. Tracks the visitor across devices and marketing channels. Expiry: Session Type: PIXEL AEC cookies ensure that requests within a browsing session are made by the user, and not by other sites. Expiry: 6 Months Type: HTTP G_ENABLED_IDPS use the cookie when customers want to make a referral from their gmail contacts; it helps auth the gmail account. Expiry: 2 Years Type: HTTP test_cookie This cookie is set by DoubleClick (which is owned by Google) to determine if the website visitor's browser supports cookies. Expiry: 1 Year Type: HTTP \n\nWebengage (2)\n\n                                                        Learn more about webengage privacy\n                                                    \n\n Webengage (2) _we_us this is used to send push notification using webengage. Expiry: 1 Year Type: HTTP WebKlipperAuth used by webenage to track auth of webenagage. Expiry: Session Type: HTTP \n\nLinkedIn (16)\n\n                                                        learn more about linkedin privacy\n                                                    \n\n LinkedIn (16) ln_or Linkedin sets this cookie to registers statistical data on users' behavior on the website for internal analytics. Expiry: 1 Day Type: HTTP JSESSIONID Use to maintain an anonymous user session by the server. Expiry: 1 Year Type: HTTP li_rm Used as part of the LinkedIn Remember Me feature and is set when a user clicks Remember Me on the device to make it easier for him or her to sign in to that device. Expiry: 1 Year Type: HTTP AnalyticsSyncHistory Used to store information about the time a sync with the lms_analytics cookie took place for users in the Designated Countries. Expiry: 6 Months Type: HTTP lms_analytics Used to store information about the time a sync with the AnalyticsSyncHistory cookie took place for users in the Designated Countries. Expiry: 6 Months Type: HTTP liap Cookie used for Sign-in with Linkedin and/or to allow for the Linkedin follow feature. Expiry: 6 Months Type: HTTP visit allow for the Linkedin follow feature. Expiry: 1 Year Type: HTTP li_at often used to identify you, including your name, interests, and previous activity. Expiry: 2 Months Type: HTTP s_plt Tracks the time that the previous page took to load Expiry: Session Type: HTTP lang Used to remember a user's language setting to ensure LinkedIn.com displays in the language selected by the user in their settings Expiry: Session Type: HTTP s_tp Tracks percent of page viewed Expiry: Session Type: HTTP AMCV_14215E3D5995C57C0A495C55%40AdobeOrg Indicates the start of a session for Adobe Experience Cloud Expiry: Session Type: HTTP s_pltp Provides page name value (URL) for use by Adobe Analytics Expiry: Session Type: HTTP s_tslv Used to retain and fetch time since last visit in Adobe Analytics Expiry: 6 Months Type: HTTP li_theme Remembers a user's display preference/theme setting Expiry: 6 Months Type: HTTP li_theme_set Remembers which users have updated their display / theme preferences Expiry: 6 Months Type: HTTP \n\nPreferences (0)\nPreference cookies enable a website to remember information that changes the way the website behaves or looks, like your preferred language or the region that you are in.\n\n Preferences (0) Preference cookies enable a website to remember information that changes the way the website behaves or looks, like your preferred language or the region that you are in. \n\nMarketing (4)\nMarketing cookies are used to track visitors across websites. The intention is to display ads that are relevant and engaging for the individual user and thereby more valuable for publishers and third party advertisers.\n\n Marketing (4) Marketing cookies are used to track visitors across websites. The intention is to display ads that are relevant and engaging for the individual user and thereby more valuable for publishers and third party advertisers. \n\nGoogle (11)\n\n                                                        learn more about google privacy\n                                                    \n\n Google (11) _gcl_au Used by Google Adsense, to store and track conversions. Expiry: 3 Months Type: HTTP SID Save certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search. Expiry: 2 Years Type: HTTP SAPISID Save certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search. Expiry: 2 Years Type: HTTP __Secure-# Save certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search. Expiry: 2 Years Type: HTTP APISID Save certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search. Expiry: 2 Years Type: HTTP SSID Save certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search. Expiry: 2 Years Type: HTTP HSID Save certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search. Expiry: 2 Years Type: HTTP DV These cookies are used for the purpose of targeted advertising. Expiry: 6 Hours Type: HTTP NID These cookies are used for the purpose of targeted advertising. Expiry: 1 Month Type: HTTP 1P_JAR These cookies are used to gather website statistics, and track conversion rates. Expiry: 1 Month Type: HTTP OTZ Aggregate analysis of website visitors Expiry: 6 Months Type: HTTP \n\nFacebook (2)\n\n                                                        learn more about facebook privacy\n                                                    \n\n Facebook (2) _fbp This cookie is set by Facebook to deliver advertisements when they are on Facebook or a digital platform powered by Facebook advertising after visiting this website. Expiry: 4 Months Type: HTTP fr Contains a unique browser and user ID, used for targeted advertising. Expiry: 2 Months Type: HTTP \n\nLinkedIn (6)\n\n                                                        Learn about linkedin policy\n                                                    \n\n LinkedIn (6) bscookie Used by LinkedIn to track the use of embedded services. Expiry: 1 Year Type: HTTP lidc Used by LinkedIn for tracking the use of embedded services. Expiry: 1 Day Type: HTTP bcookie Used by LinkedIn to track the use of embedded services. Expiry: 6 Months Type: HTTP aam_uuid Use these cookies to assign a unique ID when users visit a website. Expiry: 6 Months Type: HTTP UserMatchHistory These cookies are set by LinkedIn for advertising purposes, including: tracking visitors so that more relevant ads can be presented, allowing users to use the 'Apply with LinkedIn' or the 'Sign-in with LinkedIn' functions, collecting information about how visitors use the site, etc. Expiry: 6 Months Type: HTTP li_sugr Used to make a probabilistic match of a user's identity outside the Designated Countries Expiry: 90 Days Type: HTTP \n\nMicrosoft (2)\n\n                                                        Learn more about microsoft privacy.\n                                                    \n\n Microsoft (2) MR Used to collect information for analytics purposes. Expiry: 1 year Type: HTTP ANONCHK Used to store session ID for a users session to ensure that clicks from adverts on the Bing search engine are verified for reporting purposes and for personalisation Expiry: 1 Day Type: HTTP \n\nUnclassNameified (0)\nUnclassNameified cookies are cookies that we are in the process of classNameifying, together with the providers of individual cookies.\n\n UnclassNameified (0) UnclassNameified cookies are cookies that we are in the process of classNameifying, together with the providers of individual cookies. Cookie declaration last updated on 24/03/2023 by Analytics Vidhya. Cookies are small text files that can be used by websites to make a user's experience more efficient. The law states that we can store cookies on your device if they are strictly necessary for the operation of this site. For all other types of cookies, we need your permission. This site uses different types of cookies. Some cookies are placed by third-party services that appear on our pages. Learn more about who we are, how you can contact us, and how we process personal data in our Privacy Policy. Flagship Courses  Free Courses Popular Categories Generative AI Tools and Techniques Popular GenAI Models Data Science Tools and Techniques Company Discover Learn Engage Contribute Enterprise \nTerms & conditions\n\n\n\nRefund Policy\n\n\n\nPrivacy Policy\n\n\n\nCookies Policy\n\u00a9 Analytics Vidhya 2024.All rights reserved.\n \nGenAI \nPinnacle \nProgram\n Revolutionizing AI Learning & Development Enroll with us today! Continue your learning for FREE Enter email address to continue  Enter OTP sent to \n\nEdit Resend OTP Resend OTP in 45s"}
{"url": "https://www.geeksforgeeks.org/best-python-libraries-for-machine-learning/", "title": "Best Python libraries for Machine Learning - GeeksforGeeks", "content": "Best Python libraries for Machine Learning Machine learning has become an important component in various fields, enabling organizations to analyze data, make predictions, and automate processes. Python is known for its simplicity and versatility as it offers a wide range of libraries that facilitate machine learning tasks. These libraries allow developers and data scientists to quickly and effectively implement complex algorithms. By using Python\u2019s tools, users can efficiently tackle machine learning projects and achieve better results. Best Python libraries for Machine Learning In this article, we\u2019ll dive into the Best Python libraries for Machine Learning, exploring how they facilitate various tasks like data preprocessing, model building, and evaluation. Whether you are a beginner just getting started or a professional looking to optimize workflows, these libraries will help you leverage the full potential of Machine Learning with Python. Python libraries for Machine Learning Here\u2019s a list of some of the best Python libraries for Machine Learning that streamline development: 1. Numpy NumPy is a very popular python library for large multi-dimensional array and matrix processing, with the help of a large collection of high-level mathematical functions. It is very useful for fundamental scientific computations in Machine Learning. It is particularly useful for linear algebra, Fourier transform, and random number capabilities. High-end libraries like TensorFlow uses NumPy internally for manipulation of Tensors.\u00a0 Example: Linear Algebra Operations Output:\u00a0 2. Pandas\u00a0 Pandas is a popular Python library for data analysis. It is not directly related to Machine Learning. As we know that the dataset must be prepared before training.  Example: Data Cleaning and Preparation Output: 3. Matplotlib Matplotlib is a very popular Python library for data visualization. Like Pandas, it is not directly related to Machine Learning. It particularly comes in handy when a programmer wants to visualize the patterns in the data. It is a 2D plotting library used for creating 2D graphs and plots. Example: Creating a linear Plot  Output: Output  4. SciPy SciPy is a very popular library among Machine Learning enthusiasts as it contains different modules for optimization, linear algebra, integration and statistics. There is a difference between the SciPy library and the SciPy stack. The SciPy is one of the core packages that make up the SciPy stack. SciPy is also very useful for image manipulation.\u00a0\u00a0 Example: Image Manipulation If scipy.misc import imread, imsave,imresize does not work on your operating system then try below code instead to proceed with above code\u00a0 Original image:\u00a0  Tinted image:\u00a0\u00a0  Resized tinted image:\u00a0\u00a0  5. Scikit-Learn Scikit-learn is one of the most popular ML libraries for classical ML algorithms. It is built on top of two basic Python libraries, viz., NumPy and SciPy. Scikit-learn supports most of the supervised and unsupervised learning algorithms. Scikit-learn can also be used for data-mining and data-analysis, which makes it a great tool who is starting out with ML.\u00a0 Example:  Decision Tree Classifier Output:\u00a0 \u00a06. Theano We all know that Machine Learning is basically mathematics and statistics. Theano is a popular python library that is used to define, evaluate and optimize mathematical expressions involving multi-dimensional arrays in an efficient manner. Example\u00a0 Output:\u00a0\u00a0 7. TensorFlow TensorFlow is a very popular open-source library for high performance numerical computation developed by the Google Brain team in Google. As the name suggests, Tensorflow is a framework that involves defining and running computations involving tensors. It can train and run deep neural networks that can be used to develop several AI applications. TensorFlow is widely used in the field of deep learning research and application.\u00a0 Example\u00a0 Output:\u00a0\u00a0 8. Keras Keras is a very popular Python Libaries for Machine Learning . It is a high-level neural networks API capable of running on top of TensorFlow, CNTK, or Theano. It can run seamlessly on both CPU and GPU. Keras makes it really for ML beginners to build and design a Neural Network. One of the best thing about Keras is that it allows for easy and fast prototyping. Example\u00a0 Output:\u00a0 9. PyTorch PyTorch is a popular open-source Python Library for Machine Learning based on Torch, which is an open-source Machine Learning library that is implemented in C with a wrapper in Lua. It has an extensive choice of tools and libraries that support Computer Vision, Natural Language Processing(NLP), and many more ML programs. It allows developers to perform computations on Tensors with GPU acceleration and also helps in creating computational graphs.\u00a0 Example Output:\u00a0 Conclusion In summary, Python\u2019s versatility, simplicity, and vast ecosystem make it a go-to choice for Machine Learning tasks. From Scikit-Learn for classical algorithms to TensorFlow and PyTorch for deep learning, Python libraries cater to every stage of the Machine Learning workflow. Libraries like Pandas and NumPy streamline data preprocessing, while Matplotlib and Seaborn aid in data visualization. Specialized tools such as NLTK, XGBoost, and LightGBM further enhance the ability to solve complex problems efficiently. R Similar Reads  What kind of Experience do you want to share?"}
{"url": "https://www.pragmaticcoders.com/resources/ai-developer-tools", "title": "Best AI tools for developers in 2024: AI-powered coding | Pragmatic Coders", "content": "Coding for Health & Finance Pioneers Best AI tools for developers in 2024: AI-powered codingLast updated: October 7, 2024 Best AI tools for developers in 2024: AI-powered coding Last updated: October 7, 2024  Artificial intelligence has forever changed the way software is developed. Automated tasks and improved teamwork, fueled by AI, are fundamentally rewriting the software lifecycle. This article explores AI tools for developers. We\u2019ll look at their key features, capabilities, benefits, limitations, and areas needing improvement. Understanding these technologies\u2019 current state helps developers and managers decide whether to integrate them into their workflows. The article covers Our aim is to provide developers, managers, and tech leaders with insights to make informed tool integration decisions for greater efficiency, productivity, and innovation. Newsletter More content like this directly to your inbox. No spam. You chose what interests you, and get only these messages. Glossary Article: A scientific publication about a particular tool. Recommendation: Cursor Website: https://www.cursor.com/ Recommendation: Recommended(especially for VSCode users) About Cursor Cursor is an AI-powered code editor designed to enhance developer productivity through advanced features that facilitate coding and collaboration. Key features Additional information Cursor has gained popularity among developers due to its intuitive interface and powerful AI capabilities. It uses advanced language models to understand context and provide more accurate suggestions compared to traditional autocomplete features. The tool also includes a feature called \u201cEdit in English,\u201d which allows developers to describe desired code changes in natural language, and the AI attempts to implement those changes. While Cursor is free to use, it offers a Pro plan with additional features such as GPT-4 integration, longer context windows, and priority support. The tool is regularly updated, with new features and improvements being added based on user feedback and advancements in AI technology. Opinion Aider GitHub: https://github.com/paul-gauthier/aider Recommendation: Recommended About Aider Aider is a command-line tool for AI-assisted pair programming, enabling collaboration with large language models (LLMs) like GPT-3.5 and GPT-4 directly in the terminal. Key features Additional information Aider stands out for its command-line interface, which appeals to developers who prefer terminal-based workflows. It integrates seamlessly with git, allowing for easy version control of AI-assisted changes. The tool is particularly useful for refactoring tasks, bug fixing, and adding new features to existing codebases. One of Aider\u2019s strengths is its ability to understand and work with the entire codebase context, not just individual files. This allows it to make more informed suggestions and changes that consider the project as a whole. Aider requires an OpenAI API key to function, which means users have more control over their data and can use their own API quotas. This can be both an advantage (in terms of privacy and cost control) and a limitation (requiring setup and potential additional costs) depending on the user\u2019s needs. Opinion GitLab Duo Website: https://about.gitlab.com/gitlab-duo/ Recommendation: Promising About GitLab Duo GitLab Duo is an advanced suite of AI capabilities integrated into GitLab, designed to enhance productivity throughout the software development lifecycle. Key features Additional information GitLab Duo represents GitLab\u2019s comprehensive approach to integrating AI throughout the DevOps lifecycle. Unlike standalone AI coding assistants, Duo is deeply integrated into the GitLab platform, offering AI assistance at various stages of development, from planning to deployment. The tool uses a combination of proprietary AI models and integrations with leading AI providers to offer its features. This approach allows GitLab to tailor the AI capabilities specifically to the needs of DevOps workflows while also leveraging cutting-edge language models for tasks like code generation and natural language interaction. GitLab Duo places a strong emphasis on security and compliance. It includes features that help identify and explain potential security vulnerabilities in code, making it easier for teams to maintain secure development practices. The organizational controls allow companies to manage how and where AI is used within their GitLab instances, addressing concerns about data privacy and code confidentiality. One of the unique aspects of GitLab Duo is its holistic approach to AI assistance in software development. It doesn\u2019t just focus on code writing but also aids in project management, code review, and even DevOps processes, making it a comprehensive tool for development teams using GitLab. Opinion OpenHands (previously OpenDevin) Website:\u00a0https://docs.all-hands.dev/modules/usage/intro\u00a0 GitHub: https://github.com/OpenDevin/OpenDevin\u00a0 Recommendation: Developing About OpenDevin OpenDevin is an autonomous AI software engineer capable of executing complex engineering tasks and collaborating on software development projects. Opinion AI software development services Need help with building an AI product? We create scalable AI solutions for new and established businesses and can integrate AI with your existing solution. Seamlessly. Gain a competitive edge with next-level AI software: Automate tasks and boost productivity. Transform your business with our expert AI integration services:  MetaGPT Website: https://docs.deepwisdom.ai/main/en/ Article: https://arxiv.org/abs/2308.00352\u00a0 Other resources: https://www.1001epochs.ch/blog/metagpt-for-future-of-work\u00a0 GitHub: https://github.com/geekan/MetaGPT Recommendation: Ignore About MetaGPT MetaGPT is a multi-agent framework based on Large Language Models (LLMs) that aims to redefine the paradigms of task execution, collaboration, and decision-making in the workplace. It consists of two primary layers: Key features of MetaGPT include role definitions, quick learning, knowledge sharing, and a human-centric approach. It offers benefits such as automation, integration of human SOPs, creative program generation, and enhanced performance through multiple AI agents. Benefits Compared to other LLM-based frameworks, MetaGPT stands out in terms of scalability, customizability, and consistent performance across diverse benchmarks. Its development philosophy emphasizes adaptability, user-centricity, and a collaborative ecosystem. Limitations However, MetaGPT is still under development and may not be ideal for highly intricate projects. Its capabilities are also restricted to its training data, necessitating frequent updates for accuracy. Key Points \u00a0 Tests Prompt Result Comment Simple TODO KTOR crud application \u2013 basic prompt (5, 10 and 15 round attempts) create simple\u00a0 todo crud application in Ktor with jwt authentication, and serialization Failure \u2013 Missing classes, build files, authentication or content negotiation, some classes generated in another language + Proper dependencies used Simple TODO KTOR crud application \u2013 advanced prompt (30 rounds) Create a simple TODO CRUD application in Ktor with JWT authentication and serialization.\u00a0 \u00a0 **Requirements:** \u2013 Use Ktor for building the server-side application \u2013 Implement a CRUD functionality for managing TODO items (Create, Read, Update, Delete) \u2013 Include JWT authentication for securing the endpoints \u2013 Use Kotlin serialization for handling JSON data \u2013 Include a `build.gradle` file for managing dependencies \u00a0 Feel free to ask if you need any help or further clarification. Failure \u2013 Missing classes, build files, authentication or content negotiation, some classes generated in another language + Proper dependencies used ChatDev (whitelist access only) Website: \u2013 Article: https://arxiv.org/pdf/2307.07924.pdf GitHub: https://github.com/OpenBMB/ChatDev?tab=readme-ov-file Recommendation: Ignore About ChatDev ChatDev that leverages large language models (LLMs) to streamline the entire software development process through natural language communication. Key points Devin AI (whitelist access only) Website: https://www.cognition-labs.com/introducing-devin Article: \u2013 GitHub: \u2013 Recommendation: Ignore About Devin Devin is a tireless, skilled teammate, equally ready to build alongside you or independently complete tasks for you to review. With Devin, engineers can focus on more interesting problems and engineering teams can strive for more ambitious goals. GPT Pilot Website: \u2013 Article: \u2013 GitHub: https://github.com/Pythagora-io/gpt-pilot Recommendation: Promising About GPT Pilot Here\u2019s how GPT Pilot builds apps, according to a quote from a project\u2019s GitHub README: Tests Result Comment Simple TODO KTOR crud application Failure Quite promising. It took GPT Pilot 2 hours, some assistance, and manual intervention to complete a basic app with only one endpoint. Despite this, the overall process shows potential. Most issues stemmed from dependency management, import errors, and missing code sections. The total cost of this experiment was around $15. Gorilla Website: https://gorilla.cs.berkeley.edu/ Article: https://arxiv.org/pdf/2305.15334.pdf GitHub: https://github.com/ShishirPatil/gorilla Recommendation: Ignore About Gorilla Gorilla enables LLMs to use tools by invoking APIs. Given a natural language query, Gorilla comes up with the semantically- and syntactically- correct API to invoke. With Gorilla, we are the first to demonstrate how to use LLMs to invoke 1,600+ (and growing) API calls accurately while reducing hallucination. We also release APIBench, the largest collection of APIs, curated and easy to be trained on! Join us, as we try to expand the largest API store and teach LLMs how to write them! Hop on our Discord, or open a PR, or email us if you would like to have your API incorporated as well. Korbit Website: https://www.korbit.ai/ Article: \u2013 GitHub: \u2013 Recommendation: Ignore About Korbit Korbit is an AI-powered tool designed for automatic pull request review. Tests Result Comment 200 line MR Failure 9 comments, focused on changed lines, useless in the context of the whole project 700 line MR Failure 19 comments Potential bugs: Simple issues like hardcoded values. Invalid comment: One comment incorrectly references an IndexOutOfBoundsException. Unnecessary comments: Many comments provide no useful information. 1800 line MR Failure 54 comments AI Code Review Action Website: \u2013 Article: \u2013 GitHub: https://github.com/marketplace/actions/ai-code-review-action Recommendation: Ignore About AI Code Review Action Tests Result Comment 200 line MR Failure 27 comments, focused on changed lines, useless in the context of the whole project.  Most comments focus on test naming, but these are invalid. 700 line MR Failure 52 comments Some comments are not about the diff code. Too many comments to get some value from them. 1800 line MR Failure Action failed. Context is too big for GPT-3.5. CodeRabbit Website: https://coderabbit.ai/ Article: \u2013 GitHub: https://github.com/marketplace/coderabbitai\u00a0 Recommendation: Promising About CodeRabbit Data, privacy, and security: CodeRabbit does not use data collected during code reviews to train or influence the models. Queries to the Large Language Models (LLMs) are ephemeral and there is zero retention on LLMs. Neither we nor the LLMs provider(s) share any data collected during the code review process with third parties. Tests Result Comment 200 line MR Success Walkthrough Analysis of Code Changes1 Comment Regarding TODO in Code 700 line MR Success Walkthrough analysys of code changes. 1800 line MR Success Walkthrough analysys of code changes. 4 comments, all of them guiding towards good practices. Supermaven Website: https://supermaven.com/ Article: \u2013 GitHub: \u2013 Recommendation: Ignore About Supermaven Tests Result Comment Endpoint definition on already build domain (java) Failure Suggestions without domain knowledge. Incomplete suggestions. Gemini Code Assist Website: https://cloud.google.com/code Article: \u2013 GitHub: \u2013 Recommendation: Promising About Gemini Google Cloud Code Tests Result Comment Endpoint definition on already build domain (Java) Success Suggestions with domain context, Suggestions including good practices, Quick response object generation including practices of existing ones Screen implementation (Flutter) Failure Flutter is currently not supported GitHub Copilot Website: https://github.com/features/copilot Article: \u2013 GitHub: \u2013 Recommendation: Recommended About GitHub Copilot Tests Result Comment Endpoint definition on already build domain (Java) Success Suggestions with domain context Screen implementation (Flutter) Success Useful suggestions for widgets displaying data based of objects AI developer tools: Conclusions A tl;dr version of our research. Automated code review agents: Comparative conclusion Honest opinion I currently see 3 use cases for these tools: AI-powered code assistants: Comparative analysis AI agents: Comparative analysis MetaGPT incorporates human-like workflows and standardized operating procedures (SOPs) to address the limitations of existing LLM-based approaches. It assigns specific roles and responsibilities to different agents, promoting a coherent and structured development process. MetaGPT\u2019s features include an executable feedback mechanism for continuous code verification and debugging, as well as a focus on knowledge sharing and collaboration. Similarly, GPT Pilot takes a step-by-step approach, with each agent (e.g., specification writer, architect, developer) playing a distinct role in the software development process. This structured workflow helps to mitigate the risk of cascading errors and inconsistencies. While these AI agent-based frameworks demonstrate the potential of integrating LLMs into software development, they are still in their early stages of development and not yet ready for widespread production use. In our tests, they were unable to generate a complete and functional TODO application with Ktor, JWT authentication, and serialization, highlighting the need for further refinement and maturation before they can be reliably used for complex software projects.  Contents AI developer tools FAQ Frequently asked questions about AI tools used in software development. \u00a0 What is generative AI and how does it benefit software developers? Generative AI, such as models developed by OpenAI, can create code snippets, automate repetitive tasks, and assist in debugging, significantly enhancing productivity and efficiency for software developers. What are AI developer tools? AI developer tools are a collection of software applications and libraries that assist developers in building, testing, and deploying artificial intelligence functionalities within their software. These tools can streamline workflows and improve the efficiency of AI development. How are AI models used in coding tools? AI models are integrated into coding tools and IDEs to provide intelligent code suggestions, automate error detection, and generate documentation, streamlining the development process. Who can benefit from AI developer tools? AI developer tools are beneficial for various developers, including those with experience in machine learning, data science, and traditional software development. Even beginners can leverage user-friendly tools to integrate basic AI features. What are the common use cases for AI developer tools? What types of AI developer tools are available? I developer tools come in many flavors, designed to assist programmers in various stages of the development workflow. Here's a breakdown of some common types: Code completion and assistants: These tools use AI to predict the next line of code, suggest code snippets, or even generate entire functions. Examples include Tabnine, JetBrains AI assistant, and aiXcoder. Code review and debugging: Tools in this category can analyze code for errors, suggest improvements, and even help with debugging complex problems. Some examples include Codium, Stepsize AI, and Sourcery. Documentation generation: These AI-powered tools can automatically generate documentation from your code, saving developers time and effort. Rewind.ai is a popular example. General AI assistants: Some development environments like Replit include built-in chatbots powered by AI that can answer questions, provide suggestions, and even help with debugging. UI/UX Design assistants: There are AI tools that can help with designing user interfaces by generating mockups or suggesting layouts based on user data. While these aren't strictly code-focused, they can be valuable for developers involved in the entire application creation process. How do you use AI in your development process? At Pragmatic Coders, we use AI tools to generate code, brainstorm, or streamline daily tasks. Learn more: AI in software development: how we\u2019re saving clients\u2019 time & money Why do you need to integrate AI developer tools into your product lifecycle ASAP? Artificial intelligence is crucial to do things faster: experiment, make mistakes, and learn from then. Joe Justice, ex-Tesla employee and Agile coach shared with us his observations on AI implementation: Most importantly, AI is crucial to innovate, which you most probably want to do if you're building digital products. Learn more: How Elon Musk\u2019s innovation strategy can fuel your app\u2019s success How do AI-powered IDEs enhance productivity? AI-powered IDEs, like Visual Studio Code with Copilot, offer intelligent features that assist developers in writing, optimizing, and debugging code more efficiently.  What is the role of OpenAI in advancing AI tools for developers? OpenAI has developed powerful AI models like GPT-4 that are integrated into various development tools, providing advanced assistance in coding, debugging, and project management. \u00a0 Custom AI Software Development Services %%sep%% %%sitename%% AI Integration Services - Start Using AI Solutions %%sep%% %%sitename%% Mobile Application Development Services Custom Web Application Development Company %%sep%% %%sitename%% Research authors Let's talk We\u2019ve got answers on anything connected with software development. Ask a question You can ask us a question using the form below. We will respond as soon as possible. Schedule a meeting You can also schedule an online meeting with Wojciech, our Senior Business Consultant. Senior Business Consultant founders who contacted us wantedto work with our team. Check our AI-related articles How to Predict Customer Behavior With AI in 2025 AI-Powered Rapid UI Prototyping for Android & iOS Apps How to use AI in accounting? Newsletter You are just one click away from receiving our 1-min business newsletter. Get insights on product management, product design, Agile, fintech, digital health, and AI. LOOK INSIDE ul. Opolska 100 31-323 Krak\u00f3w, Poland VAT ID: PL 6772398603 [email\u00a0protected] +48 793 389 751"}
{"url": "https://stepsize.com/blog/best-ai-tools-for-software-developers", "title": "9 of the Best AI Tools for Software Developers in 2024", "content": "Pricing Security Pricing Security 9 of the Best AI Tools for Software Developers in 2024 9 of the Best AI Tools for Software Developers in 2024 2024 is here, so I think it\u2019s the perfect time for us to take stock of the very best AI tools for developers that we can use to get ahead in the new year and improve our development processes. So many new artificial intelligence (AI) tools have been released this year as large language models like ChatGPT have gathered meteoric pace. So I\u2019ve picked out the very best tools every software developer should know about and have a play with. 1. Stepsize AI: Effortless Sprint Reporting Stepsize AI writes perfect sprint reports for us. Your team will thank you for introducing this time-saving (and effort-saving) tool. It writes sprint reports better than humans can, and that engineering leaders, POs and Scrum Masters adore. It\u2019s designed for teams using platforms like Jira or Linear. Stepsize AI automatically creates sprint reports with the perfect amount of context and detail. With Stepsize AI, everyone\u2019s aligned an on the same page about product development progress. The AI processes data from your issue tracker, forms connections between projects and tasks and uses this to surface the perfect level of context and commentary. Key features Try it free? Yes, you can get your first report for free, and you don\u2019t need a credit card. You\u2019ll need permission to integrate with your issue tracker. My view \u2013 Developers love using Stepsize AI to stay informed, and maybe more importantly, never have to write a sprint report (or be bothered for updates by the PO!) It\u2019s great for providing the pertinent details without bombarding people with unnecessary info.\u00a0 Learn more about Stepsize AI here. You can get your first sprint report for free 2. Cody AI: Advanced Code Assistant Cody AI accelerates our process of writing code. If you\u2019re not using an AI coding assistant, you\u2019re being left behind. (And if you\u2019re still using GitHub Copilot, that\u2019s probably still true!) They save time, and can help you out in all kinds of ways from suggesting code to automatically generating entire files through their natural language processing capabilities, and more. This list has a few AI coding assistants and Cody is the first. By deeply analyzing your existing code, documentation, and code graph, Cody can give you speedy and insightful responses to queries about your codebase.\u00a0 Key features Try it free? Yes, most features are available for free with a generous rate limit. My view \u2013 For engineers dealing with expansive codebases, Cody is a great option. It enables developersunderstand what we\u2019re looking at, write better code faster, and ultimately allows more time for actual coding. Try Cody here 3. Tabnine: Established Code Assistant Tabnine is a reliable, security-conscious AI code assistant that it\u2019s easier to get approval for. Now built on GPT-3.5 and having evolved over five years, Tabnine is transparent, compliant, and offers a plethora of valuable features to help us devs write code faster. Because it\u2019s been knocking around for a while, they\u2019ve had time to deliver strong, trustworthy security and compliance credentials that your managers will be reassured by. It\u2019s not the most powerful AI coding assistant on this list, but it might be worth the trade-off to be allowed to use it! Key features Try it free? Yes, but it\u2019s very limited \u2013 you just get short code completions. Whole-line and function completions are available for $12/mo/user at the time of writing. My view \u2013 Tabnine is perfect for devs working for teams that are prioritising security and dependability over the latest tech novelties. Like its competitors, it integrates with popular IDEs (integrated development environments) seamlessly. Check out Tabnine here 4. Mutable AI: Powerful Code Assistant Mutable AI accelerates software development using AI. Aiming to eliminate the need for repetitive boilerplate code and unnecessary searches, Mutable ensures faster and better code delivery running on proprietary AI. It\u2019s the youngest code assistant on this list but perhaps one of the most high-tech and performant ones, with a rapid pace of development even compared to competitors.\u00a0 Key features Try it free? Yes, at the time of writing you get a generous number of codebase chat and search queries and file edit calls for you to give it a spin. More features unlock at $25/mo/user and $50/mo/user My view \u2013 Mutable stands out for its superior code completion capabilities, especially over multiple files. Check out Mutable AI 5. Codium AI: Test-Writing Assistant Codium ensures your code functions as intended. By suggesting tests during coding, Codium helps us avoid pesky errors and bugs. As you code, the AI suggests intelligent test ideas, and can write them for you. Plus, it can show you how your code changes impact the wider system.\u00a0 Key features Try it free? Yes, you can try all the key features for free, but you have to pay $19/mo/user at the time of writing to add tests to existing test suites, get enhanced security and a few other features. My view \u2013 Maybe you love writing test suites, but if you\u2019re like the average dev, it probably isn\u2019t in your top 3 activities. And introducing a big bug by accident can probably ruin your day, and maybe your evening. Codium helps us ship with more confidence, without the hassle of painstakingly writing tests. Visit Codium AI 6. Mintlify Writer: Code Documentation Writer Mintlify Writer writes your docs for you. This tool should be a fan favourite. It takes all the pain out of (arguably) one of the most painful and annoying parts of engineering \u2013 writing the docs. It automates the creation of context-aware documentation, allowing more focus on the fun stuff. Key features Try it free? Yes My view \u2013 Welcome to a world where the docs write themselves. You\u2019re very welcome. Check out Mintlify Writer here 7. Grit.io: Code Migration Simplifier Grit.io automates technical debt management. Grit, currently in beta, specialises in handling tasks like code migrations and dependency upgrades. With Grit, intimidating chores turn into straightforward tasks. Key features Try it free? Yes, at the moment Grit.io is in beta and is free for teams of up to 20 developers. My view \u2013 With Grit, we no longer have to have sprints dedicated to migrations. Grit is ideal for teams juggling legacy systems or frequent code migrations. It makes it much easier to get up-to-date with technology updates without derailing ongoing projects. Learn about Grit.io here 8. WhatTheDiff: Code Review Optimizer WhatTheDiff offers automated code review suggestions. By providing automated suggestions and summaries, WhatTheDiff simplifies the intricate process of code reviews, especially for extensive codebases or regular pull requests. Plus, it\u2019ll describe your pull requests for you. Key features Try it free? Yes, you get a quota of tokens that equates to roughly 10 PRs for free with no credit card required. Or upgrade for $19/mo for roughly 40 PRs at the time of writing. My view \u2013 If you\u2019re working in a high-speed development team where frequent pull requests are a fact of life, WhatTheDiff makes the process of code reviews much faster and more painless Check out WhatTheDiff here 9. Bugasura: Efficient Bug Tracker Bugasura optimises bug reporting and resolution. By consolidating all your bugs in one place and using AI to link similar bugs, it ensures we spend less time on bug detection and duplicate reports. \u200dKey features Try it free? Yes, all features are free for up to 5 users. For larger teams, it\u2019s $5/mo/user. My view \u2013 Bugasura is a blessing for engineers dealing with a deluge of bug reports, particularly if they use diverse sources for bug management. Check out Bugasura here That\u2019s it. Embracing the right AI development tools already gives savvy software developers a competitive edge. I hope you\u2019ve found these suggestions helpful. I\u2019d love to know if I\u2019ve missed any AI software for developers that you recommend. I\u2019ve been working on Stepsize AI, a tool that effortlessly gets people aligned through effortless sprint reporting. You can get your first report for free (you don\u2019t need a credit card) so I\u2019d love to hear your feedback. You can check out Stepsize AI here. More articles 6 Best-In-Class Slack Apps & Integrations for Software Teams There are a tonne of Slack apps and integrations. Here are 6 of the very best Slack add-ons for software developers, from summarisation to bug tracking. Most teams are fumbling their sprint reviews What's the challenge with sprint reviews, and why do so many antipatterns creep in? Here's why it happens, and four things we can do about it. How to Leverage AI to Enhance Developer Experience Discover how AI can significantly enhance the Developer Experience (DX), boosting productivity and innovation. Learn about groundbreaking AI tools transforming software development workflows. Watch this webinar Register for this webinar You've been registered for Please check your email for details of how to access the webinar. Get started with Stepsize AI Security Cookies Privacy Policy Terms of Use Information Security"}
{"url": "https://developer.microsoft.com/en-us/ai", "title": "AI for developers | Microsoft Developer", "content": "Global Introducing Azure AI Foundry \u2013 your all-in-one toolkit for building transformative AI apps. Accelerate your AI projects with a comprehensive toolkit Explore the comprehensive development toolkits from Microsoft + NVIDIA and accelerate application innovation. Menu Our partner Explore solutions Webinar AI Machine learning Orchestration Connect Stories Our partner Microsoft + NVIDIA Together, Microsoft and NVIDIA provide developers and data scientists with the toolsets needed to reduce complexity, accelerate time to deployment, and scale faster. Learn, build, and scale AI apps Build AI apps for popular GenAI use cases quickly and easily with AI app templates. Explore solutions Build with a complete toolchain Visual Studio Code Free, open-source code editor with debugging, syntax highlighting, code completion, snippets, and refactoring. GitHub Web-based platform for hosting, discovering, and collaborating on software projects. Azure AI Foundry Web-based platform for learning and perfecting generative AI, with cutting-edge models, data integration, model management, and content safety. Azure SDK Collection of libraries that provide a consistent and idiomatic programming experience for Azure services, with support for multiple languages and platforms. Azure Developer CLI Cross-platform command-line tool that lets you manage Azure services and perform tasks like creating and managing resources and deploying applications. NVIDIA AI Enterprise End-to-end software platform accelerating data science pipelines while streamlining development and deployment of production-grade generative AI applications. NVIDIA AI Foundation Endpoints Providing fully serverless and scalable APIs that can be deployed on Azure or NVIDIA DGX Cloud. NVIDIA GPU-accelerated VMs ND-series Azure VMs designed for deep learning, AI research, and high-performance computing tasks requiring powerful GPU acceleration. Azure AI Services Leverage cost-effective language models, generative AI, advanced search, and more to streamline workflows, enhance accuracy, and scale AI solutions. GitHub Models Access popular AI models for free, with playground testing, seamless integration, and enterprise-ready deployment through built-in tools. Azure AI model catalog Hub for discovering foundation models, curated by Microsoft, NVIDIA, Hugging Face, and Meta, packaged and optimized for Azure AI Foundry. AI Toolkit for Visual Studio Code Extension to download, evaluate, adjust, and deploy AI models, including guided walkthroughs for popular SLMS, within applications or to cloud services. AI app templates Jumpstart your AI apps on Azure in as little as five minutes with the AI app template gallery, tailored for a wide range of generative AI use cases. NVIDIA NeMo Develop custom GenAI to deliver enterprise-ready models with precise data curation, cutting-edge customization, retrieval-augmented generation (RAG), and accelerated performance. NVIDIA NIM A set of accelerated inference microservices that allow organizations to run AI models on NVIDIA GPUs anywhere. NVIDIA API Catalog Provides access to GPU-accelerated software, including performance-optimized containers, pretrained AI models, and industry-specific SDKs. Semantic Kernel Open-source SDK that allows developers to easily combine AI services with conventional programming languages. Retrieval Augmented Generation (RAG) Enhance LLM capabilities by integrating an information retrieval system, providing grounding data and allowing control over data used. LangChain Open-source library for developers to build complex and dynamic applications leveraging the capabilities of LLMs, serving as an orchestration tool for prompts. AutoGen Simplify orchestration, optimization, and automation of LLM workflows, enabling development of complex LLM applications using customizable and conversable agents. TypeChat Experimental library that uses type definitions in your codebase to retrieve structured AI responses that are type-safe, aiming to bridge the gap between natural language, application schemas, and APIs. NVIDIA NeMo Retriever World-class models for embedding, reranking, and ingestion to quickly unlock accurate insights from massive volumes of enterprise data. NVIDIA RAPIDS Libraries that speed up widely adopted operations and algorithms, reducing time to insight as questions evolve. NVIDIA LangChain Integration Bind tools with LangChain to NIM microservices to create structured outputs that bring agent capabilities to your applications. GenAIOps Optimize and automate IT operations to improve efficiency, reduce costs, and enhance decision-making for more adaptive and intelligent systems management. Azure Pipelines Automate the build and deployment process in Visual Studio Code with a single sign-on experience to access Azure resources and services. GitHub Actions Automate, customize, and execute workflows right in your repo. Discover, create, and share actions to perform any job you'd like, including CI/CD. Azure Kubernetes Service Manage and track changes to your Kubernetes apps with built-in version control. Run and debug containers directly in any Kubernetes environment. NVIDIA NeMo Curator Scalable data curation microservice that enables developers to sort through trillion-token multilingual datasets for pretraining LLMs. NVIDIA NeMo Customizer High-performance, scalable microservice that simplifies fine-tuning and alignment of LLMs for domain-specific use cases. NVIDIA NeMo Evaluator Evaluate custom LLMs and retrieval-augmented generation (RAG) efficiently and reliably across diverse academic and custom benchmarks with APIs. GitHub Advanced Security Automatically scan your code for vulnerabilities and dependencies to ensure you catch security issues early, before they become risks in production. Microsoft Security Copilot Leverage AI-driven threat analysis to quickly identify and respond to emerging security threats, enabling faster, more informed decision-making. Microsoft 365 Defender Secure your entire ecosystem\u2014from identities to endpoints\u2014by detecting and mitigating advanced cyberattacks, minimizing potential damage and reducing downtime. Azure Sentinel Centralize threat detection and investigation with a scalable, cloud-native SIEM, allowing you to efficiently respond to security incidents across your entire environment. Azure AI Content Safety Automatically filter harmful or inappropriate AI-generated content, ensuring a safe and trustworthy experience for users interacting with your applications. NVIDIA NeMo Guardrails Safeguards generative AI systems by orchestrating dialog management, ensuring accuracy, appropriateness, and security in smart applications with LLMs. Webinar Watch our latest webinar to learn how to build AI applications with Microsoft and NVIDIA AI Customize AI solutions Take your AI-enabled apps to market faster with a simplified set of developer tools from Microsoft and NVIDIA. Azure AI Foundry Develop generative AI solutions and custom copilots using pre-built and customizable AI models on your data accelerated by NVIDIA. Azure OpenAI Service Connect your data, call functions, and improve workflows with advanced language and image models. Azure AI Search Build rich search experiences and generative AI apps that combine large language models with enterprise data with an AI-powered information retrieval platform. NVIDIA AI Enterprise Accelerate data science pipelines and streamline the development and deployment of production-grade AI applications, including generative AI. Machine learning Adopt machine learning Build, train, and deploy machine learning models with ease, right from Visual Studio Code. Azure Machine Learning Build, train, and deploy machine learning models with ease across various environments including Visual Studio Code extensions. MLOps Enable automation and streamline the end-to-end lifecycle of machine learning projects with GitHub Actions. Azure AI Model Catalog Get access to a catalog of over 1600 foundational models, including NVIDIA Nemotron models, and effortlessly fine-tune and deploy the ones that align with your unique business needs. AI Project Templates with Azure Infra Find prebuilt, task specific AI project templates in the centralized and curated gallery to deploy intelligent apps solutions with the Azure Developer CLI. NVIDIA NeMo Build, customize, and deploy enterprise-grade generative AI models within Azure with an end-to-end framework as part of NVIDIA AI Enterprise. Build the future on the Copilot development stack\u00a0 Achieve more and unlock possibilities with pioneering generative AI, trusted developer tools, and global scale. Orchestration Enhance user experiences Augment LLM capabilities and orchestrate flows with tools and frameworks for enhanced performance and flexibility. Prompt flow Support prompt construction, orchestration, testing, evaluation, and deployment with a suite of development tools for end-to-end development cycles of LLM-based apps. Semantic Kernel Orchestrate plugins with AI and leverage the same AI orchestration patterns that power Microsoft's Copilots in your own apps. Retrieval Augmented Generation Ensure users receive the most accurate responses to their questions through customized solutions and eliminate the need for fine-tuning with NeMo Retriever from NVIDIA. AI Toolkit for Visual Studio Code Explore and fine-tune small language models (SML) with the AI Toolkit for VS Code and its rich set of features like model management, prompting, scoring, deployment, and more. Connect Connect, learn, and inspire Discover a vibrant AI community to search, connect, and contribute to a world of innovation and an environment of support and collaboration. Applied skills Validate your technical skills and advance your career by learning how to develop AI agents using Microsoft Azure OpenAI and Semantic Kernel. AI Show The AI Show, hosted by Seth Juarez, explores the world of artificial intelligence and machine learning. Through exciting discussions, it showcases Azure AI innovations and shares inspiring customer stories, and is a must-watch for developers and AI enthusiasts alike. AI Learning Hub Find the learning strategy that suits you best, for free, at Microsoft Learn: AI Learning Hub. Azure AI Discord Share knowledge, collaborate, and explore the world of artificial intelligence with the Azure AI Discord channel. NVIDIA Deep Learning Institute Deepen your learning with education and training solutions from NVIDIA. Global AI Community Join the Global AI Community to connect with like-minded peers, find local user groups, or start your own. Microsoft Developer Community Join discussions, blogs, and events from community members and Microsoft experts all focused on AI in the Microsoft Developer Community. Stories Learn about the apps developers are building with Azure and NVIDIA AI tools and services H&R Block is enhancing the online tax filing experience with AI Tax Assist, a new Azure AI-powered solution that streamlines the process, answers tax-related questions, and ensures accuracy and compliance. Products Azure AI Foundry Azure OpenAI Service Azure AI Content Safety +5 McDonald's China is set to innovate through digital transformation and revolutionize operations by integrating AI in collaboration with Microsoft, enhancing employee training and improving operational efficiency. Products Azure AI Service Azure Machine Learning GitHub Copilot +5 Orange France leveraged GenAI - in partnership with Microsoft - to innovate and streamline tasks, resulting in over 40 new AI use cases that improve customer service and operational efficiency. Products Azure OpenAI Service GitHub Copilot Innovate with trust Microsoft, in collaboration with NVIDIA, empowers cross-functional teams to develop AI applications securely through built-in tools and templates, integrating responsible AI in open-source, MLOps, and generative AI workflows. Deliver more trustworthy applications by using enterprise-grade privacy, security, and compliance capabilities co-developed by experts from Microsoft and NVIDIA in research, policy, and engineering for the era of AI."}
{"url": "https://hbr.org/2023/02/how-ai-will-transform-project-management", "title": "How AI Will Transform Project Management", "content": "Your Cart \n\t\t\t\tHow AI Will Transform Project Management\n\t\t\t Only 35% of projects today are completed successfully. One reason for this disappointing rate is the low level of maturity of technologies available for project management. This is about to change. Researchers, startups, and innovating organizations, are beginning to apply AI, machine learning, and other advanced technologies to project management, and by 2030 the field will undergo major shifts. Technology will soon improve project selection and prioritization, monitor progress, speed up reporting, and facilitate testing. Project managers, aided by virtual project assistants, will find their roles more focused on coaching and stakeholder management than on administration and manual tasks. The author show how organizations that want to reap the benefits of project management technologies should begin today by gathering and cleaning project data, preparing their people, and dedicating the resources necessary to drive this transformation. Sometime in the near future, the CEO of a large telecom provider is using a smartphone app to check on her organization\u2019s seven strategic initiatives. Within a few taps, she knows the status of every project and what percentage of expected benefits each one has delivered. Project charters and key performance indicators are available in moments, as are each team member\u2019s morale level and the overall buy-in of critical stakeholders. Partner Center Explore HBR HBR Store About HBR Manage My Account Follow HBR"}
{"url": "https://blog.hubspot.com/marketing/ai-project-management", "title": "I Tried 10 AI Project Management Tools to See if They\u2019re Worth It (Results & Recommendations) ", "content": "Blogs Blogs Trusted by business builders worldwide, the HubSpot Blogs are your number-one source for education and inspiration. \n        Marketing\n        \n        \n       Resources and ideas to put modern marketers ahead of the curve \n        Sales\n        \n        \n       Strategies to help you elevate your sales efforts \n        Service\n        \n        \n       Everything you need to deliver top-notch customer service \n        Website\n        \n        \n       Tutorials and how-tos to help you build better websites \n        Next in AI\n        \n        \n       Your essential daily read on all things AI and business. \n        Instagram Marketing\n        \n        \n       \n        Customer Retention\n        \n        \n       \n        Email Marketing\n        \n        \n       \n        SEO\n        \n        \n       \n        Sales Prospecting\n        \n        \n       Newsletters Newsletters All of HubSpot's handcrafted email newsletters, tucked in one place. \n        The Hustle\n        \n          \n\n\n Irreverent and insightful takes on business and tech, delivered to your inbox. \n        Masters In Marketing\n        \n        \n       Exclusive interviews with industry leaders, and curated resources, to help you become a better marketer. \n        The Pipeline\n        \n        \n       Tips, tactics, and strategies from experienced sales reps to help you make President's Club. Videos Videos Browse our collection of educational shows and videos on YouTube. \n        The Hustle\n        \n          \n\n\n Our unrivaled storytelling, in video format. Subscribe for little revelations across business and tech \n        Marketing with HubSpot\n        \n          \n\n\n Learn marketing strategies and skills straight from the HubSpot experts \n        My First Million\n\n\n        \n          \n\n\n When it comes to brainstorming business ideas, Sam and Shaan are legends of the game \n        Marketing Against the Grain\n        \n          \n\n\n Watch two cerebral CMOs tackle strategy, tactics, and trends \n        HubSpot\n        \n          \n\n\n Everything you need to know about building your business on HubSpot Podcasts Podcasts HubSpot Podcast Network is the destination for business professionals who seek the best education on how to grow a business. \n        My First Million\n        \n        \n       Each week, hosts Sam Parr and Shaan Puri explore new business ideas based on trends and opportunities in the market \n        Goal Digger\n\n\n        \n        \n       Redefining what success means and how you can find more joy, ease, and peace in the pursuit of your goals \n        The Hustle Daily Show\n        \n        \n       A daily dose of irreverent, offbeat, and informative takes on business and tech news \n        Another Bite\n        \n        \n       Each week, Another Bite breaks down the latest and greatest pitches from Shark Tank \n        Business Made Simple\n        \n        \n       Build your business for far and fast success  \n        Marketing Against the Grain\n        \n        \n       HubSpot CMO Kipp Bodnar and Zapier CMO Kieran Flanagan share what's happening now in marketing and what's ahead \n        Online Marketing Made Easy\n        \n        \n       \n        The Product Boss\n        \n        \n       \n        Nudge\n        \n        \n       \n        Side Hustle Pro\n        \n        \n       \n        Outbound Squad\n        \n        \n       Resources Resources Expand your knowledge and take control of your career with our in-depth guides, lessons, and tools. \n        Academy\n        \n        \n       Learn and get certified in the latest business trends from leading experts \n        Templates\n        \n        \n       Interactive documents and spreadsheets to customize for your business's needs \n        Ebooks\n        \n        \n       In-depth guides on dozens of topics pertaining to the marketing, sales, and customer service industries \n        Kits\n        \n        \n       Multi-use content bundled into one download to inform and empower you and your team \n        Tools\n        \n        \n       Customized assets for better branding, strategy, and insights HubSpot Products The HubSpot Customer Platform All of HubSpot's marketing, sales, customer service, CMS, operations, and commerce software on one platform. See pricing \n        Free HubSpot CRM\n        \n        \n       \n        Overview of all products\n        \n        \n       \n        Marketing Hub\n        \n        \n       Marketing automation software. Free and premium plans \n        Sales Hub\n        \n        \n       Sales CRM software. Free and premium plans \n        Service Hub\n        \n        \n       Customer service software. Free and premium plans \n        Content Hub\n        \n        \n       Content marketing software. Free and premium plans \n        Operations Hub\n        \n        \n       Operations software. Free and premium plans \n        Commerce Hub\n        \n        \n       B2B commerce software. Free and premium plans \n        About HubSpot\n        \n        \n       \n        Contact Us\n        \n        \n       \n        Customer Support\n        \n        \n       Log in \n        \u65e5\u672c\u8a9e\n        \n        \n       \n        Deutsch\n        \n        \n       \n        English\n        \n        \n       \n        Espa\u00f1ol\n        \n        \n       \n        Portugu\u00eas\n        \n        \n       \n        Fran\u00e7ais\n        \n        \n       Oh no! We couldn't find anything like that. Try another search, and we'll give it our best shot. I Tried 10 AI Project Management Tools to See if They\u2019re Worth It (Results & Recommendations) Updated: \n  \n    \n  \n  \n  \n    November 20, 2024\n  \n\n  Published: \n  \n    \n  \n  \n  \n    February 09, 2024\n  \n\n  AI project management tools simplify decision-making, keep projects rolling, and streamline communications. Pick the right project management tool, and you could save hundreds \u2014 even thousands \u2014 per year. I started in digital project management nine years ago, and AI project management tools were unheard of. The project management role was different than it is today. Project managers were doing a lot of manual admin and repetitive tasks while keeping everything together and bringing those all-important soft skills to clients and internal teams who were busy getting the job done. It was a lot. If you\u2019re reading this, you might still be working like that: more spreadsheets than you can bear to think about, project managers stressed with deliverables and shaky briefs, leaving the team to use their best guess. Today, my workflow relies on AI tools to keep my clients and team happy.  The tools take much of the project management, leaving me and the team with the mental capacity to do what humans do best: build and nurture relationships, send thoughtful updates, and deliver even faster than we could ten years ago. With the right AI tool, your workflow could look more streamlined with happier staff at work. Naturally, the AI project management tool you select will depend on how you want to use it, but this article should give you a solid guide for choosing the right AI project management tool for you. I\u2019ve included my review of each tool, how I found it, the AI features, the price, and who I think it\u2019s best for. What does AI project management software do? Testing AI Project Management Tools The Scenario 10 AI Project Management Software Free Project Management Template Plan, manage, and analyze your projects with this free template. Download Free All fields are required. You're all set! Click this link to access this resource at any time. What does AI project management software do? Before we dive into AI project management software, check out this quick refresher video on how to successfully manage a project. Ready to keep learning? AI project management software can help manage and organize projects and teams. They\u2019re commonly used for automating routine tasks, managing production schedules, storing files against projects and tasks, and providing a central hub with all content related to a project. With the rise of AI, you can automate workflows, remove decision fatigue with predictive analysis, bolster productivity, and essentially hire a digital assistant who\u2019s there to support you every day. Project management tools are worth every penny and will pay for themselves in productivity. But if you\u2019re worried about budgets, plenty of brilliant free project management tools exist. Testing AI Project Management Tools I\u2019ve tested AI project management and many other marketing tools for years. I have to admit it: I love trying and testing tools. It\u2019s almost a problem because, in the digital world, it\u2019s very easy to get overwhelmed by choice and distracted by the next amazing new development. But I can\u2019t see myself stopping anytime soon. In fact, I committed myself to try more tools in the future. Embracing the development of new tools is a fast track to an easier life, a streamlined business, and a to-do list that is as satisfying as it is productive. And, in case you\u2019re wondering, it\u2019s not just me saying this. Of those surveyed in Hubspot\u2019s State of AI report, respondents estimated they save two hours and 24 minutes per day when using AI compared to not. Automating manual tasks is estimated to save two hours and 16 minutes per day. The time saved using AI is significant. All you need to do is find the one that suits you and your needs, and I\u2019ve run extensive tests to help you out. When I\u2019m testing AI project management tools, I want a tool that: I judged the tools tested in this article by these factors: The Scenario The scenario is close to my actual life as a marketer. I run many projects with fully remote teams internally and externally (my team and the client\u2019s team). We all need to work harmoniously in a central location. The project needs to be well structured with some flexibility for changes. All team members need to add comments, set tasks, and have some accountability tracking to keep the project moving. Finally, the AI project management tools must take some elements of the project. These tasks must be monotonous, undesirable for the humans involved, and safe enough for AI intervention. The State of Artificial Intelligence in 2024 New research into how marketers are using AI and key insights into the future of marketing. Download Free All fields are required. You're all set! Click this link to access this resource at any time. 10 AI Project Management Software 1. Asana  Asana is my current go-to project management tool for my agency. I manage my team and client projects in Asana. I\u2019m a big fan, and it does everything I need. I\u2019ve used Asana consistently for about four years, and I manage without paying for premium packages, though I admit I need to invest. How I Discovered Asana A marketing operations specialist recommended Asana to an agency I worked in. The fully remote agency set-up was fantastic. I can honestly say the communication was better than other agencies I\u2019d worked in where employees were under the same roof. How Asana Supported My Project Management I\u2019ve introduced many team members to my agency\u2019s Asana setup, and I\u2019ve never had an issue with its functionality or their fast adaptation of our agency\u2019s process. Asana ticks a lot of the boxes for me. My team is happy; it\u2019s intuitive to use, you can manage projects and tasks efficiently, and communications related to projects take place within the app. This functionality is ideal for longer-term projects where different team members dip in and out. Asana is one of the oldest tools in this roundup. It\u2019s been around since 2008, which probably factors into its many app integration capabilities. Asana falls a little short regarding pricing and custom functionality, but Click Up excels here (more on that below). Favorite AI Features Workflow Although not an AI per se, Asana\u2019s workflow feature takes draining human tasks and automates them. To some degree, Asana is our Project Manager. A lot of what you do daily is repetitive \u2014 and, if we\u2019re being honest \u2014 boring. With Asana\u2019s workflow, you can forget boredom and automate repetitive tasks instead of focusing on what you love. This functionality takes a large portion of admin work off and provides AI-driven insights for\u00a0project managers. Workflow is a paid feature, but it saves hours. Simply trigger a workflow after an action (like moving a card from one column to another), and Asana will do the rest. It\u2019ll populate the workflow for the next phase and help your team to meet those SOPs. Asana Intelligence Asana intelligence is its AI component, including planning, summaries, and content edits. With Asana intelligence, you can use AI to construct responses and comments, ensuring you get the right tone. The AI will edit the status of your projects so you can see where you might have at-risk projects that need all-important human interaction. Pricing Best For 2. ClickUp  Image Source ClickUp is becoming a firm favorite within my network. It\u2019s anecdotal, but I\u2019m conversing more with people switching to ClickUp from Asana. The pull to ClickUp is always the same: it\u2019s cheaper, and you can customize it more. Asana charges a minimum of $12 a user per month, whereas ClickUp is $5 an extra user per month no matter what package you choose, a significant saving for larger teams. While you can customize Asana, customization options are for paying members, whereas ClickUp immediately gives you access to this functionality. How I Discovered ClickUp ClickUp is the chosen AI project management tool for two of my clients. Both switched from Asana to ClickUp, and both are very happy. Danielle Hixenbaugh, head of emotive growth at Emotive, recommended one client ClickUp. She says: \u201cIt\u2019s what I use for our agency. After testing many project management tools, I landed on ClickUp as the best price, flexibility, and option for marketers or bigger marketing teams.\u201d How ClickUp Supported My Project Management I use ClickUp to keep projects on track for my clients. I\u2019ve used it within the project manager role and as a consultant, responding to tasks and actions delegated to me. Like Asana, ClickUp does everything it needs to: it helps manage remote teams in different time zones, and comments are placed against projects and tasks. It\u2019s intuitive. I think the UX is slightly better than Asana\u2019s. Favorite AI features It\u2019s worth noting that the AI features are not available on the free plans. Content Writing and Editing Whatever you\u2019re writing, ClickUp AI can help you. With project management, generative AI can be pretty useful. Instead of manually typing messages, you can get a helping hand from the AI. As you\u2019d expect, you can use generative AI to edit text, adjust the tone of your comment, or edit longer form content, improving content length. Summarizing Content Summarizing content is beneficial for project management. Notes from a meeting or long descriptions can be translated and edited for a speedy brief to the team. Translation ClickUp\u2019s AI includes a translation feature, which is incredibly useful for remote teams working across the globe. Pricing Best For 3. Monday  Image Source Monday is the most expensive recommendation on this list, but it does a little more than other project management tools. Alongside project management, Monday users can also manage their Sales CRM with Monday\u2019s tools. Project managers using Enterprise plans can access reports, including work performance insights. How I Discovered Monday It was Monday\u2019s advertising that captured my attention many years ago. I downloaded and tested it to see how it compared to Click Up and Asana. How Monday Supported My Project Management Generally, I liked it! The onboarding process and setup are among the friendliest and most intuitive. You can't go wrong setting up your first project, as Monday walks you through the process. For me, Monday does a lot. In many cases, Monday could be too much and add to that team overwhelm, which is precisely what I wanted to avoid for this test. Like anything, though, the reports and functionality of Monday are only overwhelming if you don\u2019t need it. For those who want a more robust solution, Monday could be for you. Favorite AI Features Task List Generation Monday\u2019s AI assist will take the thinking out of a project\u2019s subtasks. All you need to do is ask the AI to list recommended steps for your project, and Monday will do the rest. Generative AI Many project management tools with AI features list generative AI for writing, summarizing, creating emails, etc. Monday does all of this. Pricing Monday\u2019s pricing structure is more rigid than Click Up and Asana, and if you add things like the sales CRM, the price (rightfully) increases. For this article, the prices below are for project management. You must start with three seats, so the monthly price is more than Click Up or Asana, making Monday less suitable for small teams or individuals. Best For 4. Basecamp  Image Source Basecamp is a project management tool that caps its monthly price at $299 a month, billed annually. It sounds a lot compared to Asana or Click Up, but if your team exceeds ~57 people, Basecamp will become the most affordable option, beating even Click Up. Basecamp compares its monthly cost against a tech stack such as Asana, Slack, and Google Workplace, which Basecamp claims is $604.80 a month with just 20 members. How I Discovered Basecamp Basecamp was the preferred project management tool for a remote agency I worked for. It was intuitive to use and integrated with Google Workspace, which was helpful. All documents were added to the tool, keeping everything in one place. It was intuitive, and I felt comfortable with the tool without training. How Basecamp Supported My Project Management Basecamp took the agency I worked at from working solely in Slack with documents and project conversations taking place in several places and provided a centralized platform that helped streamline our workflows. Favorite AI Features Project completion  With Basecamp\u2019s Move the Needle and Mission control feature, you can gauge a project's performance without manually piecing together all the details. The project management tool will pull the data together and provide a warning for projects that are potentially \u201cat risk\u201d or \u201cconcerned.\u201d This feature takes a lot of manual work off project managers, leaving them with more time to solve issues and get a project back on track. Pricing Best For 5. Trello   Image Source How I Discovered Trello Trello was a tool I used many years ago. It was the first project management tool I was introduced to in 2011. I used Trello to manage content as part of a small marketing agency. How Trello Supported My Project Management I still use Trello today. It\u2019s in my project management arsenal, even with Asana for client projects. I like Trello because it is simple. For clients who don\u2019t have many projects, I turn to Trello. It\u2019s intuitive and easy to use, people get on board with it quickly, and the free package is enough for how I use it. I don\u2019t think Trello suits companies looking to scale, but it's perfect for small projects or teams. Favorite AI Features Strategy AI Tello\u2019s Strategy AI helps with general project management and productivity. You can use the software to control who sees what project, and projects or tasks are marked with priority to keep the team working on the most important tasks first. Pricing Trello is one of the cheapest project management tools. It is also one of the most simple. Best For 6. Motion  Image Source I didn\u2019t even complete my free trial before signing up for Motion. This AI project management tool makes scheduling and diary management so easy that handing over my hard-earned cash was a no-brainer. How I Discovered Motion To be honest, Motion\u2019s advertising targeted me for quite some time. It was possibly a year before I finally signed up and tried it out. Two parts of Motion drew me in: calendar syncing and the predictive analysis for task prioritization. Within days of using Motion, I was hooked, and I could see how it could replace other subscriptions and tools. Favorite AI Features AI Meeting Scheduling You set your preferred meeting times, and Motion\u2019s Meeting Assist AI will prioritize your desired meeting times and availability. All you need to do is set up your preferences and share a booking link with your guest. Motion will offer them times and dates that suit you. The streamlined calendar booking system saves a lot of monotony between you and your meeting guest. Decision-Making Made Easy With Predictive AI When you set up Motion, you add projects and tasks. You set deadlines (none, hard or soft), start dates, priorities, and working hours. You can set different working hours. For example, I use 9 am to 5 pm for my client work and hours outside that for personal projects and my own business. The AI will consider the above and schedule tasks based on urgency and availability. You\u2019ll get a warning if your schedule forces a project past its deadline so you can proactively solve any scheduling issues. Calendar Synchronization My calendar management gets overwhelming as a marketer and founder with multiple small businesses or side hustles, many clients, email addresses, and calendars. Motion made this incredibly easy by synching Outlook and Google calendars in one central place \u2014 a seemingly impossible task until I discovered Motion. I would criticize Motion\u2019s calendar as it offers only daily and weekly views and not monthly. I plan, so this is an issue for me. I have sent the feedback to their development team. In the meantime, I do have a solution: using One Cal alongside Motion. More on that below. How Motion Supported My Project Management Motion is the AI tool that I use to manage myself. However, there is a high chance that I'll introduce Motion to my team. For now, Motion is part of my project management tech stack for its calendar management and synchronization and its AI predictive analysis to help with the decision fatigue of what to do next. I love that I can trust Motion\u2019s scheduling to return a task list that is achievable and productive. Plus, the heads up on potential issues with deadlines is a lifesaver. Pricing Best For 7. OneCal  Image Source For me, OneCal is a must with or without Motion. So, don't skip this section if you\u2019re not using Motion. Some elements of OneCal cross over with Motion\u2019s functionality (e.g., calendar syncing and booking system). However, OneCal can sync many calendars. Plus, what sells in OneCal for me is that OneCal syncs Outlook and Google calendars on Outlook and Google, so you\u2019re not limited to the OneCal app. With Motion, you have to use Motion\u2019s calendar, and I\u2019ve already explained my issue with that (no monthly view). How I Discovered OneCal OneCal was recommended to me by automation expert Jake Error Santos. When I told him I was struggling with calendar management and synching, Jake put me onto OneCal, and I haven\u2019t looked back. Jake recommends OneCal frequently:  \"I frequently recommend OneCal to my clients due to its hassle-free setup, which outperforms the complexity of integrating calendars with low-code solutions like Zapier, making project management more efficient and straightforward.\u201d How OneCal Supported My Project Management While OneCal isn\u2019t managing projects, it is keeping my workload manageable and the monotony of checking multiple calendars at bay. This means I can go to any calendar for an accurate display of what\u2019s happening in all of my calendars. Ultimately, it saves me a lot of time and rids me of calendar anxiety. For those who don\u2019t use Motion, OneCal also has a booking system. Favorite AI Features Calendar Syncing Once you\u2019ve integrated OneCal with your calendars, you\u2019ll have synced calendars everywhere. Booking Links You can set up a booking system so your meeting guests can book a slot in your calendar at a time that suits you (and them!). With this system, you can set buffer times and avoid back-to-back meetings, and your guests can easily see available slots in their time zones. Pricing Best For 8. Notion  Image Source Notion is not my favorite project management tool for managing actual projects, though it does have that functionality. I recommend sticking to Click Up, Asana, or Motion for that. But Notion is a business\u2018s best friend! It\u2019s very intuitive and keeps all your documents in one place. It has a great UX and design, making it inspiring. Use Notion to complement your project management tool. Store your guides, resources, templates, etc, in Notion. How I Discovered Notion Notion was first recommended by a consultant who used it to store everything: her process, client documentation, and even social media content and planning. I was enamored by its UX and design and have since used Notion with clients. It is especially useful when you have remote teams and many ideas and resources. How Notion Supported My Project Management Notion is the data hub. I interlink between Notion and my chosen project management tool. The project management tool looks after the project, and tasks that require templates of existing documentation are linked to Notion, where this resource is stored. Favorite AI Features Ask Notion Whatever you need support with, ask Notion. It\u2019s like a 24/7 virtual assistant who knows everything about Notion, so you get the most out of the platform. Plus, the AI can report on what your team is working on. Write With Notion\u2019s Generative AI I\u2019ve used generative AI quite often on Notion, and I don\u2019t love generative AI. When it comes to writing basic documents for my agency resources, Notion\u2019s AI is a timesaver. Pricing Best For 9. Slack  Image Source Love it or hate it, Slack has its uses when it comes to project management. While I highly recommend not keeping track of projects within Slack, I think it\u2019s useful for catching a team member's attention and getting something actioned ASAP. Slack does get expensive fast, especially for large teams. For this reason, it might be beneficial to insist that all communications occur in the project management tool. For this to work, you need team members active in their inbox in your chosen project management tool. How I Discovered Slack I think every company I\u2019ve worked in or consulted for uses Slack. How Slack Supported My Project Management Slack is best used for immediate communications. My general rule with Slack and Project Management tools is: Favorite AI Features Thread Summaries Slack allows conversations to take place in threads. It helps tidy conversations within channels. While useful, threads can get messy; it\u2019s like reading an entire email chain you didn\u2019t need to be cc\u2019d into. With Slack\u2019s AI, you can get a summary instead. Automated Messages Not strictly AI, but useful nonetheless. For messages you have to send regularly, you can automate them so Slackbot sends them on your behalf. Pricing Best For 10. Loom  Image Source Loom is a fantastic tool for project managers, and the AI features are time-saving and incredibly useful. The premise of Loom is simple: you can record videos with a screenshare and selfie camera view. It\u2019s very personable. In the interest of transparency, I must admit that Loom can be glitchy. I\u2019ve experienced issues with the selfie camera not working as it should and videos not uploading. It can also be very slow to load. While Loom is still my preferred tool, I have been tempted to switch to Veed.io. What keeps me wed to Loom is the price. How I Discovered Loom I watched other loom videos, loved the functionality of the selfie video while screen sharing, and signed up immediately. I\u2019ve used Loom for about five years. How Loom Supported My Project Management Where training and meetings are concerned, Loom can be a highly efficient way of working. Instead of training staff or onboarding, you can create Loom videos of tasks you need others to do. Store videos within Loom and link to them as part of onboarding or training flows. This saves project managers\u2019 time in training, meaning they can focus on other tasks. I introduced this way of working with my client, Kineon. Loom is especially useful to them because they have a fully remote team across six continents. Tom Sanderson, co-founder of Kineon, says: \u201cWe use Loom for a-synchronous meetings and find it highly effective. Team members create a Loom video at a time to suit them; decision-makers or other team members can watch it on their schedule and provide insights or decisions that keep us moving.\u201d Loom\u2019s AI features can bolster project management productivity, too. Favorite AI Features Video Title The generative AI will title your video for you. This feature is particularly useful, especially for those quick videos that won\u2019t make it to your resource library but benefit from a logical title to keep everything tidy. Summaries and Tasks The AI will summarize the video, which gives recipients the video and a written summary. The task list is handy for project management. Loom highlights any tasks mentioned in your video so you can easily add tasks generated from the meeting in project management software or highlight them to team members for accountability tracking. Pricing Best For So, are AI project management tools worth it? Yes, AI project management tools are worth it, and I urge you to try them. If you\u2019re not using them, or if you\u2019re using the tools but not the AI, you are a) missing out and b) working harder than you should be. AI project management tools take monotonous tasks off your to-do list so you can do the work that satisfies you and supports the business you love. Still overwhelmed by choice? Here\u2019s what you should do in order of priority: Get rolling with the tools, and I\u2019m confident you\u2019ll thank yourself that you did. The State of Artificial Intelligence in 2024 New research into how marketers are using AI and key insights into the future of marketing. Download Free All fields are required. You're all set! Click this link to access this resource at any time. \u00a0 Don't forget to share this post! Related Articles \n                    3 Missteps with AI Image Generation and How You Should Be Using Them\n                   \n                      \n  \n    \n  \n  \n  \n    Nov 14, 2024\n  \n\n\n                     \n                    5 Ways that AI Analytics Tools Can Make You a Better Marketer\n                   \n                      \n  \n    \n  \n  \n  \n    Nov 04, 2024\n  \n\n\n                     \n                    Are AI Agents Worth It? HubSpot's CMO and VP Weigh In\n                   \n                      \n  \n    \n  \n  \n  \n    Oct 29, 2024\n  \n\n\n                     \n                    AI Conversion Rate Optimization \u2014 What Are the Benefits & How to Use It in Your Business\n                   \n                      \n  \n    \n  \n  \n  \n    Oct 28, 2024\n  \n\n\n                     \n                    How to Generate Video Scripts With AI\n                   \n                      \n  \n    \n  \n  \n  \n    Oct 23, 2024\n  \n\n\n                     \n                    How Marketers Save Time and Make Data-Backed Decisions with AI Reporting [+ Expert Insights]\n                   \n                      \n  \n    \n  \n  \n  \n    Oct 22, 2024\n  \n\n\n                     \n                    9 Best AI Marketing Bots I Use at Work in 2024\n                   \n                      \n  \n    \n  \n  \n  \n    Oct 22, 2024\n  \n\n\n                     \n                    I Tested 7 AI Tools for Graphic Design, Here Are My Favorites\n                   \n                      \n  \n    \n  \n  \n  \n    Oct 18, 2024\n  \n\n\n                     \n                    AI in Graphic Design: The Pros, Cons and What it Means for Designers [+ Expert Insight]\n                   \n                      \n  \n    \n  \n  \n  \n    Oct 17, 2024\n  \n\n\n                     \n                    I Tried Creating a YouTube Channel Using Only AI\n                   \n                      \n  \n    \n  \n  \n  \n    Oct 15, 2024\n  \n\n\n                     \nJoin 600,000+ Fellow Marketers\nThanks for Subscribing!\n Exclusive interviews with industry leaders, and curated resources, to help you become a better marketer. Subscribe to Masters In Marketing below. Must enter a valid email We're committed to your privacy. HubSpot uses the information you provide to us to contact you about our relevant content, products, and services. You may unsubscribe from these communications at any time. For more information, check out our Privacy Policy. Want to learn how to build a blog like this?We started with one post.Download free ebook Not using HubSpot yet? New research into how marketers are using AI and key insights into the future of marketing with AI. The weekly email to help take your career to the next level. No fluff, only first-hand expert advice & useful marketing trends. Email Must enter a valid email We're committed to your privacy. HubSpot uses the information you provide to us to contact you about our relevant content, products, and services. You may unsubscribe from these communications at any time. For more information, check out our privacy policy.  This form is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. Thank you! You've been subscribed \n\nPopular Features\n\n\n\n\n \n\nFree Tools\n\n\n\n\n \n\nCompany\n\n\n\n\n \n\nCustomers\n\n\n\n\n \n\nPartners\n\n\n\n\n Copyright \u00a9 2024 HubSpot, Inc."}
{"url": "https://www.techtarget.com/searchenterpriseai/tip/AI-model-optimization-How-to-do-it-and-why-it-matters", "title": "AI model optimization: How to do it and why it matters | TechTarget", "content": "This wide-ranging guide to artificial intelligence in the enterprise provides the building blocks for becoming successful business consumers of AI technologies. It starts with introductory explanations of AI's history, how AI works and the main types of AI. The importance and impact of AI is covered next, followed by information on AI's key benefits and risks, current and potential AI use cases, building a successful AI strategy, steps for implementing AI tools in the enterprise and technological breakthroughs that are driving the field forward. Throughout the guide, we include hyperlinks to TechTarget articles that provide more detail and insights on the topics discussed. AI model optimization: How to do it and why it matters Challenges like model drift and operational inefficiency can plague AI models. These model optimization strategies can help engineers improve performance and mitigate issues. Organizations across industries are building AI models, but the process doesn't end when the model is successfully deployed. Equally important is optimizing the AI model to ensure that it delivers the best results as efficiently as possible. Optimization is one way for AI and machine learning engineers to improve their AI models. Optimization strategies, such as retraining models with better data or enhancing models' source code, can benefit performance, efficiency and accuracy. By enhancing operational efficiency and model effectiveness, engineers can ensure their AI model overcomes challenges, like model drift, and becomes an asset to the company. What is AI model optimization? AI model optimization is the process of improving an AI model. AI and machine learning engineers can use model optimization to pursue two main goals: enhancing the operational efficiency of the model, and enhancing the effectiveness of the model itself. Model operation enhancements Operational model optimization means making changes to a model that enable it to operate more efficiently. For example, tweaking the code behind a model might help it consume less memory or CPU time, leading to lower infrastructure overhead (and, potentially, reduced costs) when operating the model. Changes to a model's design could also shorten the time it takes to produce results, leading to faster decision-making. This article is part of What is enterprise AI? A complete guide for businesses Model effectiveness enhancements Effectiveness optimizations improve the accuracy and reliability of the results a model produces. For instance, by retraining a model on higher-quality or more representative data, engineers could improve a model's decision accuracy rate from 95% to 98%. The importance of model optimization Model optimization is important because more efficient, accurate and reliable models create more value. Efficient models cost less to run, and highly accurate models deliver better results. That said, AI model optimization is about more than just trying to improve models over time. It's also often a necessary process for addressing challenges like AI model drift. Model drift can occur when a model becomes less efficient due to changes in the environment that hosts it, such as installing a different version of a software library that a model depends on. Model drift might also happen if the data that engineers used to train a model no longer accurately represents real-world conditions, causing the model to make suboptimal inferences, a phenomenon also known as data drift. Model optimization lets engineers improve their models in response to challenges like these so that the models remain as efficient and effective as possible. AI model optimization strategies A variety of strategies are effective for AI model optimization, but the following are some of the most common approaches. Retraining on better data Retraining a model on data that is more accurate or representative of real-world conditions is one of the most straightforward ways to optimize a model. However, retraining only makes sense if the data that the model initially trained on was not of high quality or if the data no longer represents real-world conditions. If the training data wasn't a problem and current data isn't substantially different, retraining is not likely to help with optimization. Deployment modification Changing how models are deployed can optimize outcomes. For instance, a model that struggles to produce results quickly might perform better if it is rehosted on hardware with more resources. If the model itself is inefficient, redeployment into an environment with more resources is a temporary fix rather than a true optimization technique, because it doesn't address the root cause of inefficiency. Nonetheless, it can lead to short-term enhanced performance. Source code enhancements Models that were poorly designed or implemented might benefit from changes to their algorithm's source code. For example, switching to a different library or framework might result in better performance if the new library or framework is more efficient. Source code modifications require significant effort, and engineers typically need to retrain the updated model afterward. In most cases, making changes to the model's algorithms is an optimization strategy that should be used sparingly and only if engineers believe the rewards outweigh the effort. Model pruning Model pruning is the removal of some parameters from a model, leading to a smaller model that runs with fewer resources. If the model contains parameters that aren't necessary for the use cases it needs to support, pruning can lead to better model performance without reducing the effectiveness of the model. Data set distillation Data set distillation is the process of consolidating a large data set into a smaller one. This can enhance AI model performance by reducing the amount of data that the model needs to parse. Distillation won't directly benefit deployed models because those models are already trained. However, it can make it easier to retrain models as part of an AI optimization strategy because there is less data to retrain on. It can also help remove inaccurate or unimportant data from a data set, leading to more effective decision-making once the model is retrained. Regularization To work effectively, AI models must achieve the right fit with the data they ingest. If a model is unable to interpret data effectively, it is underfitted. If it interprets data inconsistently -- meaning it doesn't make accurate decisions based on real-world input even though it trained accurately -- it's overfitted. Regularization can help address both underfitting and overfitting. In essence, regularization modifies the weight that a model assigns to different types of data to improve the model's ability to interpret data accurately. Chris Tozzi is a freelance writer, research advisor, and professor of IT and society who has previously worked as a journalist and Linux systems administrator. Next Steps Main types of artificial intelligence: Explained Steps to achieve AI implementation in your business AI risks businesses must confront and how to address them How businesses can measure AI success with KPIs How to formulate a winning AI strategy \n\t\t\tRelated Resources \nDig Deeper on AI technologies AI inference vs. training: Key differences and tradeoffs What are ModelOps (model operations) analytics models? Qlik AutoML update targets trust with visibility, simplicity What is machine learning operations (MLOps)? Analytics applications need clean data to produce actionable insights. Six data preparation best practices can turn your messy ... Business intelligence analysts are key members of BI teams. Here's a look at the job, the skills it requires, salary levels and ... BI reporting presents data in various formats so employees can interpret and act on that information in a timely manner without ... Tech and funding issues remain. But work on error handling, an expanding software stack and the growth of quantum ecosystems are ... President Joe Biden finalized CHIPS and Science Act funding for TSMC and Intel, although at a lesser amount for Intel than its ... IT departments should view succession planning as an ongoing component of their strategy rather than an afterthought. Our free ... Factors including a combination of commitment to data quality, proper technology and pertinent processes are key to preparing ... PowerSchool is creating personalized educational experiences for teachers, students, parents, administrators and others by using ... The alliance aims to make it easier and faster for the data cloud vendor's customers to use the Claude line of large language ... Nothing is certain except death, taxes and supply chain disruptions. Companies should plan accordingly, especially as the ... Essential TMS features are real-time tracking, route optimization, carrier management, load planning, fleet management, automated... Microsoft Dynamics 365 costs are highest and project duration longest of the top-tier ERP 'titans,' according to Panorama's ... All Rights Reserved, \nCopyright 2018 - 2024, TechTarget\n\n\nPrivacy Policy\n\n\n\nCookie Preferences \n\n\n\nCookie Preferences \n\n\n\nDo Not Sell or Share My Personal Information\n"}
{"url": "https://www.eweek.com/artificial-intelligence/ai-model-optimization/", "title": "AI Model Optimization: 6 Key Techniques", "content": "AI Model Optimization: 6 Key Techniques \neWEEK content and product recommendations are editorially independent. We may make money when you click on links to our partners. Learn More.\n While AI modeling involves building computational models that enable AI software to learn from data and create content, AI model optimization techniques enhance the efficiency and effectiveness of these artificial intelligence systems. The process of optimizing an AI model is crucial to create AI models that are high performing, consume reasonable amounts of resources, and are highly applicable to complex real-world scenarios. From model optimization strategies like model pruning to regularization, it\u2019s possible to fine tune models to not only perform more accurately in rigorous use cases but also leverage the full potential of AI. TABLE OF CONTENTS 6 Top AI Model Optimization Strategies Model optimization in artificial intelligence is about refining algorithms to improve their performance, reduce computational costs, and ensure their fitness for real-world business uses. It involves various techniques that address overfitting, underfitting, and the efficiency of the model to ensure that the AI system is both accurate and resource-efficient. However, AI model optimization can be complex and difficult. It includes challenges like balancing accuracy with computational demand, dealing with limited data, and adapting models to new or evolving tasks. These challenges show just how much businesses have to keep innovating to maintain the effectiveness of AI systems. Here are some of the strategies that enable us to optimize AI models. Retraining on Better Data The quality of the AI model is an amplified reflection of the quality of the data. Therefore, retraining AI models on enhanced datasets \u2014 datasets rich in quality, diversity, and relevance \u2014 is foundational for optimization. These enhanced datasets have minimal noise and errors and represent a wide range of scenarios and outcomes. They are also closely aligned with the current dynamics of their problem spaces, such as trends and scenarios. This ensures that models are updated with the latest information, which means that they can make more accurate predictions and adapt to not only changing data landscapes but also evolving use cases. It also makes sure models are adaptable to new trends, which is indispensable in fast changing fields like social media trend analysis and market forecasting. To gain a deeper understanding of today\u2019s top large language models, read our guide to Best Large Language Models Deployment Modification This strategy is based on adjusting the operating environment of the AI model to enhance performance and efficiency. In terms of hardware, deployment modification can involve changes to configurations, such as upgrading to more powerful processors or GPUs to accelerate computation. You can also look to optimize the software environment, such as by selecting more efficient libraries or frameworks that are a better fit for your underlying hardware. Additionally, deployment modifications might involve containerization of AI models using technologies like Docker, which allows for more scalable and manageable deployments across different platforms. Another technique to consider here is quantization, which converts models from floating-point to lower-precision formats to reduce model size and improve inference speed, especially on edge devices. Source Code Enhancements Refining the source code is all about enhancing the underlying algorithms and computational processes of AI models for substantial efficiency gains. It could involve adopting more efficient data structures, utilizing parallel computing, or leveraging specialized hardware accelerators like GPUs. Efficient data structures such as hash maps can greatly shorten lookup times for frequent operations, especially in use cases like online recommendation systems where models need to process large data amounts in real time. Parallel computing, on the other hand, divides complex tasks into smaller ones that can be solved concurrently instead of sequentially, which greatly speeds up the model training and inference processes. For example, in training deep neural networks, the computation of gradients for different batches of data can be parallelized, greatly reducing training time. By streamlining algorithms, removing redundant code, and employing more efficient computing practices, AI models can run faster and more reliably. An example of source code modification to optimize an AI model. In many cases, optimizing an AI model involves revising source code.\u00a0 Model Pruning Complexity, thanks to redundant and insignificant parameters, can not only make an AI model slower but also less accurate. As a result, to reduce this complexity, model pruning involves trimming down the AI model by removing non-essential features and parameters. This streamlining process can lead to faster inference times and lower memory requirements while maintaining or even improving its predictive accuracy. In practice, in the context of neural networks, pruning can be carried out after training a model by systematically removing weights that have the least effect on the output. This is known as weight pruning and can be carried out across all layers or within each layer of a neural network. You can also choose an approach that eliminates entire neurons or layers that contribute minimally to the model\u2019s output, known as neuron pruning. Data Set Distillation Data set distillation condenses the information contained in large training datasets into smaller, more manageable forms without losing the essential patterns needed for model training. This technique allows models to be trained faster and with less computational overhead, which leads to more rapid development cycles and enables efficient retraining as new data becomes available. The essence of distillation lies in its ability to transfer the \u2018knowledge\u2019 from a comprehensive dataset to a model trained on a distilled, smaller dataset. Simply put, it involves identifying the key patterns, trends, and relationships within a large data set and encapsulating this information into a smaller subset, and the smaller subset retains the key features for the model\u2019s training, which ensures that the model can still learn effectively even with the much lower data volume. This makes sure the model remains effective and far more efficient in terms of training time and resource usage. Regularization Regularization techniques like L1 and L2 regularization address overfitting by adding a penalty to the loss function used to train the model. This discourages overly complex models that might overfit the training data. By encouraging simpler models, regularization helps ensure that AI systems generalize better to unseen data, improving their reliability and performance in real-world applications. L1 regularization, also known as Lasso, can lead to models that only retain significant features, otherwise known as sparse models, by driving certain coefficients to zero, which results in effective feature selection. L2 regularization, or Ridge regression, minimizes coefficient values without making them zero, which stabilizes the model by distributing weight among features. This helps distribute the model\u2019s reliance on its features more evenly, which can lead to more stable predictions. It also ensures that the model isn\u2019t overly dependent on a small number of features. Optimization is vital for the practical deployment of AI models. Its greatest importance is making sure AI models perform optimally while using resources sensibly. It\u2019s an essential step in AI development that enables models to scale efficiently, adapt to new data, and maintain high levels of accuracy and reliability over time to maximize the success of AI initiatives across various domains. For more information about generative AI providers, read our in-depth guide: Generative AI Companies: Top 20 Leaders AIO (AI Optimization) and Process Improvement AI optimization (AIO) simply means using AI to improve the efficiency of operation or proces. Based on context, this factor can be an AI model or even AI\u2019s role in improving tasks and processes in a particular AI use case. AIO involves iterative processes and methodologies for fine-tuning AI systems, ensuring they not only meet but exceed their intended objectives, which ensures that AI solutions that are both effective and resource-conscious are deployed. In the context of AI model optimization, AIO applies advanced algorithms and techniques to improve model accuracy, reduce latency, and lower the computational costs associated with AI deployments. By addressing issues like overfitting, underfitting, and model complexity, AIO ensures that AI models are not just theoretically sound but practically viable and efficient in real-world scenarios. Some of the popular techniques and use cases include: Hyperparameter Tuning Hyperparameter tuning involves adjusting the parameters that govern the learning process of AI models to enhance their performance. A common use case is in deep learning, where tuning parameters like learning rate and batch size can significantly impact model accuracy and training speed. Feature Engineering Feature engineering is the process of selecting, modifying, or creating new features from raw data to improve model relevance. In financial fraud detection, for example, engineered features can help models better identify patterns that indicate fraudulent activity. Transfer Learning Transfer learning takes pre-trained models and uses them on new, related problems, which reduces the need for large datasets and extensive training. This technique is widely used in image recognition tasks, where models trained on vast image datasets are fine-tuned for specific recognition tasks with smaller datasets. Neural Architecture Search (NAS) NAS automates the design of neural network architectures, which improves their performance and makes them more efficient. In autonomous vehicle systems, NAS can be used to design lightweight but powerful neural networks capable of making decisions in real-time. The benefits of AIO extend across various domains, enhancing model accuracy, efficiency, and applicability. By optimizing AI models, organizations can achieve faster decision-making processes, reduce operational costs, and improve customer experiences. In healthcare, for example, AIO can help in the development of predictive models for patient outcomes, with the promise of earlier diagnoses, leading to better and personalized treatment plans, and improved patient care. Similarly, the cybersecurity domain benefits from AIO through more accurate advanced threat detection models, which help to bolster security measures and safeguard digital assets against evolving threats. Ultimately, the applications of AIO are diverse. But they do have one thing in common. Each application shows how impactful AIO is in harnessing the full capabilities of AI technologies for practical, impactful solutions. Importance of AI Model Optimization Although there are many reasons why AI model optimization is important, in essence it involves better performance, broader applicability, and maintained relevance, as follows: Enhancing Performance and Efficiency AI model optimization\u2019s greatest importance is in boosting the performance and efficiency of AI systems. Through tuning and adjustments, AI models are refined to make use of computational resources more effectively for faster and more accurate outcomes. In environments where there\u2019s a need to maintain a balance between speed and precision, this optimization is not negotiable as it will directly impact the utility and success of AI applications. Broadening Applicability Across Industries Optimization makes AI technologies more applicable across various sectors by adapting models to specific tasks and challenges. This bespoke approach allows AI systems to deliver targeted solutions, from improving patient diagnostics in healthcare to enhancing predictive models in finance, which showcases the versatility and potential for transformation powered by optimized AI models. Maintaining Relevance in Evolving Data Landscapes The dynamic nature of data necessitates regular optimization of AI models to maintain their accuracy and effectiveness. As data patterns and environments change, optimization ensures that AI systems adapt and evolve, providing consistent, valuable insights and solutions. This point shows us that AI model optimization is a continuous process to sustain the long-term relevance and impact of AI technologies. In this example of model optimization, model development is placed in a larger context of business goals, data processing, deployment and other key factors. Source: AWS 6 Common Challenges of AI Model Optimization As important as AI model optimization is, it presents an array of challenges that can impact the effectiveness and efficiency of models. Understanding these challenges is crucial for maintaining efficient AI systems. Balancing Complexity and Performance One of the primary challenges in AI model optimization is finding the right balance between model complexity and performance. Complex models may offer high accuracy but at the cost of increased computational resources and slower response times. Simplifying these models to improve efficiency often requires careful consideration to avoid significant losses in accuracy. Data Quality and Availability The quality and availability of data significantly impacts the optimization process. In many cases, the data needed to train and refine AI models may be scarce, imbalanced, or of poor quality. In such cases, you\u2019re guaranteed suboptimal model performance. Having constant access to high-quality, relevant data is a persistent challenge in AI optimization. Overfitting and Generalization Overfitting remains a critical challenge, where models perform quite well on training data but fail to generalize to new, unseen data. Addressing overfitting requires techniques that enhance the model\u2019s ability to generalize from the training data without compromising its ability to capture underlying patterns. Computational Resources The resources required for training and optimizing AI models, especially deep learning models, can be substantial, in some cases requiring a major enterprise infrastructure. Managing these resources effectively is a significant challenge, particularly in environments with limited computational capacity. Keeping Pace with Evolving Data Data powers AI models, but this data is constantly changing. Therefore, AI models need regular updates and optimizations to stay relevant as the data they are based on evolves. This continuous need for retraining and optimization can be resource-intensive and requires robust strategies to ensure models remain effective over time. Integration with Existing Systems Integrating optimized AI models into existing systems and workflows can be a challenge. Making sure they are compatible, as well as maintaining performance post-integration, needs careful planning and execution. Key Applications Showcasing AI\u2019s Role in Process Streamlining AI model optimization has countless applications across various industries. Here are some of the top use cases of AI technologies in not only streamlining processes but also enhancing operational efficiencies. Personalization and Diagnostics in Healthcare In healthcare, optimized AI models are revolutionizing patient care by enabling the development of personalized treatment plans. By analyzing vast datasets, AI can predict patient outcomes with high accuracy, allowing healthcare providers to adapt treatments to individual needs, resulting in improved recovery rates and patient satisfaction. AI model optimization also enhances the accuracy of diagnostics. Through advanced algorithms that analyze medical imagery, AI can identify patterns indicative of diseases such as cancer at early stages, leading to timely and more effective treatments. Enhanced Fraud Detection in Finance The finance sector benefits from AI optimization through improved fraud detection systems. Optimized models can analyze transaction patterns in real-time and spot anomalies that may indicate fraudulent activity, thus safeguarding assets, consumers, and institutions alike. Process Optimization in Manufacturing Process optimization in manufacturing yields reduced waste and higher efficiency. AI models can predict machine failures, optimize resource allocation, and enhance quality control, leading to increased efficiency and reduced downtime. By predicting maintenance needs, optimizing supply chains, and streamlining production lines, AI drives significant cost savings and productivity gains. Inventory Management in Retail Retailers leverage optimized AI for advanced inventory management, which involves predicting stock levels with high precision. This ensures optimal stock levels, reduces overstock and stockouts, and improves customer satisfaction by ensuring that the correct products are available at the correct time. Improved Navigation in Autonomous Vehicles For autonomous vehicles, AI optimization enhances navigation systems, which in turn makes real-time decisions safer and more reliable. By processing vast amounts of sensor data, AI can navigate complex environments and reduce the likelihood of accidents. It also leads to improved efficiency in route planning and a reduction in energy consumption. Advanced Threat Detection Optimized AI models play a crucial role in cybersecurity as they enhance threat detection and response. By continuously learning and adapting to new threats, AI supports increasingly robust defense and more successful threat detection and response systems, which makes protecting data and infrastructure from evolving cyberthreats much faster and more effective. Frequently Asked Questions (FAQs): AI Model Optimization How Can AI Optimize Design? AI optimizes design by using algorithms that can generate numerous iterations based on set parameters to significantly enhance creativity and efficiency. This approach is instrumental in fields like architecture and product design, where AI-driven tools offer innovative solutions that balance aesthetics, functionality, and sustainability. How Is AI Used in Route Optimization? In route optimization, AI analyzes vast datasets, such as traffic patterns, weather conditions, and vehicle capacities, to provide users with the most efficient travel routes. This application is particularly useful in logistics and delivery services as it helps reduce travel time and fuel consumption and enhance operational efficiency and sustainability. How Do You Optimize an ML Model? Optimizing a machine learning (ML) model involves various techniques such as hyperparameter tuning for algorithm configuration, regularization to prevent overfitting, and feature selection to enhance model performance. These strategies ensure the model is as efficient and accurate as possible, which is crucial for its application in real-world scenarios. What Are the Three AI Models? The AI landscape features three primary models, namely: Bottom Line: Maximizing Potential with AI Model Optimization Improving your AI models with optimization amplifies the efficiency of AI systems. It\u2019s a continuous process that should be carried out from time to time to make sure your models remain relevant in their use cases. This not only helps you achieve peak performance but also broadens the horizon for AI\u2019s applicability across diverse sectors. From improving healthcare\u2019s personalized patient care to greater precision in manufacturing to better efficiency in AIOps tools, AI model optimization ensures that AI\u2019s potential is both realized and maximized. To see a list of the leading generative AI apps, read our guide: Top 20 Generative AI Tools and Apps 2024 \n                    Get the Free Newsletter!                 \n                    Subscribe to Daily Tech Insider for top news, trends & analysis                 \n                    Get the Free Newsletter!                 \n                    Subscribe to Daily Tech Insider for top news, trends & analysis                 MOST POPULAR ARTICLES 9 Best Artificial Intelligence (AI) 3D Generators... RingCentral Expands Its Collaboration Platform 8 Best AI Data Analytics Software &... Zeus Kerravala on Networking: Multicloud, 5G, and... Datadog President Amit Agarwal on Trends in... eWeek has the latest technology news and analysis, buying guides, and product reviews for IT professionals and technology buyers. The site\u2019s focus is on innovative solutions and covering in-depth technical content. eWeek stays on the cutting edge of technology news and IT trends through interviews and expert analysis. Gain insight from top innovators and thought leaders in the fields of IT, business, enterprise software, startups, and more. Advertisers Advertise with TechnologyAdvice on eWeek and our other IT-focused platforms. Menu Our Brands Property of TechnologyAdvice.\n\u00a9 2024 TechnologyAdvice. All Rights Reserved\nAdvertiser Disclosure: Some of the products that appear on this site are from companies from which TechnologyAdvice receives compensation. This compensation may impact how and where products appear on this site including, for example, the order in which they appear. TechnologyAdvice does not include all companies or all types of products available in the marketplace."}
{"url": "https://owasp.org/www-project-ai-security-and-privacy-guide/", "title": "OWASP AI Security and Privacy Guide | OWASP Foundation", "content": "This website uses cookies to analyze our traffic and only share that information with our analytics partners. OWASP AI Security and Privacy Guide  This page is the OWASP AI security & privacy guide. It has two parts: Artificial Intelligence (AI) is on the rise and so are the concerns regarding AI security and privacy. This guide is a working document to provide clear and actionable insights on designing, creating, testing, and procuring secure and privacy-preserving AI systems. See also this useful recording or the slides from Rob van der Veer\u2019s talk at the OWASP Global appsec event in Dublin on February 15 2023, during which this guide was launched. And check out the Appsec Podcast episode on this guide (audio,video), or the September 2023 MLSecops Podcast. If you want the short story, check out the 13 minute AI security quick-talk.   Please provide your input through pull requests / submitting issues (see repo) or emailing the project lead, and let\u2019s make this guide better and better. Many thanks to Engin Bozdag, lead privacy architect at Uber, for his great contributions. How to address AI security This content is now found at the OWASP AI exchange\n\n\n How to address AI privacy Privacy principles and requirements come from different legislations (e.g. GDPR, LGPD, PIPEDA, etc.) and privacy standards (e.g. ISO 31700, ISO 29100, ISO 27701, FIPS, NIST Privacy Framework, etc.). This guideline does not guarantee compliance with privacy legislation and it is also not a guide on privacy engineering of systems in general. For that purpose, please consider work from ENISA, NIST, mplsplunk, OWASP and OpenCRE. The general principle for engineers is to regard personal data as \u2018radioactive gold\u2019. It\u2019s valuable, but it\u2019s also something to minimize, carefully store, carefully handle, limit its usage, limit sharing, keep track of where it is, etc. In this section, we will discuss how privacy principles apply to AI systems: 1. Use Limitation and Purpose Specification Essentially, you should not simply use data collected for one purpose (e.g. safety or security) as a training dataset to train your model for other purposes (e.g. profiling, personalized marketing, etc.) For example, if you collect phone numbers and other identifiers as part of your MFA flow (to improve security ), that doesn\u2019t mean you can also use it for user targeting and other unrelated purposes. Similarly, you may need to collect sensitive data under KYC requirements, but such data should not be used for ML models used for business analytics without proper controls. Some privacy laws require a lawful basis (or bases if for more than one purpose) for processing personal data (See GDPR\u2019s Art 6 and 9). \nHere is a link with certain restrictions on the purpose of an AI application, like for example the prohibited practices in the European AI Act such as using machine learning for individual criminal profiling. Some practices are regarded as too riskful when it comes to potential harm and unfairness towards individuals and society. Note that a use case may not even involve personal data, but can still be potentially harmful or unfair to indiduals. For example: an algorithm that decides who may join the army, based on the amount of weight a person can lift and how fast the person can run. This data can not be used to reidentify individuals (with some exceptions), but still the use case may be unrightfully unfair towards gender (if the algorithm for example is based on an unfair training set). In practical terms, you should reduce access to sensitive data and create anonymized copies for incompatible purposes (e.g. analytics). You should also document a purpose/lawful basis before collecting the data and communicate that purpose to the user in an appropriate way. New techniques that enable use limitation include: 2. Fairness Fairness means handling personal data in a way individuals expect and not using it in ways that lead to unjustified adverse effects. The algorithm should not behave in a discriminating way. (See also this article). Furthermore: accuracy issues of a model becomes a privacy problem if the model output leads to actions that invade privacy (e.g. undergoing fraud investigation). Accuracy issues can be caused by a complex problem, insufficient data, mistakes in data and model engineering, and manipulation by attackers. The latter example shows that there can be a relation between model security and privacy. GDPR\u2019s Article 5 refers to \u201cfair processing\u201d and EDPS\u2019 guideline defines fairness as the prevention of \u201cunjustifiably detrimental, unlawfully discriminatory, unexpected or misleading\u201d processing of personal data. GDPR does not specify how fairness can be measured, but the EDPS recommends the right to information (transparency), the right to intervene (access, erasure, data portability, rectify), and the right to limit the processing (right not to be subject to automated decision-making and non-discrimination) as measures and safeguard to implement the principle of fairness. In the literature, there are different fairness metrics that you can use. These range from group fairness, false positive error rate,  unawareness, and counterfactual fairness. There is no industry standard yet on which metric to use, but you should assess fairness especially if your algorithm is making significant decisions about the individuals (e.g. banning access to the platform, financial implications, denial of services/opportunities, etc.). There are also efforts to test algorithms using different metrics. For example,  NIST\u2019s FRVT project tests different face recognition algorithms on fairness using different metrics. The elephant in the room for fairness across groups (protected attributes) is that in situations a model is more accurate if it DOES discriminate protected attributes. Certain groups have in practice a lower success rate in areas because of all kinds of societal aspects rooted in culture and history. We want to get rid of that. Some of these aspects can be regarded as institutional discrimination. Others have more practical background, like for example that for language reasons we see that new immigrants statistically tend to be hindered in getting higher education.\nTherefore, if we want to be completely fair across groups, we need to accept that in many cases this will be balancing accuracy with discrimination. In the case that sufficient accuracy cannot be attained while staying within discrimination boundaries, there is no other option than to abandon the algorithm idea. For fraud detection cases, this could for example mean that transactions need to be selected randomly instead of by using an algorithm. A machine learning use case may have unsolvable bias issues, that are critical to recognize before you even start. Before you do any data analysis, you need to think if any of the key data elements involved have a skewed representation of protected groups (e.g. more men than women for certain types of education). I mean, not skewed in your training data, but in the real world. If so, bias is probably impossible to avoid - unless you can correct for the protected attributes. If you don\u2019t have those attributes (e.g. racial data) or proxies, there is no way. Then you have a dilemma between the benefit of an accurate model and a certain level of discrimination. This dilemma can be decided on before you even start, and save you a lot of trouble. Even with a diverse team, with an equally distributed dataset, and without any historical bias, your AI may still discriminate. And there may be nothing you can do about it.\nFor example: take a dataset of students with two variables: study program and  score on a math test. The goal is to let the model select students good at math for a special math program. Let\u2019s say that the study program \u2018computer science\u2019 has the best scoring students. And let\u2019s say that much more males then females are studying computer science. The result is that the model will select more males than females. Without having gender data in the dataset, this bias is impossible to counter. 3. Data Minimization and Storage Limitation This principle requires that you should minimize the amount, granularity and storage duration of personal information in your training dataset. To make it more concrete: There are also privacy-preserving techniques being developed that support data minimization: Further reading: 4. Transparency Privacy standards such as FIPP or ISO29100 refer to maintaining privacy notices, providing a copy of user\u2019s data upon request, giving notice when major changes in personal data procesing occur, etc. GDPR also refers to such practices but also has a specific clause related to algorithmic-decision making. \nGDPR\u2019s Article 22 allows individuals specific rights under specific conditions. This includes getting a human intervention to an algorithmic decision, an ability to contest the decision, and get a meaningful information about the logic involved. For examples of \u201cmeaningful information\u201d, see EDPS\u2019s guideline.  The US Equal Credit Opportunity Act requires detailed explanations on individual decisions by algorithms that deny credit. Transparency is not only needed for the end-user. Your models and datasets should be understandable by internal stakeholders as well: model developers, internal audit, privacy engineers, domain experts, and more. This typically requires the following: 5. Privacy Rights Also known as \u201cindividual participation\u201d under privacy standards, this principle allows individuals to submit requests to your organization related to their personal data. Most referred rights are: 6. Data accuracy You should ensure that your data is correct as the output of an algorithmic decision with incorrect data may lead to severe consequences for the individual. For example, if the user\u2019s phone number is incorrectly added to the system and if such number is associated with fraud, the user might be banned from a service/system in an unjust manner. You should have processes/tools in place to fix such accuracy issues as soon as possible when a proper request is made by the individual. To satisfy the accuracy principle, you should also have tools and processes in place to ensure that the data is obtained from reliable sources, its validity and correctness claims are validated and data quality and accuracy are periodically assessed. 7. Consent Consent may be used or required in specific circumstances. In such cases, consent must satisfy the following: Please note that consent will not be possible in specific circumstances (e.g. you cannot collect consent from a fraudster and an employer cannot collect consent from an employee as there is a power imbalance). If you must collect consent, then ensure that it is properly obtained, recorded and proper actions are taken if it is withdrawn. 8. Model attacks See the security section for security threats to data confidentiality, as they of course represent a privacy risk if that data is personal data. Notable: membership inference, model inversion, and training data leaking from the engineering process. In addition, models can disclose sensitive data that was unintendedly stored during training. Scope boundaries of AI privacy As said, many of the discussion topics on AI are about human rights, social justice, safety and only a part of it has to do with privacy. So as a data protection officer or engineer it\u2019s important not to drag everything into your responsibilities. At the same time, organizations do need to assign those non-privacy AI responsibilities somewhere. Before you start: Privacy restrictions on what you can do with AI The GDPR does not restrict the applications of AI explicitly but does provide safeguards that may limit what you can do, in particular regarding Lawfulness and limitations on purposes of collection, processing, and storage - as mentioned above. For more information on lawful grounds, see article 6 In an upcoming update, more will be discussed on the US AI bill of rights. The US Federal Trade Committe provides some good (global) guidance in communicating carefully about your AI, including not to overpromise. The EU AI act does pose explicit application limitations, such as mass surveillance, predictive policing, and restrictions on high-risk purposes such as selecting people for jobs. In addition, there are regulations for specific domains that restrict the use of data, putting limits to some AI approaches (e.g. the medical domain). The EU AI Act in a nutshell: Human rights are at the core of the AI Act, so risks are analyzed from a perspective of harmfulness to people. The Act identifies four risk levels for AI systems: So organizations will have to know their AI initiatives and perform high-level risk analysis to determine the risk level. AI is broadly defined here and includes wider statistical approaches and optimization algorithms. Generative AI needs to disclose what copyrighted sources were used, and prevent illegal content. To illustrate: if OpenAI for example would violate this rule, they could face a 10 billion dollar fine. Links: Further reading on AI privacy Project status This page is the current outcome of the project. The goal is to collect and present the state of the art on these topics through community collaboration. First in the form of this page, and later in other document forms. Please provide your input through pull requests / submitting issues (see repo) or emailing the project lead, and let\u2019s make this guide better and better. The work in this guide will serve as input to the upcoming ISO/IEC 27090 (AI security) and 27091 (AI privacy) standards, which will be done through membership of ISO/IEC JTC1/SC27/WG4, WG5, CEN/CENELEC JTC 21/WG1-TG, and the SC42 AHG4 group. Example Put whatever you like here: news, screenshots, features, supporters, or remove this file and don\u2019t use tabs at all. AI Security and Privacy Guide Information Social Links Document Repository Change Log FEATURED EVENT Leaders About Rob van der Veer Senior director at Software Improvement Group, lead author of the ISO/IEC 5338 standard on AI engineering, with 30 years experience in AI, security and privacy. Rob also contributes to OWASP SAMM and is co-lead of the Integration standards project - known for its Wayfinder and OpenCRE.org. Contact at [email\u00a0protected] Kudos to SIG for supporting the idea to open source results coming from SIG research and from working with clients on making their AI successful. Upcoming OWASP Global Events Corporate Supporters \r\n      OWASP, the OWASP logo, and Global AppSec are registered trademarks and AppSec Days, AppSec California, AppSec Cali, SnowFROC, and LASCON are trademarks of the OWASP Foundation, Inc. Unless otherwise specified, all content on the site is Creative Commons Attribution-ShareAlike v4.0 and provided without warranty of service or accuracy. For more information, please refer to our General Disclaimer. OWASP does not endorse or recommend commercial products or services, allowing our community to remain vendor neutral with the collective wisdom of the best minds in software security worldwide. Copyright 2024, OWASP Foundation, Inc.\r\n    "}
